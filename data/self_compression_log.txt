======================================================================
SELF-COMPRESSION EXPERIMENT
Testing: Do base models compress their instruct outputs better?
======================================================================

######################################################################
# Phase 1: Llama 3.2-3B-Instruct
######################################################################

Waiting for meta-llama/Llama-3.2-3B-Instruct to finish downloading...
  Still waiting... (0s elapsed)  Still waiting... (30s elapsed)  Still waiting... (60s elapsed)  Still waiting... (90s elapsed)  Still waiting... (120s elapsed)  ✓ meta-llama/Llama-3.2-3B-Instruct is ready at /home/alexn/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95

Generating code with llama32_3b...
Loading model from /home/alexn/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95...
`torch_dtype` is deprecated! Use `dtype` instead!
2026-02-16 22:16:33.288118: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
/home/alexn/Documents/code/research/LLMzip_from_scratch/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:6105: UserWarning: expandable_segments not supported on this platform (Triggered internally at /pytorch/c10/hip/HIPAllocatorConfig.h:29.)
  _ = torch.empty(byte_count // factor, dtype=torch.float16, device=device, requires_grad=False)
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.27s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.34s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.78s/it]

[1/5] Generating: Write a Python program that implements a binary search tree ...
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
/home/alexn/Documents/code/research/LLMzip_from_scratch/.venv/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:101: UserWarning: Attempting to use hipBLASLt on an unsupported architecture! Overriding blas backend to hipblas (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:296.)
  freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
  Saved: source_text/generated_llama32_3b_1.py.txt (1907 chars)

[2/5] Generating: Write a Python program that reads a CSV file and computes st...
  Saved: source_text/generated_llama32_3b_2.py.txt (599 chars)

[3/5] Generating: Write a Python program that implements a simple LRU cache us...
  Saved: source_text/generated_llama32_3b_3.py.txt (1145 chars)

[4/5] Generating: Write a Python program that implements the Sieve of Eratosth...
  Saved: source_text/generated_llama32_3b_4.py.txt (717 chars)

[5/5] Generating: Write a Python program that implements a basic HTTP server u...
  Saved: source_text/generated_llama32_3b_5.py.txt (2055 chars)

Generated 5 files:
  source_text/generated_llama32_3b_1.py.txt
  source_text/generated_llama32_3b_2.py.txt
  source_text/generated_llama32_3b_3.py.txt
  source_text/generated_llama32_3b_4.py.txt
  source_text/generated_llama32_3b_5.py.txt
  Generated 5 files

======================================================================
Compressing 5 files from Llama 3.2-3B-Instruct
with 7 base models (window=50)
======================================================================

[1/35] Qwen 2.5-3B | generated_llama32_3b_1.py.txt
Loading model from /home/alexn/.cache/huggingface/hub/models--Qwen--Qwen2.5-3B/snapshots/3aab1f1954e9cc14eb9509a215f9e5ca08227a9b...
`torch_dtype` is deprecated! Use `dtype` instead!
2026-02-16 22:18:59.966698: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
/home/alexn/Documents/code/research/LLMzip_from_scratch/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:6105: UserWarning: expandable_segments not supported on this platform (Triggered internally at /pytorch/c10/hip/HIPAllocatorConfig.h:29.)
  _ = torch.empty(byte_count // factor, dtype=torch.float16, device=device, requires_grad=False)
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.16s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.52s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.77s/it]
Compressing /home/alexn/Documents/code/research/LLMzip_from_scratch/source_text/generated_llama32_3b_1.py.txt to /home/alexn/Documents/code/research/LLMzip_from_scratch/encoded_documents/selfcomp_Qwen_253B_generated_llama32_3b_1.py_w50.llmzip...
Total tokens to encode: 422
/home/alexn/Documents/code/research/LLMzip_from_scratch/.venv/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:301: UserWarning: Attempting to use hipBLASLt on an unsupported architecture! Overriding blas backend to hipblas (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:296.)
  freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
Encoded 10/422 tokens (cache: 10)Encoded 20/422 tokens (cache: 20)Encoded 30/422 tokens (cache: 30)Encoded 40/422 tokens (cache: 40)Encoded 50/422 tokens (cache: 50)Encoded 60/422 tokens (cache: 60)Encoded 70/422 tokens (cache: 70)Encoded 80/422 tokens (cache: 80)Encoded 90/422 tokens (cache: 90)Encoded 100/422 tokens (cache: 100)Encoded 110/422 tokens (cache: 110)Encoded 120/422 tokens (cache: 120)Encoded 130/422 tokens (cache: 130)Encoded 140/422 tokens (cache: 140)Encoded 150/422 tokens (cache: 150)Encoded 160/422 tokens (cache: 160)Encoded 170/422 tokens (cache: 170)Encoded 180/422 tokens (cache: 180)Encoded 190/422 tokens (cache: 190)Encoded 200/422 tokens (cache: 200)Encoded 210/422 tokens (cache: 210)Encoded 220/422 tokens (cache: 220)Encoded 230/422 tokens (cache: 230)Encoded 240/422 tokens (cache: 240)Encoded 250/422 tokens (cache: 250)Encoded 260/422 tokens (cache: 260)Encoded 270/422 tokens (cache: 270)Encoded 280/422 tokens (cache: 280)Encoded 290/422 tokens (cache: 290)Encoded 300/422 tokens (cache: 300)Encoded 310/422 tokens (cache: 310)Encoded 320/422 tokens (cache: 320)Encoded 330/422 tokens (cache: 330)Encoded 340/422 tokens (cache: 340)Encoded 350/422 tokens (cache: 350)Encoded 360/422 tokens (cache: 360)Encoded 370/422 tokens (cache: 370)Encoded 380/422 tokens (cache: 380)Encoded 390/422 tokens (cache: 390)Encoded 400/422 tokens (cache: 400)Encoded 410/422 tokens (cache: 410)Encoded 420/422 tokens (cache: 420)
Encoding complete.
Original size: 1907 chars
Compressed size: 24 bytes
Bits per character: 0.1007
Compression finished in 33.44 seconds.
Results saved to benchmarks.db
Results saved to model_results.json
  BPC: 0.1007 | Ratio: 79.4583

[2/35] Qwen 2.5-3B | generated_llama32_3b_2.py.txt
Loading model from /home/alexn/.cache/huggingface/hub/models--Qwen--Qwen2.5-3B/snapshots/3aab1f1954e9cc14eb9509a215f9e5ca08227a9b...
`torch_dtype` is deprecated! Use `dtype` instead!
2026-02-16 22:19:47.783101: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
/home/alexn/Documents/code/research/LLMzip_from_scratch/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:6105: UserWarning: expandable_segments not supported on this platform (Triggered internally at /pytorch/c10/hip/HIPAllocatorConfig.h:29.)
  _ = torch.empty(byte_count // factor, dtype=torch.float16, device=device, requires_grad=False)
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.52s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.56s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.70s/it]
Compressing /home/alexn/Documents/code/research/LLMzip_from_scratch/source_text/generated_llama32_3b_2.py.txt to /home/alexn/Documents/code/research/LLMzip_from_scratch/encoded_documents/selfcomp_Qwen_253B_generated_llama32_3b_2.py_w50.llmzip...
Total tokens to encode: 171
/home/alexn/Documents/code/research/LLMzip_from_scratch/.venv/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:301: UserWarning: Attempting to use hipBLASLt on an unsupported architecture! Overriding blas backend to hipblas (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:296.)
  freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
Encoded 10/171 tokens (cache: 10)Encoded 20/171 tokens (cache: 20)Encoded 30/171 tokens (cache: 30)Encoded 40/171 tokens (cache: 40)Encoded 50/171 tokens (cache: 50)Encoded 60/171 tokens (cache: 60)Encoded 70/171 tokens (cache: 70)Encoded 80/171 tokens (cache: 80)Encoded 90/171 tokens (cache: 90)Encoded 100/171 tokens (cache: 100)Encoded 110/171 tokens (cache: 110)Encoded 120/171 tokens (cache: 120)Encoded 130/171 tokens (cache: 130)Encoded 140/171 tokens (cache: 140)Encoded 150/171 tokens (cache: 150)Encoded 160/171 tokens (cache: 160)Encoded 170/171 tokens (cache: 170)
Encoding complete.
Original size: 599 chars
Compressed size: 22 bytes
Bits per character: 0.2938
Compression finished in 13.77 seconds.
Results saved to benchmarks.db
Results saved to model_results.json
  BPC: 0.2938 | Ratio: 27.2273

[3/35] Qwen 2.5-3B | generated_llama32_3b_3.py.txt
Loading model from /home/alexn/.cache/huggingface/hub/models--Qwen--Qwen2.5-3B/snapshots/3aab1f1954e9cc14eb9509a215f9e5ca08227a9b...
`torch_dtype` is deprecated! Use `dtype` instead!
2026-02-16 22:20:13.754735: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
/home/alexn/Documents/code/research/LLMzip_from_scratch/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:6105: UserWarning: expandable_segments not supported on this platform (Triggered internally at /pytorch/c10/hip/HIPAllocatorConfig.h:29.)
  _ = torch.empty(byte_count // factor, dtype=torch.float16, device=device, requires_grad=False)
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.56s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.10s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.17s/it]
Compressing /home/alexn/Documents/code/research/LLMzip_from_scratch/source_text/generated_llama32_3b_3.py.txt to /home/alexn/Documents/code/research/LLMzip_from_scratch/encoded_documents/selfcomp_Qwen_253B_generated_llama32_3b_3.py_w50.llmzip...
Total tokens to encode: 292
/home/alexn/Documents/code/research/LLMzip_from_scratch/.venv/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:301: UserWarning: Attempting to use hipBLASLt on an unsupported architecture! Overriding blas backend to hipblas (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:296.)
  freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
Encoded 10/292 tokens (cache: 10)Encoded 20/292 tokens (cache: 20)Encoded 30/292 tokens (cache: 30)Encoded 40/292 tokens (cache: 40)Encoded 50/292 tokens (cache: 50)Encoded 60/292 tokens (cache: 60)Encoded 70/292 tokens (cache: 70)Encoded 80/292 tokens (cache: 80)Encoded 90/292 tokens (cache: 90)Encoded 100/292 tokens (cache: 100)Encoded 110/292 tokens (cache: 110)Encoded 120/292 tokens (cache: 120)Encoded 130/292 tokens (cache: 130)Encoded 140/292 tokens (cache: 140)Encoded 150/292 tokens (cache: 150)Encoded 160/292 tokens (cache: 160)Encoded 170/292 tokens (cache: 170)Encoded 180/292 tokens (cache: 180)Encoded 190/292 tokens (cache: 190)Encoded 200/292 tokens (cache: 200)Encoded 210/292 tokens (cache: 210)Encoded 220/292 tokens (cache: 220)Encoded 230/292 tokens (cache: 230)Encoded 240/292 tokens (cache: 240)Encoded 250/292 tokens (cache: 250)Encoded 260/292 tokens (cache: 260)Encoded 270/292 tokens (cache: 270)Encoded 280/292 tokens (cache: 280)Encoded 290/292 tokens (cache: 290)
Encoding complete.
Original size: 1145 chars
Compressed size: 26 bytes
Bits per character: 0.1817
Compression finished in 23.19 seconds.
Results saved to benchmarks.db
Results saved to model_results.json
  BPC: 0.1817 | Ratio: 44.0385

[4/35] Qwen 2.5-3B | generated_llama32_3b_4.py.txt
Loading model from /home/alexn/.cache/huggingface/hub/models--Qwen--Qwen2.5-3B/snapshots/3aab1f1954e9cc14eb9509a215f9e5ca08227a9b...
`torch_dtype` is deprecated! Use `dtype` instead!
2026-02-16 22:20:48.132803: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
/home/alexn/Documents/code/research/LLMzip_from_scratch/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:6105: UserWarning: expandable_segments not supported on this platform (Triggered internally at /pytorch/c10/hip/HIPAllocatorConfig.h:29.)
  _ = torch.empty(byte_count // factor, dtype=torch.float16, device=device, requires_grad=False)
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.18s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.32s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.45s/it]
Compressing /home/alexn/Documents/code/research/LLMzip_from_scratch/source_text/generated_llama32_3b_4.py.txt to /home/alexn/Documents/code/research/LLMzip_from_scratch/encoded_documents/selfcomp_Qwen_253B_generated_llama32_3b_4.py_w50.llmzip...
Total tokens to encode: 221
/home/alexn/Documents/code/research/LLMzip_from_scratch/.venv/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:301: UserWarning: Attempting to use hipBLASLt on an unsupported architecture! Overriding blas backend to hipblas (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:296.)
  freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
Encoded 10/221 tokens (cache: 10)Encoded 20/221 tokens (cache: 20)Encoded 30/221 tokens (cache: 30)Encoded 40/221 tokens (cache: 40)Encoded 50/221 tokens (cache: 50)Encoded 60/221 tokens (cache: 60)Encoded 70/221 tokens (cache: 70)Encoded 80/221 tokens (cache: 80)Encoded 90/221 tokens (cache: 90)Encoded 100/221 tokens (cache: 100)Encoded 110/221 tokens (cache: 110)Encoded 120/221 tokens (cache: 120)Encoded 130/221 tokens (cache: 130)Encoded 140/221 tokens (cache: 140)Encoded 150/221 tokens (cache: 150)Encoded 160/221 tokens (cache: 160)Encoded 170/221 tokens (cache: 170)Encoded 180/221 tokens (cache: 180)Encoded 190/221 tokens (cache: 190)Encoded 200/221 tokens (cache: 200)Encoded 210/221 tokens (cache: 210)Encoded 220/221 tokens (cache: 220)
Encoding complete.
Original size: 717 chars
Compressed size: 16 bytes
Bits per character: 0.1785
Compression finished in 17.65 seconds.
Results saved to benchmarks.db
Results saved to model_results.json
  BPC: 0.1785 | Ratio: 44.8125

[5/35] Qwen 2.5-3B | generated_llama32_3b_5.py.txt
Loading model from /home/alexn/.cache/huggingface/hub/models--Qwen--Qwen2.5-3B/snapshots/3aab1f1954e9cc14eb9509a215f9e5ca08227a9b...
`torch_dtype` is deprecated! Use `dtype` instead!
2026-02-16 22:21:17.556601: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
/home/alexn/Documents/code/research/LLMzip_from_scratch/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:6105: UserWarning: expandable_segments not supported on this platform (Triggered internally at /pytorch/c10/hip/HIPAllocatorConfig.h:29.)
  _ = torch.empty(byte_count // factor, dtype=torch.float16, device=device, requires_grad=False)
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.62s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.65s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.80s/it]
Compressing /home/alexn/Documents/code/research/LLMzip_from_scratch/source_text/generated_llama32_3b_5.py.txt to /home/alexn/Documents/code/research/LLMzip_from_scratch/encoded_documents/selfcomp_Qwen_253B_generated_llama32_3b_5.py_w50.llmzip...
Total tokens to encode: 448
/home/alexn/Documents/code/research/LLMzip_from_scratch/.venv/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:301: UserWarning: Attempting to use hipBLASLt on an unsupported architecture! Overriding blas backend to hipblas (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:296.)
  freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
Encoded 10/448 tokens (cache: 10)Encoded 20/448 tokens (cache: 20)Encoded 30/448 tokens (cache: 30)Encoded 40/448 tokens (cache: 40)Encoded 50/448 tokens (cache: 50)Encoded 60/448 tokens (cache: 60)Encoded 70/448 tokens (cache: 70)Encoded 80/448 tokens (cache: 80)Encoded 90/448 tokens (cache: 90)Encoded 100/448 tokens (cache: 100)Encoded 110/448 tokens (cache: 110)Encoded 120/448 tokens (cache: 120)Encoded 130/448 tokens (cache: 130)Encoded 140/448 tokens (cache: 140)Encoded 150/448 tokens (cache: 150)Encoded 160/448 tokens (cache: 160)Encoded 170/448 tokens (cache: 170)Encoded 180/448 tokens (cache: 180)Encoded 190/448 tokens (cache: 190)Encoded 200/448 tokens (cache: 200)Encoded 210/448 tokens (cache: 210)Encoded 220/448 tokens (cache: 220)Encoded 230/448 tokens (cache: 230)Encoded 240/448 tokens (cache: 240)Encoded 250/448 tokens (cache: 250)Encoded 260/448 tokens (cache: 260)Encoded 270/448 tokens (cache: 270)Encoded 280/448 tokens (cache: 280)Encoded 290/448 tokens (cache: 290)Encoded 300/448 tokens (cache: 300)Encoded 310/448 tokens (cache: 310)Encoded 320/448 tokens (cache: 320)Encoded 330/448 tokens (cache: 330)Encoded 340/448 tokens (cache: 340)Encoded 350/448 tokens (cache: 350)Encoded 360/448 tokens (cache: 360)Encoded 370/448 tokens (cache: 370)Encoded 380/448 tokens (cache: 380)Encoded 390/448 tokens (cache: 390)Encoded 400/448 tokens (cache: 400)Encoded 410/448 tokens (cache: 410)Encoded 420/448 tokens (cache: 420)Encoded 430/448 tokens (cache: 430)Encoded 440/448 tokens (cache: 440)
Encoding complete.
Original size: 2055 chars
Compressed size: 34 bytes
Bits per character: 0.1324
Compression finished in 35.46 seconds.
Results saved to benchmarks.db
Results saved to model_results.json
  BPC: 0.1324 | Ratio: 60.4412

[6/35] LiquidAI 1.2B | generated_llama32_3b_1.py.txt
Loading model from /home/alexn/.cache/huggingface/hub/models--LiquidAI--LFM2.5-1.2B-Base/snapshots/1e601c5c9d33bcc8da794c253243d6b258a4d38b...
`torch_dtype` is deprecated! Use `dtype` instead!
2026-02-16 22:22:03.325107: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
/home/alexn/Documents/code/research/LLMzip_from_scratch/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:6105: UserWarning: expandable_segments not supported on this platform (Triggered internally at /pytorch/c10/hip/HIPAllocatorConfig.h:29.)
  _ = torch.empty(byte_count // factor, dtype=torch.float16, device=device, requires_grad=False)
Compressing /home/alexn/Documents/code/research/LLMzip_from_scratch/source_text/generated_llama32_3b_1.py.txt to /home/alexn/Documents/code/research/LLMzip_from_scratch/encoded_documents/selfcomp_LiquidAI_12B_generated_llama32_3b_1.py_w50.llmzip...
Total tokens to encode: 492
/home/alexn/Documents/code/research/LLMzip_from_scratch/.venv/lib/python3.12/site-packages/transformers/models/lfm2/modeling_lfm2.py:97: UserWarning: Attempting to use hipBLASLt on an unsupported architecture! Overriding blas backend to hipblas (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:296.)
  freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
Encoded 10/492 tokens (cache: 10)Encoded 20/492 tokens (cache: 20)Encoded 30/492 tokens (cache: 30)Encoded 40/492 tokens (cache: 40)Encoded 50/492 tokens (cache: 50)Encoded 60/492 tokens (cache: 60)Encoded 70/492 tokens (cache: 70)Encoded 80/492 tokens (cache: 80)Encoded 90/492 tokens (cache: 90)Encoded 100/492 tokens (cache: 100)Encoded 110/492 tokens (cache: 110)Encoded 120/492 tokens (cache: 120)Encoded 130/492 tokens (cache: 130)Encoded 140/492 tokens (cache: 140)Encoded 150/492 tokens (cache: 150)Encoded 160/492 tokens (cache: 160)Encoded 170/492 tokens (cache: 170)Encoded 180/492 tokens (cache: 180)Encoded 190/492 tokens (cache: 190)Encoded 200/492 tokens (cache: 200)Encoded 210/492 tokens (cache: 210)Encoded 220/492 tokens (cache: 220)Encoded 230/492 tokens (cache: 230)Encoded 240/492 tokens (cache: 240)Encoded 250/492 tokens (cache: 250)Encoded 260/492 tokens (cache: 260)Encoded 270/492 tokens (cache: 270)Encoded 280/492 tokens (cache: 280)Encoded 290/492 tokens (cache: 290)Encoded 300/492 tokens (cache: 300)Encoded 310/492 tokens (cache: 310)Encoded 320/492 tokens (cache: 320)Encoded 330/492 tokens (cache: 330)Encoded 340/492 tokens (cache: 340)Encoded 350/492 tokens (cache: 350)Encoded 360/492 tokens (cache: 360)Encoded 370/492 tokens (cache: 370)Encoded 380/492 tokens (cache: 380)Encoded 390/492 tokens (cache: 390)Encoded 400/492 tokens (cache: 400)Encoded 410/492 tokens (cache: 410)Encoded 420/492 tokens (cache: 420)Encoded 430/492 tokens (cache: 430)Encoded 440/492 tokens (cache: 440)Encoded 450/492 tokens (cache: 450)Encoded 460/492 tokens (cache: 460)Encoded 470/492 tokens (cache: 470)Encoded 480/492 tokens (cache: 480)Encoded 490/492 tokens (cache: 490)
Encoding complete.
Original size: 1907 chars
Compressed size: 34 bytes
Bits per character: 0.1426
Compression finished in 19.57 seconds.
Results saved to benchmarks.db
Results saved to model_results.json
  BPC: 0.1426 | Ratio: 56.0882

[7/35] LiquidAI 1.2B | generated_llama32_3b_2.py.txt
Loading model from /home/alexn/.cache/huggingface/hub/models--LiquidAI--LFM2.5-1.2B-Base/snapshots/1e601c5c9d33bcc8da794c253243d6b258a4d38b...
`torch_dtype` is deprecated! Use `dtype` instead!
2026-02-16 22:22:32.212772: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
/home/alexn/Documents/code/research/LLMzip_from_scratch/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:6105: UserWarning: expandable_segments not supported on this platform (Triggered internally at /pytorch/c10/hip/HIPAllocatorConfig.h:29.)
  _ = torch.empty(byte_count // factor, dtype=torch.float16, device=device, requires_grad=False)
Compressing /home/alexn/Documents/code/research/LLMzip_from_scratch/source_text/generated_llama32_3b_2.py.txt to /home/alexn/Documents/code/research/LLMzip_from_scratch/encoded_documents/selfcomp_LiquidAI_12B_generated_llama32_3b_2.py_w50.llmzip...
Total tokens to encode: 197
/home/alexn/Documents/code/research/LLMzip_from_scratch/.venv/lib/python3.12/site-packages/transformers/models/lfm2/modeling_lfm2.py:97: UserWarning: Attempting to use hipBLASLt on an unsupported architecture! Overriding blas backend to hipblas (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:296.)
  freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
Encoded 10/197 tokens (cache: 10)Encoded 20/197 tokens (cache: 20)Encoded 30/197 tokens (cache: 30)Encoded 40/197 tokens (cache: 40)Encoded 50/197 tokens (cache: 50)Encoded 60/197 tokens (cache: 60)Encoded 70/197 tokens (cache: 70)Encoded 80/197 tokens (cache: 80)Encoded 90/197 tokens (cache: 90)Encoded 100/197 tokens (cache: 100)Encoded 110/197 tokens (cache: 110)Encoded 120/197 tokens (cache: 120)Encoded 130/197 tokens (cache: 130)Encoded 140/197 tokens (cache: 140)Encoded 150/197 tokens (cache: 150)Encoded 160/197 tokens (cache: 160)Encoded 170/197 tokens (cache: 170)Encoded 180/197 tokens (cache: 180)Encoded 190/197 tokens (cache: 190)
Encoding complete.
Original size: 599 chars
Compressed size: 32 bytes
Bits per character: 0.4274
Compression finished in 8.23 seconds.
Results saved to benchmarks.db
Results saved to model_results.json
  BPC: 0.4274 | Ratio: 18.7188

[8/35] LiquidAI 1.2B | generated_llama32_3b_3.py.txt
Loading model from /home/alexn/.cache/huggingface/hub/models--LiquidAI--LFM2.5-1.2B-Base/snapshots/1e601c5c9d33bcc8da794c253243d6b258a4d38b...
`torch_dtype` is deprecated! Use `dtype` instead!
2026-02-16 22:22:48.436507: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
/home/alexn/Documents/code/research/LLMzip_from_scratch/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:6105: UserWarning: expandable_segments not supported on this platform (Triggered internally at /pytorch/c10/hip/HIPAllocatorConfig.h:29.)
  _ = torch.empty(byte_count // factor, dtype=torch.float16, device=device, requires_grad=False)
Compressing /home/alexn/Documents/code/research/LLMzip_from_scratch/source_text/generated_llama32_3b_3.py.txt to /home/alexn/Documents/code/research/LLMzip_from_scratch/encoded_documents/selfcomp_LiquidAI_12B_generated_llama32_3b_3.py_w50.llmzip...
Total tokens to encode: 330
/home/alexn/Documents/code/research/LLMzip_from_scratch/.venv/lib/python3.12/site-packages/transformers/models/lfm2/modeling_lfm2.py:97: UserWarning: Attempting to use hipBLASLt on an unsupported architecture! Overriding blas backend to hipblas (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:296.)
  freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
Encoded 10/330 tokens (cache: 10)Encoded 20/330 tokens (cache: 20)Encoded 30/330 tokens (cache: 30)Encoded 40/330 tokens (cache: 40)Encoded 50/330 tokens (cache: 50)Encoded 60/330 tokens (cache: 60)Encoded 70/330 tokens (cache: 70)Encoded 80/330 tokens (cache: 80)Encoded 90/330 tokens (cache: 90)Encoded 100/330 tokens (cache: 100)Encoded 110/330 tokens (cache: 110)Encoded 120/330 tokens (cache: 120)Encoded 130/330 tokens (cache: 130)Encoded 140/330 tokens (cache: 140)Encoded 150/330 tokens (cache: 150)Encoded 160/330 tokens (cache: 160)Encoded 170/330 tokens (cache: 170)Encoded 180/330 tokens (cache: 180)Encoded 190/330 tokens (cache: 190)Encoded 200/330 tokens (cache: 200)Encoded 210/330 tokens (cache: 210)Encoded 220/330 tokens (cache: 220)Encoded 230/330 tokens (cache: 230)Encoded 240/330 tokens (cache: 240)Encoded 250/330 tokens (cache: 250)Encoded 260/330 tokens (cache: 260)Encoded 270/330 tokens (cache: 270)Encoded 280/330 tokens (cache: 280)Encoded 290/330 tokens (cache: 290)Encoded 300/330 tokens (cache: 300)Encoded 310/330 tokens (cache: 310)Encoded 320/330 tokens (cache: 320)
Encoding complete.
Original size: 1145 chars
Compressed size: 38 bytes
Bits per character: 0.2655
Compression finished in 13.31 seconds.
Results saved to benchmarks.db
Results saved to model_results.json
  BPC: 0.2655 | Ratio: 30.1316

[9/35] LiquidAI 1.2B | generated_llama32_3b_4.py.txt
Loading model from /home/alexn/.cache/huggingface/hub/models--LiquidAI--LFM2.5-1.2B-Base/snapshots/1e601c5c9d33bcc8da794c253243d6b258a4d38b...
`torch_dtype` is deprecated! Use `dtype` instead!
2026-02-16 22:23:09.982567: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
/home/alexn/Documents/code/research/LLMzip_from_scratch/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:6105: UserWarning: expandable_segments not supported on this platform (Triggered internally at /pytorch/c10/hip/HIPAllocatorConfig.h:29.)
  _ = torch.empty(byte_count // factor, dtype=torch.float16, device=device, requires_grad=False)
Compressing /home/alexn/Documents/code/research/LLMzip_from_scratch/source_text/generated_llama32_3b_4.py.txt to /home/alexn/Documents/code/research/LLMzip_from_scratch/encoded_documents/selfcomp_LiquidAI_12B_generated_llama32_3b_4.py_w50.llmzip...
Total tokens to encode: 247
/home/alexn/Documents/code/research/LLMzip_from_scratch/.venv/lib/python3.12/site-packages/transformers/models/lfm2/modeling_lfm2.py:97: UserWarning: Attempting to use hipBLASLt on an unsupported architecture! Overriding blas backend to hipblas (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:296.)
  freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
Encoded 10/247 tokens (cache: 10)Encoded 20/247 tokens (cache: 20)Encoded 30/247 tokens (cache: 30)Encoded 40/247 tokens (cache: 40)Encoded 50/247 tokens (cache: 50)Encoded 60/247 tokens (cache: 60)Encoded 70/247 tokens (cache: 70)Encoded 80/247 tokens (cache: 80)Encoded 90/247 tokens (cache: 90)Encoded 100/247 tokens (cache: 100)Encoded 110/247 tokens (cache: 110)Encoded 120/247 tokens (cache: 120)Encoded 130/247 tokens (cache: 130)Encoded 140/247 tokens (cache: 140)Encoded 150/247 tokens (cache: 150)Encoded 160/247 tokens (cache: 160)Encoded 170/247 tokens (cache: 170)Encoded 180/247 tokens (cache: 180)Encoded 190/247 tokens (cache: 190)Encoded 200/247 tokens (cache: 200)Encoded 210/247 tokens (cache: 210)Encoded 220/247 tokens (cache: 220)Encoded 230/247 tokens (cache: 230)Encoded 240/247 tokens (cache: 240)
Encoding complete.
Original size: 717 chars
Compressed size: 26 bytes
Bits per character: 0.2901
Compression finished in 10.16 seconds.
Results saved to benchmarks.db
Results saved to model_results.json
  BPC: 0.2901 | Ratio: 27.5769

[10/35] LiquidAI 1.2B | generated_llama32_3b_5.py.txt
Loading model from /home/alexn/.cache/huggingface/hub/models--LiquidAI--LFM2.5-1.2B-Base/snapshots/1e601c5c9d33bcc8da794c253243d6b258a4d38b...
`torch_dtype` is deprecated! Use `dtype` instead!
2026-02-16 22:23:28.017176: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
/home/alexn/Documents/code/research/LLMzip_from_scratch/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:6105: UserWarning: expandable_segments not supported on this platform (Triggered internally at /pytorch/c10/hip/HIPAllocatorConfig.h:29.)
  _ = torch.empty(byte_count // factor, dtype=torch.float16, device=device, requires_grad=False)
Compressing /home/alexn/Documents/code/research/LLMzip_from_scratch/source_text/generated_llama32_3b_5.py.txt to /home/alexn/Documents/code/research/LLMzip_from_scratch/encoded_documents/selfcomp_LiquidAI_12B_generated_llama32_3b_5.py_w50.llmzip...
Total tokens to encode: 491
/home/alexn/Documents/code/research/LLMzip_from_scratch/.venv/lib/python3.12/site-packages/transformers/models/lfm2/modeling_lfm2.py:97: UserWarning: Attempting to use hipBLASLt on an unsupported architecture! Overriding blas backend to hipblas (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:296.)
  freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
Encoded 10/491 tokens (cache: 10)Encoded 20/491 tokens (cache: 20)Encoded 30/491 tokens (cache: 30)Encoded 40/491 tokens (cache: 40)Encoded 50/491 tokens (cache: 50)Encoded 60/491 tokens (cache: 60)Encoded 70/491 tokens (cache: 70)Encoded 80/491 tokens (cache: 80)Encoded 90/491 tokens (cache: 90)Encoded 100/491 tokens (cache: 100)Encoded 110/491 tokens (cache: 110)Encoded 120/491 tokens (cache: 120)Encoded 130/491 tokens (cache: 130)Encoded 140/491 tokens (cache: 140)Encoded 150/491 tokens (cache: 150)Encoded 160/491 tokens (cache: 160)Encoded 170/491 tokens (cache: 170)Encoded 180/491 tokens (cache: 180)Encoded 190/491 tokens (cache: 190)Encoded 200/491 tokens (cache: 200)Encoded 210/491 tokens (cache: 210)Encoded 220/491 tokens (cache: 220)Encoded 230/491 tokens (cache: 230)Encoded 240/491 tokens (cache: 240)Encoded 250/491 tokens (cache: 250)Encoded 260/491 tokens (cache: 260)Encoded 270/491 tokens (cache: 270)Encoded 280/491 tokens (cache: 280)Encoded 290/491 tokens (cache: 290)Encoded 300/491 tokens (cache: 300)Encoded 310/491 tokens (cache: 310)Encoded 320/491 tokens (cache: 320)Encoded 330/491 tokens (cache: 330)Encoded 340/491 tokens (cache: 340)Encoded 350/491 tokens (cache: 350)Encoded 360/491 tokens (cache: 360)Encoded 370/491 tokens (cache: 370)Encoded 380/491 tokens (cache: 380)Encoded 390/491 tokens (cache: 390)Encoded 400/491 tokens (cache: 400)Encoded 410/491 tokens (cache: 410)Encoded 420/491 tokens (cache: 420)Encoded 430/491 tokens (cache: 430)Encoded 440/491 tokens (cache: 440)Encoded 450/491 tokens (cache: 450)Encoded 460/491 tokens (cache: 460)Encoded 470/491 tokens (cache: 470)Encoded 480/491 tokens (cache: 480)Encoded 490/491 tokens (cache: 490)
Encoding complete.
Original size: 2055 chars
Compressed size: 50 bytes
Bits per character: 0.1946
Compression finished in 19.64 seconds.
Results saved to benchmarks.db
Results saved to model_results.json
  BPC: 0.1946 | Ratio: 41.1000

[11/35] Gemma 3-1B | generated_llama32_3b_1.py.txt
Loading model from /home/alexn/.cache/huggingface/hub/models--google--gemma-3-1b-pt/snapshots/fcf18a2a879aab110ca39f8bffbccd5d49d8eb29...
`torch_dtype` is deprecated! Use `dtype` instead!
2026-02-16 22:23:57.375575: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
/home/alexn/Documents/code/research/LLMzip_from_scratch/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:6105: UserWarning: expandable_segments not supported on this platform (Triggered internally at /pytorch/c10/hip/HIPAllocatorConfig.h:29.)
  _ = torch.empty(byte_count // factor, dtype=torch.float16, device=device, requires_grad=False)
Compressing /home/alexn/Documents/code/research/LLMzip_from_scratch/source_text/generated_llama32_3b_1.py.txt to /home/alexn/Documents/code/research/LLMzip_from_scratch/encoded_documents/selfcomp_Gemma_31B_generated_llama32_3b_1.py_w50.llmzip...
Total tokens to encode: 555
/home/alexn/Documents/code/research/LLMzip_from_scratch/.venv/lib/python3.12/site-packages/transformers/models/gemma3/modeling_gemma3.py:174: UserWarning: Attempting to use hipBLASLt on an unsupported architecture! Overriding blas backend to hipblas (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:296.)
  freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
Encoded 10/555 tokens (cache: 10)Encoded 20/555 tokens (cache: 20)Encoded 30/555 tokens (cache: 30)Encoded 40/555 tokens (cache: 40)Encoded 50/555 tokens (cache: 50)Encoded 60/555 tokens (cache: 60)Encoded 70/555 tokens (cache: 70)Encoded 80/555 tokens (cache: 80)Encoded 90/555 tokens (cache: 90)Encoded 100/555 tokens (cache: 100)Encoded 110/555 tokens (cache: 110)Encoded 120/555 tokens (cache: 120)Encoded 130/555 tokens (cache: 130)Encoded 140/555 tokens (cache: 140)Encoded 150/555 tokens (cache: 150)Encoded 160/555 tokens (cache: 160)Encoded 170/555 tokens (cache: 170)Encoded 180/555 tokens (cache: 180)Encoded 190/555 tokens (cache: 190)Encoded 200/555 tokens (cache: 200)Encoded 210/555 tokens (cache: 210)Encoded 220/555 tokens (cache: 220)Encoded 230/555 tokens (cache: 230)Encoded 240/555 tokens (cache: 240)Encoded 250/555 tokens (cache: 250)Encoded 260/555 tokens (cache: 260)Encoded 270/555 tokens (cache: 270)Encoded 280/555 tokens (cache: 280)Encoded 290/555 tokens (cache: 290)Encoded 300/555 tokens (cache: 300)Encoded 310/555 tokens (cache: 310)Encoded 320/555 tokens (cache: 320)Encoded 330/555 tokens (cache: 330)Encoded 340/555 tokens (cache: 340)Encoded 350/555 tokens (cache: 350)Encoded 360/555 tokens (cache: 360)Encoded 370/555 tokens (cache: 370)Encoded 380/555 tokens (cache: 380)Encoded 390/555 tokens (cache: 390)Encoded 400/555 tokens (cache: 400)Encoded 410/555 tokens (cache: 410)Encoded 420/555 tokens (cache: 420)Encoded 430/555 tokens (cache: 430)Encoded 440/555 tokens (cache: 440)Encoded 450/555 tokens (cache: 450)Encoded 460/555 tokens (cache: 460)Encoded 470/555 tokens (cache: 470)Encoded 480/555 tokens (cache: 480)Encoded 490/555 tokens (cache: 490)Encoded 500/555 tokens (cache: 500)Encoded 510/555 tokens (cache: 510)Encoded 520/555 tokens (cache: 520)Encoded 530/555 tokens (cache: 530)Encoded 540/555 tokens (cache: 540)Encoded 550/555 tokens (cache: 550)
Encoding complete.
Original size: 1907 chars
Compressed size: 53 bytes
Bits per character: 0.2223
Compression finished in 73.35 seconds.
Results saved to benchmarks.db
Results saved to model_results.json
  BPC: 0.2223 | Ratio: 35.9811

[12/35] Gemma 3-1B | generated_llama32_3b_2.py.txt
Loading model from /home/alexn/.cache/huggingface/hub/models--google--gemma-3-1b-pt/snapshots/fcf18a2a879aab110ca39f8bffbccd5d49d8eb29...
`torch_dtype` is deprecated! Use `dtype` instead!
2026-02-16 22:25:25.844553: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
/home/alexn/Documents/code/research/LLMzip_from_scratch/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:6105: UserWarning: expandable_segments not supported on this platform (Triggered internally at /pytorch/c10/hip/HIPAllocatorConfig.h:29.)
  _ = torch.empty(byte_count // factor, dtype=torch.float16, device=device, requires_grad=False)
Compressing /home/alexn/Documents/code/research/LLMzip_from_scratch/source_text/generated_llama32_3b_2.py.txt to /home/alexn/Documents/code/research/LLMzip_from_scratch/encoded_documents/selfcomp_Gemma_31B_generated_llama32_3b_2.py_w50.llmzip...
Total tokens to encode: 205
/home/alexn/Documents/code/research/LLMzip_from_scratch/.venv/lib/python3.12/site-packages/transformers/models/gemma3/modeling_gemma3.py:174: UserWarning: Attempting to use hipBLASLt on an unsupported architecture! Overriding blas backend to hipblas (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:296.)
  freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
Encoded 10/205 tokens (cache: 10)Encoded 20/205 tokens (cache: 20)Encoded 30/205 tokens (cache: 30)Encoded 40/205 tokens (cache: 40)Encoded 50/205 tokens (cache: 50)Encoded 60/205 tokens (cache: 60)Encoded 70/205 tokens (cache: 70)Encoded 80/205 tokens (cache: 80)Encoded 90/205 tokens (cache: 90)Encoded 100/205 tokens (cache: 100)Encoded 110/205 tokens (cache: 110)Encoded 120/205 tokens (cache: 120)Encoded 130/205 tokens (cache: 130)Encoded 140/205 tokens (cache: 140)Encoded 150/205 tokens (cache: 150)Encoded 160/205 tokens (cache: 160)Encoded 170/205 tokens (cache: 170)Encoded 180/205 tokens (cache: 180)Encoded 190/205 tokens (cache: 190)Encoded 200/205 tokens (cache: 200)
Encoding complete.
Original size: 599 chars
Compressed size: 34 bytes
Bits per character: 0.4541
Compression finished in 27.11 seconds.
Results saved to benchmarks.db
Results saved to model_results.json
  BPC: 0.4541 | Ratio: 17.6176

[13/35] Gemma 3-1B | generated_llama32_3b_3.py.txt
Loading model from /home/alexn/.cache/huggingface/hub/models--google--gemma-3-1b-pt/snapshots/fcf18a2a879aab110ca39f8bffbccd5d49d8eb29...
`torch_dtype` is deprecated! Use `dtype` instead!
2026-02-16 22:26:03.286279: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
/home/alexn/Documents/code/research/LLMzip_from_scratch/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:6105: UserWarning: expandable_segments not supported on this platform (Triggered internally at /pytorch/c10/hip/HIPAllocatorConfig.h:29.)
  _ = torch.empty(byte_count // factor, dtype=torch.float16, device=device, requires_grad=False)
Compressing /home/alexn/Documents/code/research/LLMzip_from_scratch/source_text/generated_llama32_3b_3.py.txt to /home/alexn/Documents/code/research/LLMzip_from_scratch/encoded_documents/selfcomp_Gemma_31B_generated_llama32_3b_3.py_w50.llmzip...
Total tokens to encode: 366
/home/alexn/Documents/code/research/LLMzip_from_scratch/.venv/lib/python3.12/site-packages/transformers/models/gemma3/modeling_gemma3.py:174: UserWarning: Attempting to use hipBLASLt on an unsupported architecture! Overriding blas backend to hipblas (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:296.)
  freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
Encoded 10/366 tokens (cache: 10)Encoded 20/366 tokens (cache: 20)Encoded 30/366 tokens (cache: 30)Encoded 40/366 tokens (cache: 40)Encoded 50/366 tokens (cache: 50)Encoded 60/366 tokens (cache: 60)Encoded 70/366 tokens (cache: 70)Encoded 80/366 tokens (cache: 80)Encoded 90/366 tokens (cache: 90)Encoded 100/366 tokens (cache: 100)Encoded 110/366 tokens (cache: 110)Encoded 120/366 tokens (cache: 120)Encoded 130/366 tokens (cache: 130)Encoded 140/366 tokens (cache: 140)Encoded 150/366 tokens (cache: 150)Encoded 160/366 tokens (cache: 160)Encoded 170/366 tokens (cache: 170)Encoded 180/366 tokens (cache: 180)Encoded 190/366 tokens (cache: 190)Encoded 200/366 tokens (cache: 200)Encoded 210/366 tokens (cache: 210)Encoded 220/366 tokens (cache: 220)Encoded 230/366 tokens (cache: 230)Encoded 240/366 tokens (cache: 240)Encoded 250/366 tokens (cache: 250)Encoded 260/366 tokens (cache: 260)Encoded 270/366 tokens (cache: 270)Encoded 280/366 tokens (cache: 280)Encoded 290/366 tokens (cache: 290)Encoded 300/366 tokens (cache: 300)Encoded 310/366 tokens (cache: 310)Encoded 320/366 tokens (cache: 320)Encoded 330/366 tokens (cache: 330)Encoded 340/366 tokens (cache: 340)Encoded 350/366 tokens (cache: 350)Encoded 360/366 tokens (cache: 360)
Encoding complete.
Original size: 1145 chars
Compressed size: 48 bytes
Bits per character: 0.3354
Compression finished in 48.25 seconds.
Results saved to benchmarks.db
Results saved to model_results.json
  BPC: 0.3354 | Ratio: 23.8542

[14/35] Gemma 3-1B | generated_llama32_3b_4.py.txt
Loading model from /home/alexn/.cache/huggingface/hub/models--google--gemma-3-1b-pt/snapshots/fcf18a2a879aab110ca39f8bffbccd5d49d8eb29...
`torch_dtype` is deprecated! Use `dtype` instead!
2026-02-16 22:27:01.813420: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
/home/alexn/Documents/code/research/LLMzip_from_scratch/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:6105: UserWarning: expandable_segments not supported on this platform (Triggered internally at /pytorch/c10/hip/HIPAllocatorConfig.h:29.)
  _ = torch.empty(byte_count // factor, dtype=torch.float16, device=device, requires_grad=False)
Compressing /home/alexn/Documents/code/research/LLMzip_from_scratch/source_text/generated_llama32_3b_4.py.txt to /home/alexn/Documents/code/research/LLMzip_from_scratch/encoded_documents/selfcomp_Gemma_31B_generated_llama32_3b_4.py_w50.llmzip...
Total tokens to encode: 253
/home/alexn/Documents/code/research/LLMzip_from_scratch/.venv/lib/python3.12/site-packages/transformers/models/gemma3/modeling_gemma3.py:174: UserWarning: Attempting to use hipBLASLt on an unsupported architecture! Overriding blas backend to hipblas (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:296.)
  freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
Encoded 10/253 tokens (cache: 10)Encoded 20/253 tokens (cache: 20)Encoded 30/253 tokens (cache: 30)Encoded 40/253 tokens (cache: 40)Encoded 50/253 tokens (cache: 50)Encoded 60/253 tokens (cache: 60)Encoded 70/253 tokens (cache: 70)Encoded 80/253 tokens (cache: 80)Encoded 90/253 tokens (cache: 90)Encoded 100/253 tokens (cache: 100)Encoded 110/253 tokens (cache: 110)Encoded 120/253 tokens (cache: 120)Encoded 130/253 tokens (cache: 130)Encoded 140/253 tokens (cache: 140)Encoded 150/253 tokens (cache: 150)Encoded 160/253 tokens (cache: 160)Encoded 170/253 tokens (cache: 170)Encoded 180/253 tokens (cache: 180)Encoded 190/253 tokens (cache: 190)Encoded 200/253 tokens (cache: 200)Encoded 210/253 tokens (cache: 210)Encoded 220/253 tokens (cache: 220)Encoded 230/253 tokens (cache: 230)Encoded 240/253 tokens (cache: 240)Encoded 250/253 tokens (cache: 250)
Encoding complete.
Original size: 717 chars
Compressed size: 40 bytes
Bits per character: 0.4463
Compression finished in 33.38 seconds.
Results saved to benchmarks.db
Results saved to model_results.json
  BPC: 0.4463 | Ratio: 17.9250

[15/35] Gemma 3-1B | generated_llama32_3b_5.py.txt
Loading model from /home/alexn/.cache/huggingface/hub/models--google--gemma-3-1b-pt/snapshots/fcf18a2a879aab110ca39f8bffbccd5d49d8eb29...
`torch_dtype` is deprecated! Use `dtype` instead!
2026-02-16 22:27:45.751238: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
/home/alexn/Documents/code/research/LLMzip_from_scratch/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:6105: UserWarning: expandable_segments not supported on this platform (Triggered internally at /pytorch/c10/hip/HIPAllocatorConfig.h:29.)
  _ = torch.empty(byte_count // factor, dtype=torch.float16, device=device, requires_grad=False)
Compressing /home/alexn/Documents/code/research/LLMzip_from_scratch/source_text/generated_llama32_3b_5.py.txt to /home/alexn/Documents/code/research/LLMzip_from_scratch/encoded_documents/selfcomp_Gemma_31B_generated_llama32_3b_5.py_w50.llmzip...
Total tokens to encode: 603
/home/alexn/Documents/code/research/LLMzip_from_scratch/.venv/lib/python3.12/site-packages/transformers/models/gemma3/modeling_gemma3.py:174: UserWarning: Attempting to use hipBLASLt on an unsupported architecture! Overriding blas backend to hipblas (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:296.)
  freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
Encoded 10/603 tokens (cache: 10)Encoded 20/603 tokens (cache: 20)Encoded 30/603 tokens (cache: 30)Encoded 40/603 tokens (cache: 40)Encoded 50/603 tokens (cache: 50)Encoded 60/603 tokens (cache: 60)Encoded 70/603 tokens (cache: 70)Encoded 80/603 tokens (cache: 80)Encoded 90/603 tokens (cache: 90)Encoded 100/603 tokens (cache: 100)Encoded 110/603 tokens (cache: 110)Encoded 120/603 tokens (cache: 120)Encoded 130/603 tokens (cache: 130)Encoded 140/603 tokens (cache: 140)Encoded 150/603 tokens (cache: 150)Encoded 160/603 tokens (cache: 160)Encoded 170/603 tokens (cache: 170)Encoded 180/603 tokens (cache: 180)Encoded 190/603 tokens (cache: 190)Encoded 200/603 tokens (cache: 200)Encoded 210/603 tokens (cache: 210)Encoded 220/603 tokens (cache: 220)Encoded 230/603 tokens (cache: 230)Encoded 240/603 tokens (cache: 240)Encoded 250/603 tokens (cache: 250)Encoded 260/603 tokens (cache: 260)Encoded 270/603 tokens (cache: 270)Encoded 280/603 tokens (cache: 280)Encoded 290/603 tokens (cache: 290)Encoded 300/603 tokens (cache: 300)Encoded 310/603 tokens (cache: 310)Encoded 320/603 tokens (cache: 320)Encoded 330/603 tokens (cache: 330)Encoded 340/603 tokens (cache: 340)Encoded 350/603 tokens (cache: 350)Encoded 360/603 tokens (cache: 360)Encoded 370/603 tokens (cache: 370)Encoded 380/603 tokens (cache: 380)Encoded 390/603 tokens (cache: 390)Encoded 400/603 tokens (cache: 400)Encoded 410/603 tokens (cache: 410)Encoded 420/603 tokens (cache: 420)Encoded 430/603 tokens (cache: 430)Encoded 440/603 tokens (cache: 440)Encoded 450/603 tokens (cache: 450)Encoded 460/603 tokens (cache: 460)Encoded 470/603 tokens (cache: 470)Encoded 480/603 tokens (cache: 480)Encoded 490/603 tokens (cache: 490)Encoded 500/603 tokens (cache: 500)Encoded 510/603 tokens (cache: 510)Encoded 520/603 tokens (cache: 520)Encoded 530/603 tokens (cache: 530)Encoded 540/603 tokens (cache: 540)Encoded 550/603 tokens (cache: 550)Encoded 560/603 tokens (cache: 560)Encoded 570/603 tokens (cache: 570)Encoded 580/603 tokens (cache: 580)Encoded 590/603 tokens (cache: 590)Encoded 600/603 tokens (cache: 600)
Encoding complete.
Original size: 2055 chars
Compressed size: 63 bytes
Bits per character: 0.2453
Compression finished in 79.62 seconds.
Results saved to benchmarks.db
Results saved to model_results.json
  BPC: 0.2453 | Ratio: 32.6190

[16/35] Gemma 3-4B | generated_llama32_3b_1.py.txt
Loading model from /home/alexn/.cache/huggingface/hub/models--google--gemma-3-4b-pt/snapshots/cc012e0a6d0787b4adcc0fa2c4da74402494554d...
`torch_dtype` is deprecated! Use `dtype` instead!
2026-02-16 22:29:15.886886: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
/home/alexn/Documents/code/research/LLMzip_from_scratch/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:6105: UserWarning: expandable_segments not supported on this platform (Triggered internally at /pytorch/c10/hip/HIPAllocatorConfig.h:29.)
  _ = torch.empty(byte_count // factor, dtype=torch.float16, device=device, requires_grad=False)
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:34<00:34, 34.49s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:47<00:00, 21.72s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:47<00:00, 23.63s/it]
Compressing /home/alexn/Documents/code/research/LLMzip_from_scratch/source_text/generated_llama32_3b_1.py.txt to /home/alexn/Documents/code/research/LLMzip_from_scratch/encoded_documents/selfcomp_Gemma_34B_generated_llama32_3b_1.py_w50.llmzip...
Total tokens to encode: 555
/home/alexn/Documents/code/research/LLMzip_from_scratch/.venv/lib/python3.12/site-packages/transformers/models/gemma3/modeling_gemma3.py:174: UserWarning: Attempting to use hipBLASLt on an unsupported architecture! Overriding blas backend to hipblas (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:296.)
  freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
Encoded 10/555 tokens (cache: 10)Encoded 20/555 tokens (cache: 20)Encoded 30/555 tokens (cache: 30)Encoded 40/555 tokens (cache: 40)Encoded 50/555 tokens (cache: 50)Encoded 60/555 tokens (cache: 60)Encoded 70/555 tokens (cache: 70)Encoded 80/555 tokens (cache: 80)Encoded 90/555 tokens (cache: 90)Encoded 100/555 tokens (cache: 100)Encoded 110/555 tokens (cache: 110)Encoded 120/555 tokens (cache: 120)Encoded 130/555 tokens (cache: 130)Encoded 140/555 tokens (cache: 140)Encoded 150/555 tokens (cache: 150)Encoded 160/555 tokens (cache: 160)Encoded 170/555 tokens (cache: 170)Encoded 180/555 tokens (cache: 180)Encoded 190/555 tokens (cache: 190)Encoded 200/555 tokens (cache: 200)Encoded 210/555 tokens (cache: 210)Encoded 220/555 tokens (cache: 220)Encoded 230/555 tokens (cache: 230)Encoded 240/555 tokens (cache: 240)Encoded 250/555 tokens (cache: 250)Encoded 260/555 tokens (cache: 260)Encoded 270/555 tokens (cache: 270)Encoded 280/555 tokens (cache: 280)Encoded 290/555 tokens (cache: 290)Encoded 300/555 tokens (cache: 300)Encoded 310/555 tokens (cache: 310)Encoded 320/555 tokens (cache: 320)Encoded 330/555 tokens (cache: 330)Encoded 340/555 tokens (cache: 340)Encoded 350/555 tokens (cache: 350)Encoded 360/555 tokens (cache: 360)Encoded 370/555 tokens (cache: 370)Encoded 380/555 tokens (cache: 380)Encoded 390/555 tokens (cache: 390)Encoded 400/555 tokens (cache: 400)Encoded 410/555 tokens (cache: 410)Encoded 420/555 tokens (cache: 420)Encoded 430/555 tokens (cache: 430)Encoded 440/555 tokens (cache: 440)Encoded 450/555 tokens (cache: 450)Encoded 460/555 tokens (cache: 460)Encoded 470/555 tokens (cache: 470)Encoded 480/555 tokens (cache: 480)Encoded 490/555 tokens (cache: 490)Encoded 500/555 tokens (cache: 500)Encoded 510/555 tokens (cache: 510)Encoded 520/555 tokens (cache: 520)Encoded 530/555 tokens (cache: 530)Encoded 540/555 tokens (cache: 540)Encoded 550/555 tokens (cache: 550)
Encoding complete.
Original size: 1907 chars
Compressed size: 37 bytes
Bits per character: 0.1552
Compression finished in 225.47 seconds.
Results saved to benchmarks.db
Results saved to model_results.json
  BPC: 0.1552 | Ratio: 51.5405

[17/35] Gemma 3-4B | generated_llama32_3b_2.py.txt
Loading model from /home/alexn/.cache/huggingface/hub/models--google--gemma-3-4b-pt/snapshots/cc012e0a6d0787b4adcc0fa2c4da74402494554d...
`torch_dtype` is deprecated! Use `dtype` instead!
2026-02-16 22:33:56.522721: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
/home/alexn/Documents/code/research/LLMzip_from_scratch/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:6105: UserWarning: expandable_segments not supported on this platform (Triggered internally at /pytorch/c10/hip/HIPAllocatorConfig.h:29.)
  _ = torch.empty(byte_count // factor, dtype=torch.float16, device=device, requires_grad=False)
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.03s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  4.99s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  5.15s/it]
Compressing /home/alexn/Documents/code/research/LLMzip_from_scratch/source_text/generated_llama32_3b_2.py.txt to /home/alexn/Documents/code/research/LLMzip_from_scratch/encoded_documents/selfcomp_Gemma_34B_generated_llama32_3b_2.py_w50.llmzip...
Total tokens to encode: 205
/home/alexn/Documents/code/research/LLMzip_from_scratch/.venv/lib/python3.12/site-packages/transformers/models/gemma3/modeling_gemma3.py:174: UserWarning: Attempting to use hipBLASLt on an unsupported architecture! Overriding blas backend to hipblas (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:296.)
  freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
Encoded 10/205 tokens (cache: 10)Encoded 20/205 tokens (cache: 20)Encoded 30/205 tokens (cache: 30)Encoded 40/205 tokens (cache: 40)Encoded 50/205 tokens (cache: 50)Encoded 60/205 tokens (cache: 60)Encoded 70/205 tokens (cache: 70)Encoded 80/205 tokens (cache: 80)Encoded 90/205 tokens (cache: 90)Encoded 100/205 tokens (cache: 100)Encoded 110/205 tokens (cache: 110)Encoded 120/205 tokens (cache: 120)Encoded 130/205 tokens (cache: 130)Encoded 140/205 tokens (cache: 140)Encoded 150/205 tokens (cache: 150)Encoded 160/205 tokens (cache: 160)Encoded 170/205 tokens (cache: 170)Encoded 180/205 tokens (cache: 180)Encoded 190/205 tokens (cache: 190)Encoded 200/205 tokens (cache: 200)
Encoding complete.
Original size: 599 chars
Compressed size: 30 bytes
Bits per character: 0.4007
Compression finished in 83.16 seconds.
Results saved to benchmarks.db
Results saved to model_results.json
  BPC: 0.4007 | Ratio: 19.9667

[18/35] Gemma 3-4B | generated_llama32_3b_3.py.txt
Loading model from /home/alexn/.cache/huggingface/hub/models--google--gemma-3-4b-pt/snapshots/cc012e0a6d0787b4adcc0fa2c4da74402494554d...
`torch_dtype` is deprecated! Use `dtype` instead!
2026-02-16 22:35:39.793294: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
/home/alexn/Documents/code/research/LLMzip_from_scratch/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:6105: UserWarning: expandable_segments not supported on this platform (Triggered internally at /pytorch/c10/hip/HIPAllocatorConfig.h:29.)
  _ = torch.empty(byte_count // factor, dtype=torch.float16, device=device, requires_grad=False)
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.87s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  5.09s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  5.21s/it]
Compressing /home/alexn/Documents/code/research/LLMzip_from_scratch/source_text/generated_llama32_3b_3.py.txt to /home/alexn/Documents/code/research/LLMzip_from_scratch/encoded_documents/selfcomp_Gemma_34B_generated_llama32_3b_3.py_w50.llmzip...
Total tokens to encode: 366
/home/alexn/Documents/code/research/LLMzip_from_scratch/.venv/lib/python3.12/site-packages/transformers/models/gemma3/modeling_gemma3.py:174: UserWarning: Attempting to use hipBLASLt on an unsupported architecture! Overriding blas backend to hipblas (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:296.)
  freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
Encoded 10/366 tokens (cache: 10)Encoded 20/366 tokens (cache: 20)Encoded 30/366 tokens (cache: 30)Encoded 40/366 tokens (cache: 40)Encoded 50/366 tokens (cache: 50)Encoded 60/366 tokens (cache: 60)Encoded 70/366 tokens (cache: 70)Encoded 80/366 tokens (cache: 80)Encoded 90/366 tokens (cache: 90)Encoded 100/366 tokens (cache: 100)Encoded 110/366 tokens (cache: 110)Encoded 120/366 tokens (cache: 120)Encoded 130/366 tokens (cache: 130)Encoded 140/366 tokens (cache: 140)Encoded 150/366 tokens (cache: 150)Encoded 160/366 tokens (cache: 160)Encoded 170/366 tokens (cache: 170)Encoded 180/366 tokens (cache: 180)Encoded 190/366 tokens (cache: 190)Encoded 200/366 tokens (cache: 200)Encoded 210/366 tokens (cache: 210)Encoded 220/366 tokens (cache: 220)Encoded 230/366 tokens (cache: 230)Encoded 240/366 tokens (cache: 240)Encoded 250/366 tokens (cache: 250)Encoded 260/366 tokens (cache: 260)Encoded 270/366 tokens (cache: 270)Encoded 280/366 tokens (cache: 280)Encoded 290/366 tokens (cache: 290)Encoded 300/366 tokens (cache: 300)Encoded 310/366 tokens (cache: 310)Encoded 320/366 tokens (cache: 320)Encoded 330/366 tokens (cache: 330)Encoded 340/366 tokens (cache: 340)Encoded 350/366 tokens (cache: 350)Encoded 360/366 tokens (cache: 360)
Encoding complete.
Original size: 1145 chars
Compressed size: 37 bytes
Bits per character: 0.2585
Compression finished in 148.36 seconds.
Results saved to benchmarks.db
Results saved to model_results.json
  BPC: 0.2585 | Ratio: 30.9459

[19/35] Gemma 3-4B | generated_llama32_3b_4.py.txt
Loading model from /home/alexn/.cache/huggingface/hub/models--google--gemma-3-4b-pt/snapshots/cc012e0a6d0787b4adcc0fa2c4da74402494554d...
`torch_dtype` is deprecated! Use `dtype` instead!
2026-02-16 22:38:26.501585: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
/home/alexn/Documents/code/research/LLMzip_from_scratch/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:6105: UserWarning: expandable_segments not supported on this platform (Triggered internally at /pytorch/c10/hip/HIPAllocatorConfig.h:29.)
  _ = torch.empty(byte_count // factor, dtype=torch.float16, device=device, requires_grad=False)
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.02s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  5.11s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  5.25s/it]
Compressing /home/alexn/Documents/code/research/LLMzip_from_scratch/source_text/generated_llama32_3b_4.py.txt to /home/alexn/Documents/code/research/LLMzip_from_scratch/encoded_documents/selfcomp_Gemma_34B_generated_llama32_3b_4.py_w50.llmzip...
Total tokens to encode: 253
/home/alexn/Documents/code/research/LLMzip_from_scratch/.venv/lib/python3.12/site-packages/transformers/models/gemma3/modeling_gemma3.py:174: UserWarning: Attempting to use hipBLASLt on an unsupported architecture! Overriding blas backend to hipblas (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:296.)
  freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
Encoded 10/253 tokens (cache: 10)Encoded 20/253 tokens (cache: 20)Encoded 30/253 tokens (cache: 30)Encoded 40/253 tokens (cache: 40)Encoded 50/253 tokens (cache: 50)Encoded 60/253 tokens (cache: 60)Encoded 70/253 tokens (cache: 70)Encoded 80/253 tokens (cache: 80)Encoded 90/253 tokens (cache: 90)Encoded 100/253 tokens (cache: 100)Encoded 110/253 tokens (cache: 110)Encoded 120/253 tokens (cache: 120)Encoded 130/253 tokens (cache: 130)Encoded 140/253 tokens (cache: 140)Encoded 150/253 tokens (cache: 150)Encoded 160/253 tokens (cache: 160)Encoded 170/253 tokens (cache: 170)Encoded 180/253 tokens (cache: 180)Encoded 190/253 tokens (cache: 190)Encoded 200/253 tokens (cache: 200)Encoded 210/253 tokens (cache: 210)Encoded 220/253 tokens (cache: 220)Encoded 230/253 tokens (cache: 230)Encoded 240/253 tokens (cache: 240)Encoded 250/253 tokens (cache: 250)
Encoding complete.
Original size: 717 chars
Compressed size: 25 bytes
Bits per character: 0.2789
Compression finished in 102.45 seconds.
Results saved to benchmarks.db
Results saved to model_results.json
  BPC: 0.2789 | Ratio: 28.6800

[20/35] Gemma 3-4B | generated_llama32_3b_5.py.txt
Loading model from /home/alexn/.cache/huggingface/hub/models--google--gemma-3-4b-pt/snapshots/cc012e0a6d0787b4adcc0fa2c4da74402494554d...
`torch_dtype` is deprecated! Use `dtype` instead!
2026-02-16 22:40:27.217634: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
/home/alexn/Documents/code/research/LLMzip_from_scratch/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:6105: UserWarning: expandable_segments not supported on this platform (Triggered internally at /pytorch/c10/hip/HIPAllocatorConfig.h:29.)
  _ = torch.empty(byte_count // factor, dtype=torch.float16, device=device, requires_grad=False)
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  6.00s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  5.00s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  5.15s/it]
Compressing /home/alexn/Documents/code/research/LLMzip_from_scratch/source_text/generated_llama32_3b_5.py.txt to /home/alexn/Documents/code/research/LLMzip_from_scratch/encoded_documents/selfcomp_Gemma_34B_generated_llama32_3b_5.py_w50.llmzip...
Total tokens to encode: 603
/home/alexn/Documents/code/research/LLMzip_from_scratch/.venv/lib/python3.12/site-packages/transformers/models/gemma3/modeling_gemma3.py:174: UserWarning: Attempting to use hipBLASLt on an unsupported architecture! Overriding blas backend to hipblas (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:296.)
  freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
Encoded 10/603 tokens (cache: 10)Encoded 20/603 tokens (cache: 20)Encoded 30/603 tokens (cache: 30)Encoded 40/603 tokens (cache: 40)Encoded 50/603 tokens (cache: 50)Encoded 60/603 tokens (cache: 60)Encoded 70/603 tokens (cache: 70)Encoded 80/603 tokens (cache: 80)Encoded 90/603 tokens (cache: 90)Encoded 100/603 tokens (cache: 100)Encoded 110/603 tokens (cache: 110)Encoded 120/603 tokens (cache: 120)Encoded 130/603 tokens (cache: 130)Encoded 140/603 tokens (cache: 140)Encoded 150/603 tokens (cache: 150)Encoded 160/603 tokens (cache: 160)Encoded 170/603 tokens (cache: 170)Encoded 180/603 tokens (cache: 180)Encoded 190/603 tokens (cache: 190)Encoded 200/603 tokens (cache: 200)Encoded 210/603 tokens (cache: 210)Encoded 220/603 tokens (cache: 220)Encoded 230/603 tokens (cache: 230)Encoded 240/603 tokens (cache: 240)Encoded 250/603 tokens (cache: 250)Encoded 260/603 tokens (cache: 260)Encoded 270/603 tokens (cache: 270)Encoded 280/603 tokens (cache: 280)Encoded 290/603 tokens (cache: 290)Encoded 300/603 tokens (cache: 300)Encoded 310/603 tokens (cache: 310)Encoded 320/603 tokens (cache: 320)Encoded 330/603 tokens (cache: 330)Encoded 340/603 tokens (cache: 340)Encoded 350/603 tokens (cache: 350)Encoded 360/603 tokens (cache: 360)Encoded 370/603 tokens (cache: 370)Encoded 380/603 tokens (cache: 380)Encoded 390/603 tokens (cache: 390)Encoded 400/603 tokens (cache: 400)Encoded 410/603 tokens (cache: 410)Encoded 420/603 tokens (cache: 420)Encoded 430/603 tokens (cache: 430)Encoded 440/603 tokens (cache: 440)Encoded 450/603 tokens (cache: 450)Encoded 460/603 tokens (cache: 460)Encoded 470/603 tokens (cache: 470)Encoded 480/603 tokens (cache: 480)Encoded 490/603 tokens (cache: 490)Encoded 500/603 tokens (cache: 500)Encoded 510/603 tokens (cache: 510)Encoded 520/603 tokens (cache: 520)Encoded 530/603 tokens (cache: 530)Encoded 540/603 tokens (cache: 540)Encoded 550/603 tokens (cache: 550)Encoded 560/603 tokens (cache: 560)Encoded 570/603 tokens (cache: 570)Encoded 580/603 tokens (cache: 580)Encoded 590/603 tokens (cache: 590)Encoded 600/603 tokens (cache: 600)
Encoding complete.
Original size: 2055 chars
Compressed size: 50 bytes
Bits per character: 0.1946
Compression finished in 245.14 seconds.
Results saved to benchmarks.db
Results saved to model_results.json
  BPC: 0.1946 | Ratio: 41.1000

[21/35] Qwen 3-1.7B | generated_llama32_3b_1.py.txt
Loading model from /home/alexn/.cache/huggingface/hub/models--Qwen--Qwen3-1.7B/snapshots/70d244cc86ccca08cf5af4e1e306ecf908b1ad5e...
`torch_dtype` is deprecated! Use `dtype` instead!
2026-02-16 22:44:49.713340: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
/home/alexn/Documents/code/research/LLMzip_from_scratch/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:6105: UserWarning: expandable_segments not supported on this platform (Triggered internally at /pytorch/c10/hip/HIPAllocatorConfig.h:29.)
  _ = torch.empty(byte_count // factor, dtype=torch.float16, device=device, requires_grad=False)
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.43s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.57s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.70s/it]
Compressing /home/alexn/Documents/code/research/LLMzip_from_scratch/source_text/generated_llama32_3b_1.py.txt to /home/alexn/Documents/code/research/LLMzip_from_scratch/encoded_documents/selfcomp_Qwen_317B_generated_llama32_3b_1.py_w50.llmzip...
Total tokens to encode: 422
/home/alexn/Documents/code/research/LLMzip_from_scratch/.venv/lib/python3.12/site-packages/transformers/models/qwen3/modeling_qwen3.py:327: UserWarning: Attempting to use hipBLASLt on an unsupported architecture! Overriding blas backend to hipblas (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:296.)
  freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
Encoded 10/422 tokens (cache: 10)Encoded 20/422 tokens (cache: 20)Encoded 30/422 tokens (cache: 30)Encoded 40/422 tokens (cache: 40)Encoded 50/422 tokens (cache: 50)Encoded 60/422 tokens (cache: 60)Encoded 70/422 tokens (cache: 70)Encoded 80/422 tokens (cache: 80)Encoded 90/422 tokens (cache: 90)Encoded 100/422 tokens (cache: 100)Encoded 110/422 tokens (cache: 110)Encoded 120/422 tokens (cache: 120)Encoded 130/422 tokens (cache: 130)Encoded 140/422 tokens (cache: 140)Encoded 150/422 tokens (cache: 150)Encoded 160/422 tokens (cache: 160)Encoded 170/422 tokens (cache: 170)Encoded 180/422 tokens (cache: 180)Encoded 190/422 tokens (cache: 190)Encoded 200/422 tokens (cache: 200)Encoded 210/422 tokens (cache: 210)Encoded 220/422 tokens (cache: 220)Encoded 230/422 tokens (cache: 230)Encoded 240/422 tokens (cache: 240)Encoded 250/422 tokens (cache: 250)Encoded 260/422 tokens (cache: 260)Encoded 270/422 tokens (cache: 270)Encoded 280/422 tokens (cache: 280)Encoded 290/422 tokens (cache: 290)Encoded 300/422 tokens (cache: 300)Encoded 310/422 tokens (cache: 310)Encoded 320/422 tokens (cache: 320)Encoded 330/422 tokens (cache: 330)Encoded 340/422 tokens (cache: 340)Encoded 350/422 tokens (cache: 350)Encoded 360/422 tokens (cache: 360)Encoded 370/422 tokens (cache: 370)Encoded 380/422 tokens (cache: 380)Encoded 390/422 tokens (cache: 390)Encoded 400/422 tokens (cache: 400)Encoded 410/422 tokens (cache: 410)Encoded 420/422 tokens (cache: 420)
Encoding complete.
Original size: 1907 chars
Compressed size: 37 bytes
Bits per character: 0.1552
Compression finished in 21.21 seconds.
Results saved to benchmarks.db
Results saved to model_results.json
  BPC: 0.1552 | Ratio: 51.5405

[22/35] Qwen 3-1.7B | generated_llama32_3b_2.py.txt
Loading model from /home/alexn/.cache/huggingface/hub/models--Qwen--Qwen3-1.7B/snapshots/70d244cc86ccca08cf5af4e1e306ecf908b1ad5e...
`torch_dtype` is deprecated! Use `dtype` instead!
2026-02-16 22:45:21.232166: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
/home/alexn/Documents/code/research/LLMzip_from_scratch/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:6105: UserWarning: expandable_segments not supported on this platform (Triggered internally at /pytorch/c10/hip/HIPAllocatorConfig.h:29.)
  _ = torch.empty(byte_count // factor, dtype=torch.float16, device=device, requires_grad=False)
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.61s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.03s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.12s/it]
Compressing /home/alexn/Documents/code/research/LLMzip_from_scratch/source_text/generated_llama32_3b_2.py.txt to /home/alexn/Documents/code/research/LLMzip_from_scratch/encoded_documents/selfcomp_Qwen_317B_generated_llama32_3b_2.py_w50.llmzip...
Total tokens to encode: 171
/home/alexn/Documents/code/research/LLMzip_from_scratch/.venv/lib/python3.12/site-packages/transformers/models/qwen3/modeling_qwen3.py:327: UserWarning: Attempting to use hipBLASLt on an unsupported architecture! Overriding blas backend to hipblas (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:296.)
  freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
Encoded 10/171 tokens (cache: 10)Encoded 20/171 tokens (cache: 20)Encoded 30/171 tokens (cache: 30)Encoded 40/171 tokens (cache: 40)Encoded 50/171 tokens (cache: 50)Encoded 60/171 tokens (cache: 60)Encoded 70/171 tokens (cache: 70)Encoded 80/171 tokens (cache: 80)Encoded 90/171 tokens (cache: 90)Encoded 100/171 tokens (cache: 100)Encoded 110/171 tokens (cache: 110)Encoded 120/171 tokens (cache: 120)Encoded 130/171 tokens (cache: 130)Encoded 140/171 tokens (cache: 140)Encoded 150/171 tokens (cache: 150)Encoded 160/171 tokens (cache: 160)Encoded 170/171 tokens (cache: 170)
Encoding complete.
Original size: 599 chars
Compressed size: 32 bytes
Bits per character: 0.4274
Compression finished in 8.86 seconds.
Results saved to benchmarks.db
Results saved to model_results.json
  BPC: 0.4274 | Ratio: 18.7188

[23/35] Qwen 3-1.7B | generated_llama32_3b_3.py.txt
Loading model from /home/alexn/.cache/huggingface/hub/models--Qwen--Qwen3-1.7B/snapshots/70d244cc86ccca08cf5af4e1e306ecf908b1ad5e...
`torch_dtype` is deprecated! Use `dtype` instead!
2026-02-16 22:45:39.141237: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
/home/alexn/Documents/code/research/LLMzip_from_scratch/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:6105: UserWarning: expandable_segments not supported on this platform (Triggered internally at /pytorch/c10/hip/HIPAllocatorConfig.h:29.)
  _ = torch.empty(byte_count // factor, dtype=torch.float16, device=device, requires_grad=False)
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.56s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.03s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.11s/it]
Compressing /home/alexn/Documents/code/research/LLMzip_from_scratch/source_text/generated_llama32_3b_3.py.txt to /home/alexn/Documents/code/research/LLMzip_from_scratch/encoded_documents/selfcomp_Qwen_317B_generated_llama32_3b_3.py_w50.llmzip...
Total tokens to encode: 292
/home/alexn/Documents/code/research/LLMzip_from_scratch/.venv/lib/python3.12/site-packages/transformers/models/qwen3/modeling_qwen3.py:327: UserWarning: Attempting to use hipBLASLt on an unsupported architecture! Overriding blas backend to hipblas (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:296.)
  freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
Encoded 10/292 tokens (cache: 10)Encoded 20/292 tokens (cache: 20)Encoded 30/292 tokens (cache: 30)Encoded 40/292 tokens (cache: 40)Encoded 50/292 tokens (cache: 50)Encoded 60/292 tokens (cache: 60)Encoded 70/292 tokens (cache: 70)Encoded 80/292 tokens (cache: 80)Encoded 90/292 tokens (cache: 90)Encoded 100/292 tokens (cache: 100)Encoded 110/292 tokens (cache: 110)Encoded 120/292 tokens (cache: 120)Encoded 130/292 tokens (cache: 130)Encoded 140/292 tokens (cache: 140)Encoded 150/292 tokens (cache: 150)Encoded 160/292 tokens (cache: 160)Encoded 170/292 tokens (cache: 170)Encoded 180/292 tokens (cache: 180)Encoded 190/292 tokens (cache: 190)Encoded 200/292 tokens (cache: 200)Encoded 210/292 tokens (cache: 210)Encoded 220/292 tokens (cache: 220)Encoded 230/292 tokens (cache: 230)Encoded 240/292 tokens (cache: 240)Encoded 250/292 tokens (cache: 250)Encoded 260/292 tokens (cache: 260)Encoded 270/292 tokens (cache: 270)Encoded 280/292 tokens (cache: 280)Encoded 290/292 tokens (cache: 290)
Encoding complete.
Original size: 1145 chars
Compressed size: 34 bytes
Bits per character: 0.2376
Compression finished in 14.81 seconds.
Results saved to benchmarks.db
Results saved to model_results.json
  BPC: 0.2376 | Ratio: 33.6765

[24/35] Qwen 3-1.7B | generated_llama32_3b_4.py.txt
Loading model from /home/alexn/.cache/huggingface/hub/models--Qwen--Qwen3-1.7B/snapshots/70d244cc86ccca08cf5af4e1e306ecf908b1ad5e...
`torch_dtype` is deprecated! Use `dtype` instead!
2026-02-16 22:46:03.063942: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
/home/alexn/Documents/code/research/LLMzip_from_scratch/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:6105: UserWarning: expandable_segments not supported on this platform (Triggered internally at /pytorch/c10/hip/HIPAllocatorConfig.h:29.)
  _ = torch.empty(byte_count // factor, dtype=torch.float16, device=device, requires_grad=False)
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.44s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.08it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.00s/it]
Compressing /home/alexn/Documents/code/research/LLMzip_from_scratch/source_text/generated_llama32_3b_4.py.txt to /home/alexn/Documents/code/research/LLMzip_from_scratch/encoded_documents/selfcomp_Qwen_317B_generated_llama32_3b_4.py_w50.llmzip...
Total tokens to encode: 221
/home/alexn/Documents/code/research/LLMzip_from_scratch/.venv/lib/python3.12/site-packages/transformers/models/qwen3/modeling_qwen3.py:327: UserWarning: Attempting to use hipBLASLt on an unsupported architecture! Overriding blas backend to hipblas (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:296.)
  freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
Encoded 10/221 tokens (cache: 10)Encoded 20/221 tokens (cache: 20)Encoded 30/221 tokens (cache: 30)Encoded 40/221 tokens (cache: 40)Encoded 50/221 tokens (cache: 50)Encoded 60/221 tokens (cache: 60)Encoded 70/221 tokens (cache: 70)Encoded 80/221 tokens (cache: 80)Encoded 90/221 tokens (cache: 90)Encoded 100/221 tokens (cache: 100)Encoded 110/221 tokens (cache: 110)Encoded 120/221 tokens (cache: 120)Encoded 130/221 tokens (cache: 130)Encoded 140/221 tokens (cache: 140)Encoded 150/221 tokens (cache: 150)Encoded 160/221 tokens (cache: 160)Encoded 170/221 tokens (cache: 170)Encoded 180/221 tokens (cache: 180)Encoded 190/221 tokens (cache: 190)Encoded 200/221 tokens (cache: 200)Encoded 210/221 tokens (cache: 210)Encoded 220/221 tokens (cache: 220)
Encoding complete.
Original size: 717 chars
Compressed size: 26 bytes
Bits per character: 0.2901
Compression finished in 11.26 seconds.
Results saved to benchmarks.db
Results saved to model_results.json
  BPC: 0.2901 | Ratio: 27.5769

[25/35] Qwen 3-1.7B | generated_llama32_3b_5.py.txt
Loading model from /home/alexn/.cache/huggingface/hub/models--Qwen--Qwen3-1.7B/snapshots/70d244cc86ccca08cf5af4e1e306ecf908b1ad5e...
`torch_dtype` is deprecated! Use `dtype` instead!
2026-02-16 22:46:23.274776: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
/home/alexn/Documents/code/research/LLMzip_from_scratch/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:6105: UserWarning: expandable_segments not supported on this platform (Triggered internally at /pytorch/c10/hip/HIPAllocatorConfig.h:29.)
  _ = torch.empty(byte_count // factor, dtype=torch.float16, device=device, requires_grad=False)
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.33s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.14it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.06it/s]
Compressing /home/alexn/Documents/code/research/LLMzip_from_scratch/source_text/generated_llama32_3b_5.py.txt to /home/alexn/Documents/code/research/LLMzip_from_scratch/encoded_documents/selfcomp_Qwen_317B_generated_llama32_3b_5.py_w50.llmzip...
Total tokens to encode: 448
/home/alexn/Documents/code/research/LLMzip_from_scratch/.venv/lib/python3.12/site-packages/transformers/models/qwen3/modeling_qwen3.py:327: UserWarning: Attempting to use hipBLASLt on an unsupported architecture! Overriding blas backend to hipblas (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:296.)
  freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
Encoded 10/448 tokens (cache: 10)Encoded 20/448 tokens (cache: 20)Encoded 30/448 tokens (cache: 30)Encoded 40/448 tokens (cache: 40)Encoded 50/448 tokens (cache: 50)Encoded 60/448 tokens (cache: 60)Encoded 70/448 tokens (cache: 70)Encoded 80/448 tokens (cache: 80)Encoded 90/448 tokens (cache: 90)Encoded 100/448 tokens (cache: 100)Encoded 110/448 tokens (cache: 110)Encoded 120/448 tokens (cache: 120)Encoded 130/448 tokens (cache: 130)Encoded 140/448 tokens (cache: 140)Encoded 150/448 tokens (cache: 150)Encoded 160/448 tokens (cache: 160)Encoded 170/448 tokens (cache: 170)Encoded 180/448 tokens (cache: 180)Encoded 190/448 tokens (cache: 190)Encoded 200/448 tokens (cache: 200)Encoded 210/448 tokens (cache: 210)Encoded 220/448 tokens (cache: 220)Encoded 230/448 tokens (cache: 230)Encoded 240/448 tokens (cache: 240)Encoded 250/448 tokens (cache: 250)Encoded 260/448 tokens (cache: 260)Encoded 270/448 tokens (cache: 270)Encoded 280/448 tokens (cache: 280)Encoded 290/448 tokens (cache: 290)Encoded 300/448 tokens (cache: 300)Encoded 310/448 tokens (cache: 310)Encoded 320/448 tokens (cache: 320)Encoded 330/448 tokens (cache: 330)Encoded 340/448 tokens (cache: 340)Encoded 350/448 tokens (cache: 350)Encoded 360/448 tokens (cache: 360)Encoded 370/448 tokens (cache: 370)Encoded 380/448 tokens (cache: 380)Encoded 390/448 tokens (cache: 390)Encoded 400/448 tokens (cache: 400)Encoded 410/448 tokens (cache: 410)Encoded 420/448 tokens (cache: 420)Encoded 430/448 tokens (cache: 430)Encoded 440/448 tokens (cache: 440)
Encoding complete.
Original size: 2055 chars
Compressed size: 45 bytes
Bits per character: 0.1752
Compression finished in 22.61 seconds.
Results saved to benchmarks.db
Results saved to model_results.json
  BPC: 0.1752 | Ratio: 45.6667

[26/35] Llama 3.2-1B | generated_llama32_3b_1.py.txt
Loading model from /home/alexn/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08...
`torch_dtype` is deprecated! Use `dtype` instead!
2026-02-16 22:46:54.672718: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
/home/alexn/Documents/code/research/LLMzip_from_scratch/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:6105: UserWarning: expandable_segments not supported on this platform (Triggered internally at /pytorch/c10/hip/HIPAllocatorConfig.h:29.)
  _ = torch.empty(byte_count // factor, dtype=torch.float16, device=device, requires_grad=False)
Compressing /home/alexn/Documents/code/research/LLMzip_from_scratch/source_text/generated_llama32_3b_1.py.txt to /home/alexn/Documents/code/research/LLMzip_from_scratch/encoded_documents/selfcomp_Llama_321B_generated_llama32_3b_1.py_w50.llmzip...
Total tokens to encode: 423
/home/alexn/Documents/code/research/LLMzip_from_scratch/.venv/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:101: UserWarning: Attempting to use hipBLASLt on an unsupported architecture! Overriding blas backend to hipblas (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:296.)
  freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
Encoded 10/423 tokens (cache: 10)Encoded 20/423 tokens (cache: 20)Encoded 30/423 tokens (cache: 30)Encoded 40/423 tokens (cache: 40)Encoded 50/423 tokens (cache: 50)Encoded 60/423 tokens (cache: 60)Encoded 70/423 tokens (cache: 70)Encoded 80/423 tokens (cache: 80)Encoded 90/423 tokens (cache: 90)Encoded 100/423 tokens (cache: 100)Encoded 110/423 tokens (cache: 110)Encoded 120/423 tokens (cache: 120)Encoded 130/423 tokens (cache: 130)Encoded 140/423 tokens (cache: 140)Encoded 150/423 tokens (cache: 150)Encoded 160/423 tokens (cache: 160)Encoded 170/423 tokens (cache: 170)Encoded 180/423 tokens (cache: 180)Encoded 190/423 tokens (cache: 190)Encoded 200/423 tokens (cache: 200)Encoded 210/423 tokens (cache: 210)Encoded 220/423 tokens (cache: 220)Encoded 230/423 tokens (cache: 230)Encoded 240/423 tokens (cache: 240)Encoded 250/423 tokens (cache: 250)Encoded 260/423 tokens (cache: 260)Encoded 270/423 tokens (cache: 270)Encoded 280/423 tokens (cache: 280)Encoded 290/423 tokens (cache: 290)Encoded 300/423 tokens (cache: 300)Encoded 310/423 tokens (cache: 310)Encoded 320/423 tokens (cache: 320)Encoded 330/423 tokens (cache: 330)Encoded 340/423 tokens (cache: 340)Encoded 350/423 tokens (cache: 350)Encoded 360/423 tokens (cache: 360)Encoded 370/423 tokens (cache: 370)Encoded 380/423 tokens (cache: 380)Encoded 390/423 tokens (cache: 390)Encoded 400/423 tokens (cache: 400)Encoded 410/423 tokens (cache: 410)Encoded 420/423 tokens (cache: 420)
Encoding complete.
Original size: 1907 chars
Compressed size: 31 bytes
Bits per character: 0.1300
Compression finished in 18.51 seconds.
Results saved to benchmarks.db
Results saved to model_results.json
  BPC: 0.1300 | Ratio: 61.5161

[27/35] Llama 3.2-1B | generated_llama32_3b_2.py.txt
Loading model from /home/alexn/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08...
`torch_dtype` is deprecated! Use `dtype` instead!
2026-02-16 22:47:23.089821: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
/home/alexn/Documents/code/research/LLMzip_from_scratch/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:6105: UserWarning: expandable_segments not supported on this platform (Triggered internally at /pytorch/c10/hip/HIPAllocatorConfig.h:29.)
  _ = torch.empty(byte_count // factor, dtype=torch.float16, device=device, requires_grad=False)
Compressing /home/alexn/Documents/code/research/LLMzip_from_scratch/source_text/generated_llama32_3b_2.py.txt to /home/alexn/Documents/code/research/LLMzip_from_scratch/encoded_documents/selfcomp_Llama_321B_generated_llama32_3b_2.py_w50.llmzip...
Total tokens to encode: 172
/home/alexn/Documents/code/research/LLMzip_from_scratch/.venv/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:101: UserWarning: Attempting to use hipBLASLt on an unsupported architecture! Overriding blas backend to hipblas (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:296.)
  freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
Encoded 10/172 tokens (cache: 10)Encoded 20/172 tokens (cache: 20)Encoded 30/172 tokens (cache: 30)Encoded 40/172 tokens (cache: 40)Encoded 50/172 tokens (cache: 50)Encoded 60/172 tokens (cache: 60)Encoded 70/172 tokens (cache: 70)Encoded 80/172 tokens (cache: 80)Encoded 90/172 tokens (cache: 90)Encoded 100/172 tokens (cache: 100)Encoded 110/172 tokens (cache: 110)Encoded 120/172 tokens (cache: 120)Encoded 130/172 tokens (cache: 130)Encoded 140/172 tokens (cache: 140)Encoded 150/172 tokens (cache: 150)Encoded 160/172 tokens (cache: 160)Encoded 170/172 tokens (cache: 170)
Encoding complete.
Original size: 599 chars
Compressed size: 28 bytes
Bits per character: 0.3740
Compression finished in 7.89 seconds.
Results saved to benchmarks.db
Results saved to model_results.json
  BPC: 0.3740 | Ratio: 21.3929

[28/35] Llama 3.2-1B | generated_llama32_3b_3.py.txt
Loading model from /home/alexn/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08...
`torch_dtype` is deprecated! Use `dtype` instead!
2026-02-16 22:47:39.367284: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
/home/alexn/Documents/code/research/LLMzip_from_scratch/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:6105: UserWarning: expandable_segments not supported on this platform (Triggered internally at /pytorch/c10/hip/HIPAllocatorConfig.h:29.)
  _ = torch.empty(byte_count // factor, dtype=torch.float16, device=device, requires_grad=False)
Compressing /home/alexn/Documents/code/research/LLMzip_from_scratch/source_text/generated_llama32_3b_3.py.txt to /home/alexn/Documents/code/research/LLMzip_from_scratch/encoded_documents/selfcomp_Llama_321B_generated_llama32_3b_3.py_w50.llmzip...
Total tokens to encode: 293
/home/alexn/Documents/code/research/LLMzip_from_scratch/.venv/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:101: UserWarning: Attempting to use hipBLASLt on an unsupported architecture! Overriding blas backend to hipblas (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:296.)
  freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
Encoded 10/293 tokens (cache: 10)Encoded 20/293 tokens (cache: 20)Encoded 30/293 tokens (cache: 30)Encoded 40/293 tokens (cache: 40)Encoded 50/293 tokens (cache: 50)Encoded 60/293 tokens (cache: 60)Encoded 70/293 tokens (cache: 70)Encoded 80/293 tokens (cache: 80)Encoded 90/293 tokens (cache: 90)Encoded 100/293 tokens (cache: 100)Encoded 110/293 tokens (cache: 110)Encoded 120/293 tokens (cache: 120)Encoded 130/293 tokens (cache: 130)Encoded 140/293 tokens (cache: 140)Encoded 150/293 tokens (cache: 150)Encoded 160/293 tokens (cache: 160)Encoded 170/293 tokens (cache: 170)Encoded 180/293 tokens (cache: 180)Encoded 190/293 tokens (cache: 190)Encoded 200/293 tokens (cache: 200)Encoded 210/293 tokens (cache: 210)Encoded 220/293 tokens (cache: 220)Encoded 230/293 tokens (cache: 230)Encoded 240/293 tokens (cache: 240)Encoded 250/293 tokens (cache: 250)Encoded 260/293 tokens (cache: 260)Encoded 270/293 tokens (cache: 270)Encoded 280/293 tokens (cache: 280)Encoded 290/293 tokens (cache: 290)
Encoding complete.
Original size: 1145 chars
Compressed size: 31 bytes
Bits per character: 0.2166
Compression finished in 13.02 seconds.
Results saved to benchmarks.db
Results saved to model_results.json
  BPC: 0.2166 | Ratio: 36.9355

[29/35] Llama 3.2-1B | generated_llama32_3b_4.py.txt
Loading model from /home/alexn/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08...
`torch_dtype` is deprecated! Use `dtype` instead!
2026-02-16 22:48:00.835004: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
/home/alexn/Documents/code/research/LLMzip_from_scratch/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:6105: UserWarning: expandable_segments not supported on this platform (Triggered internally at /pytorch/c10/hip/HIPAllocatorConfig.h:29.)
  _ = torch.empty(byte_count // factor, dtype=torch.float16, device=device, requires_grad=False)
Compressing /home/alexn/Documents/code/research/LLMzip_from_scratch/source_text/generated_llama32_3b_4.py.txt to /home/alexn/Documents/code/research/LLMzip_from_scratch/encoded_documents/selfcomp_Llama_321B_generated_llama32_3b_4.py_w50.llmzip...
Total tokens to encode: 222
/home/alexn/Documents/code/research/LLMzip_from_scratch/.venv/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:101: UserWarning: Attempting to use hipBLASLt on an unsupported architecture! Overriding blas backend to hipblas (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:296.)
  freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
Encoded 10/222 tokens (cache: 10)Encoded 20/222 tokens (cache: 20)Encoded 30/222 tokens (cache: 30)Encoded 40/222 tokens (cache: 40)Encoded 50/222 tokens (cache: 50)Encoded 60/222 tokens (cache: 60)Encoded 70/222 tokens (cache: 70)Encoded 80/222 tokens (cache: 80)Encoded 90/222 tokens (cache: 90)Encoded 100/222 tokens (cache: 100)Encoded 110/222 tokens (cache: 110)Encoded 120/222 tokens (cache: 120)Encoded 130/222 tokens (cache: 130)Encoded 140/222 tokens (cache: 140)Encoded 150/222 tokens (cache: 150)Encoded 160/222 tokens (cache: 160)Encoded 170/222 tokens (cache: 170)Encoded 180/222 tokens (cache: 180)Encoded 190/222 tokens (cache: 190)Encoded 200/222 tokens (cache: 200)Encoded 210/222 tokens (cache: 210)Encoded 220/222 tokens (cache: 220)
Encoding complete.
Original size: 717 chars
Compressed size: 23 bytes
Bits per character: 0.2566
Compression finished in 9.96 seconds.
Results saved to benchmarks.db
Results saved to model_results.json
  BPC: 0.2566 | Ratio: 31.1739

[30/35] Llama 3.2-1B | generated_llama32_3b_5.py.txt
Loading model from /home/alexn/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08...
`torch_dtype` is deprecated! Use `dtype` instead!
2026-02-16 22:48:19.154418: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
/home/alexn/Documents/code/research/LLMzip_from_scratch/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:6105: UserWarning: expandable_segments not supported on this platform (Triggered internally at /pytorch/c10/hip/HIPAllocatorConfig.h:29.)
  _ = torch.empty(byte_count // factor, dtype=torch.float16, device=device, requires_grad=False)
Compressing /home/alexn/Documents/code/research/LLMzip_from_scratch/source_text/generated_llama32_3b_5.py.txt to /home/alexn/Documents/code/research/LLMzip_from_scratch/encoded_documents/selfcomp_Llama_321B_generated_llama32_3b_5.py_w50.llmzip...
Total tokens to encode: 433
/home/alexn/Documents/code/research/LLMzip_from_scratch/.venv/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:101: UserWarning: Attempting to use hipBLASLt on an unsupported architecture! Overriding blas backend to hipblas (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:296.)
  freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
Encoded 10/433 tokens (cache: 10)Encoded 20/433 tokens (cache: 20)Encoded 30/433 tokens (cache: 30)Encoded 40/433 tokens (cache: 40)Encoded 50/433 tokens (cache: 50)Encoded 60/433 tokens (cache: 60)Encoded 70/433 tokens (cache: 70)Encoded 80/433 tokens (cache: 80)Encoded 90/433 tokens (cache: 90)Encoded 100/433 tokens (cache: 100)Encoded 110/433 tokens (cache: 110)Encoded 120/433 tokens (cache: 120)Encoded 130/433 tokens (cache: 130)Encoded 140/433 tokens (cache: 140)Encoded 150/433 tokens (cache: 150)Encoded 160/433 tokens (cache: 160)Encoded 170/433 tokens (cache: 170)Encoded 180/433 tokens (cache: 180)Encoded 190/433 tokens (cache: 190)Encoded 200/433 tokens (cache: 200)Encoded 210/433 tokens (cache: 210)Encoded 220/433 tokens (cache: 220)Encoded 230/433 tokens (cache: 230)Encoded 240/433 tokens (cache: 240)Encoded 250/433 tokens (cache: 250)Encoded 260/433 tokens (cache: 260)Encoded 270/433 tokens (cache: 270)Encoded 280/433 tokens (cache: 280)Encoded 290/433 tokens (cache: 290)Encoded 300/433 tokens (cache: 300)Encoded 310/433 tokens (cache: 310)Encoded 320/433 tokens (cache: 320)Encoded 330/433 tokens (cache: 330)Encoded 340/433 tokens (cache: 340)Encoded 350/433 tokens (cache: 350)Encoded 360/433 tokens (cache: 360)Encoded 370/433 tokens (cache: 370)Encoded 380/433 tokens (cache: 380)Encoded 390/433 tokens (cache: 390)Encoded 400/433 tokens (cache: 400)Encoded 410/433 tokens (cache: 410)Encoded 420/433 tokens (cache: 420)Encoded 430/433 tokens (cache: 430)
Encoding complete.
Original size: 2055 chars
Compressed size: 40 bytes
Bits per character: 0.1557
Compression finished in 18.95 seconds.
Results saved to benchmarks.db
Results saved to model_results.json
  BPC: 0.1557 | Ratio: 51.3750

[31/35] Llama 3.2-3B | generated_llama32_3b_1.py.txt
Loading model from /home/alexn/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B/snapshots/13afe5124825b4f3751f836b40dafda64c1ed062...
`torch_dtype` is deprecated! Use `dtype` instead!
2026-02-16 22:48:46.271013: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
/home/alexn/Documents/code/research/LLMzip_from_scratch/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:6105: UserWarning: expandable_segments not supported on this platform (Triggered internally at /pytorch/c10/hip/HIPAllocatorConfig.h:29.)
  _ = torch.empty(byte_count // factor, dtype=torch.float16, device=device, requires_grad=False)
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.23s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  2.84s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.20s/it]
Compressing /home/alexn/Documents/code/research/LLMzip_from_scratch/source_text/generated_llama32_3b_1.py.txt to /home/alexn/Documents/code/research/LLMzip_from_scratch/encoded_documents/selfcomp_Llama_323B_generated_llama32_3b_1.py_w50.llmzip...
Total tokens to encode: 423
/home/alexn/Documents/code/research/LLMzip_from_scratch/.venv/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:101: UserWarning: Attempting to use hipBLASLt on an unsupported architecture! Overriding blas backend to hipblas (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:296.)
  freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
Encoded 10/423 tokens (cache: 10)Encoded 20/423 tokens (cache: 20)Encoded 30/423 tokens (cache: 30)Encoded 40/423 tokens (cache: 40)Encoded 50/423 tokens (cache: 50)Encoded 60/423 tokens (cache: 60)Encoded 70/423 tokens (cache: 70)Encoded 80/423 tokens (cache: 80)Encoded 90/423 tokens (cache: 90)Encoded 100/423 tokens (cache: 100)Encoded 110/423 tokens (cache: 110)Encoded 120/423 tokens (cache: 120)Encoded 130/423 tokens (cache: 130)Encoded 140/423 tokens (cache: 140)Encoded 150/423 tokens (cache: 150)Encoded 160/423 tokens (cache: 160)Encoded 170/423 tokens (cache: 170)Encoded 180/423 tokens (cache: 180)Encoded 190/423 tokens (cache: 190)Encoded 200/423 tokens (cache: 200)Encoded 210/423 tokens (cache: 210)Encoded 220/423 tokens (cache: 220)Encoded 230/423 tokens (cache: 230)Encoded 240/423 tokens (cache: 240)Encoded 250/423 tokens (cache: 250)Encoded 260/423 tokens (cache: 260)Encoded 270/423 tokens (cache: 270)Encoded 280/423 tokens (cache: 280)Encoded 290/423 tokens (cache: 290)Encoded 300/423 tokens (cache: 300)Encoded 310/423 tokens (cache: 310)Encoded 320/423 tokens (cache: 320)Encoded 330/423 tokens (cache: 330)Encoded 340/423 tokens (cache: 340)Encoded 350/423 tokens (cache: 350)Encoded 360/423 tokens (cache: 360)Encoded 370/423 tokens (cache: 370)Encoded 380/423 tokens (cache: 380)Encoded 390/423 tokens (cache: 390)Encoded 400/423 tokens (cache: 400)Encoded 410/423 tokens (cache: 410)Encoded 420/423 tokens (cache: 420)
Encoding complete.
Original size: 1907 chars
Compressed size: 30 bytes
Bits per character: 0.1259
Compression finished in 36.35 seconds.
Results saved to benchmarks.db
Results saved to model_results.json
  BPC: 0.1259 | Ratio: 63.5667

[32/35] Llama 3.2-3B | generated_llama32_3b_2.py.txt
Loading model from /home/alexn/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B/snapshots/13afe5124825b4f3751f836b40dafda64c1ed062...
`torch_dtype` is deprecated! Use `dtype` instead!
2026-02-16 22:49:35.968244: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
/home/alexn/Documents/code/research/LLMzip_from_scratch/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:6105: UserWarning: expandable_segments not supported on this platform (Triggered internally at /pytorch/c10/hip/HIPAllocatorConfig.h:29.)
  _ = torch.empty(byte_count // factor, dtype=torch.float16, device=device, requires_grad=False)
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.46s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.59s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.72s/it]
Compressing /home/alexn/Documents/code/research/LLMzip_from_scratch/source_text/generated_llama32_3b_2.py.txt to /home/alexn/Documents/code/research/LLMzip_from_scratch/encoded_documents/selfcomp_Llama_323B_generated_llama32_3b_2.py_w50.llmzip...
Total tokens to encode: 172
/home/alexn/Documents/code/research/LLMzip_from_scratch/.venv/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:101: UserWarning: Attempting to use hipBLASLt on an unsupported architecture! Overriding blas backend to hipblas (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:296.)
  freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
Encoded 10/172 tokens (cache: 10)Encoded 20/172 tokens (cache: 20)Encoded 30/172 tokens (cache: 30)Encoded 40/172 tokens (cache: 40)Encoded 50/172 tokens (cache: 50)Encoded 60/172 tokens (cache: 60)Encoded 70/172 tokens (cache: 70)Encoded 80/172 tokens (cache: 80)Encoded 90/172 tokens (cache: 90)Encoded 100/172 tokens (cache: 100)Encoded 110/172 tokens (cache: 110)Encoded 120/172 tokens (cache: 120)Encoded 130/172 tokens (cache: 130)Encoded 140/172 tokens (cache: 140)Encoded 150/172 tokens (cache: 150)Encoded 160/172 tokens (cache: 160)Encoded 170/172 tokens (cache: 170)
Encoding complete.
Original size: 599 chars
Compressed size: 27 bytes
Bits per character: 0.3606
Compression finished in 14.94 seconds.
Results saved to benchmarks.db
Results saved to model_results.json
  BPC: 0.3606 | Ratio: 22.1852

[33/35] Llama 3.2-3B | generated_llama32_3b_3.py.txt
Loading model from /home/alexn/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B/snapshots/13afe5124825b4f3751f836b40dafda64c1ed062...
`torch_dtype` is deprecated! Use `dtype` instead!
2026-02-16 22:50:01.328829: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
/home/alexn/Documents/code/research/LLMzip_from_scratch/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:6105: UserWarning: expandable_segments not supported on this platform (Triggered internally at /pytorch/c10/hip/HIPAllocatorConfig.h:29.)
  _ = torch.empty(byte_count // factor, dtype=torch.float16, device=device, requires_grad=False)
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.50s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.34s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.52s/it]
Compressing /home/alexn/Documents/code/research/LLMzip_from_scratch/source_text/generated_llama32_3b_3.py.txt to /home/alexn/Documents/code/research/LLMzip_from_scratch/encoded_documents/selfcomp_Llama_323B_generated_llama32_3b_3.py_w50.llmzip...
Total tokens to encode: 293
/home/alexn/Documents/code/research/LLMzip_from_scratch/.venv/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:101: UserWarning: Attempting to use hipBLASLt on an unsupported architecture! Overriding blas backend to hipblas (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:296.)
  freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
Encoded 10/293 tokens (cache: 10)Encoded 20/293 tokens (cache: 20)Encoded 30/293 tokens (cache: 30)Encoded 40/293 tokens (cache: 40)Encoded 50/293 tokens (cache: 50)Encoded 60/293 tokens (cache: 60)Encoded 70/293 tokens (cache: 70)Encoded 80/293 tokens (cache: 80)Encoded 90/293 tokens (cache: 90)Encoded 100/293 tokens (cache: 100)Encoded 110/293 tokens (cache: 110)Encoded 120/293 tokens (cache: 120)Encoded 130/293 tokens (cache: 130)Encoded 140/293 tokens (cache: 140)Encoded 150/293 tokens (cache: 150)Encoded 160/293 tokens (cache: 160)Encoded 170/293 tokens (cache: 170)Encoded 180/293 tokens (cache: 180)Encoded 190/293 tokens (cache: 190)Encoded 200/293 tokens (cache: 200)Encoded 210/293 tokens (cache: 210)Encoded 220/293 tokens (cache: 220)Encoded 230/293 tokens (cache: 230)Encoded 240/293 tokens (cache: 240)Encoded 250/293 tokens (cache: 250)Encoded 260/293 tokens (cache: 260)Encoded 270/293 tokens (cache: 270)Encoded 280/293 tokens (cache: 280)Encoded 290/293 tokens (cache: 290)
Encoding complete.
Original size: 1145 chars
Compressed size: 29 bytes
Bits per character: 0.2026
Compression finished in 25.15 seconds.
Results saved to benchmarks.db
Results saved to model_results.json
  BPC: 0.2026 | Ratio: 39.4828

[34/35] Llama 3.2-3B | generated_llama32_3b_4.py.txt
Loading model from /home/alexn/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B/snapshots/13afe5124825b4f3751f836b40dafda64c1ed062...
`torch_dtype` is deprecated! Use `dtype` instead!
2026-02-16 22:50:36.544037: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
/home/alexn/Documents/code/research/LLMzip_from_scratch/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:6105: UserWarning: expandable_segments not supported on this platform (Triggered internally at /pytorch/c10/hip/HIPAllocatorConfig.h:29.)
  _ = torch.empty(byte_count // factor, dtype=torch.float16, device=device, requires_grad=False)
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.41s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.28s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.45s/it]
Compressing /home/alexn/Documents/code/research/LLMzip_from_scratch/source_text/generated_llama32_3b_4.py.txt to /home/alexn/Documents/code/research/LLMzip_from_scratch/encoded_documents/selfcomp_Llama_323B_generated_llama32_3b_4.py_w50.llmzip...
Total tokens to encode: 222
/home/alexn/Documents/code/research/LLMzip_from_scratch/.venv/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:101: UserWarning: Attempting to use hipBLASLt on an unsupported architecture! Overriding blas backend to hipblas (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:296.)
  freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
Encoded 10/222 tokens (cache: 10)Encoded 20/222 tokens (cache: 20)Encoded 30/222 tokens (cache: 30)Encoded 40/222 tokens (cache: 40)Encoded 50/222 tokens (cache: 50)Encoded 60/222 tokens (cache: 60)Encoded 70/222 tokens (cache: 70)Encoded 80/222 tokens (cache: 80)Encoded 90/222 tokens (cache: 90)Encoded 100/222 tokens (cache: 100)Encoded 110/222 tokens (cache: 110)Encoded 120/222 tokens (cache: 120)Encoded 130/222 tokens (cache: 130)Encoded 140/222 tokens (cache: 140)Encoded 150/222 tokens (cache: 150)Encoded 160/222 tokens (cache: 160)Encoded 170/222 tokens (cache: 170)Encoded 180/222 tokens (cache: 180)Encoded 190/222 tokens (cache: 190)Encoded 200/222 tokens (cache: 200)Encoded 210/222 tokens (cache: 210)Encoded 220/222 tokens (cache: 220)
Encoding complete.
Original size: 717 chars
Compressed size: 23 bytes
Bits per character: 0.2566
Compression finished in 19.17 seconds.
Results saved to benchmarks.db
Results saved to model_results.json
  BPC: 0.2566 | Ratio: 31.1739

[35/35] Llama 3.2-3B | generated_llama32_3b_5.py.txt
Loading model from /home/alexn/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B/snapshots/13afe5124825b4f3751f836b40dafda64c1ed062...
`torch_dtype` is deprecated! Use `dtype` instead!
2026-02-16 22:51:05.532447: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
/home/alexn/Documents/code/research/LLMzip_from_scratch/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:6105: UserWarning: expandable_segments not supported on this platform (Triggered internally at /pytorch/c10/hip/HIPAllocatorConfig.h:29.)
  _ = torch.empty(byte_count // factor, dtype=torch.float16, device=device, requires_grad=False)
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.39s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.27s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.44s/it]
Compressing /home/alexn/Documents/code/research/LLMzip_from_scratch/source_text/generated_llama32_3b_5.py.txt to /home/alexn/Documents/code/research/LLMzip_from_scratch/encoded_documents/selfcomp_Llama_323B_generated_llama32_3b_5.py_w50.llmzip...
Total tokens to encode: 433
/home/alexn/Documents/code/research/LLMzip_from_scratch/.venv/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:101: UserWarning: Attempting to use hipBLASLt on an unsupported architecture! Overriding blas backend to hipblas (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:296.)
  freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
Encoded 10/433 tokens (cache: 10)Encoded 20/433 tokens (cache: 20)Encoded 30/433 tokens (cache: 30)Encoded 40/433 tokens (cache: 40)Encoded 50/433 tokens (cache: 50)Encoded 60/433 tokens (cache: 60)Encoded 70/433 tokens (cache: 70)Encoded 80/433 tokens (cache: 80)Encoded 90/433 tokens (cache: 90)Encoded 100/433 tokens (cache: 100)Encoded 110/433 tokens (cache: 110)Encoded 120/433 tokens (cache: 120)Encoded 130/433 tokens (cache: 130)Encoded 140/433 tokens (cache: 140)Encoded 150/433 tokens (cache: 150)Encoded 160/433 tokens (cache: 160)Encoded 170/433 tokens (cache: 170)Encoded 180/433 tokens (cache: 180)Encoded 190/433 tokens (cache: 190)Encoded 200/433 tokens (cache: 200)Encoded 210/433 tokens (cache: 210)Encoded 220/433 tokens (cache: 220)Encoded 230/433 tokens (cache: 230)Encoded 240/433 tokens (cache: 240)Encoded 250/433 tokens (cache: 250)Encoded 260/433 tokens (cache: 260)Encoded 270/433 tokens (cache: 270)Encoded 280/433 tokens (cache: 280)Encoded 290/433 tokens (cache: 290)Encoded 300/433 tokens (cache: 300)Encoded 310/433 tokens (cache: 310)Encoded 320/433 tokens (cache: 320)Encoded 330/433 tokens (cache: 330)Encoded 340/433 tokens (cache: 340)Encoded 350/433 tokens (cache: 350)Encoded 360/433 tokens (cache: 360)Encoded 370/433 tokens (cache: 370)Encoded 380/433 tokens (cache: 380)Encoded 390/433 tokens (cache: 390)Encoded 400/433 tokens (cache: 400)Encoded 410/433 tokens (cache: 410)Encoded 420/433 tokens (cache: 420)Encoded 430/433 tokens (cache: 430)
Encoding complete.
Original size: 2055 chars
Compressed size: 36 bytes
Bits per character: 0.1401
Compression finished in 37.21 seconds.
Results saved to benchmarks.db
Results saved to model_results.json
  BPC: 0.1401 | Ratio: 57.0833

Saved 35 results to /home/alexn/Documents/code/research/LLMzip_from_scratch/self_compression_results.json (total: 35)

Generating plots after Llama 3.2-3B-Instruct phase...
Loaded 35 results

Generators: ['llama32_3b']
Compressors: ['Gemma 3-1B', 'Gemma 3-4B', 'LiquidAI 1.2B', 'Llama 3.2-1B', 'Llama 3.2-3B', 'Qwen 2.5-3B', 'Qwen 3-1.7B']

Saved: results/self_compression_bpc_by_compressor.png
Saved: results/self_compression_self_vs_other.png
Saved: results/self_compression_heatmap.png
Saved: results/self_compression_model_summary.png
Saved: results/self_compression_affinity.png
Saved: results/self_compression_per_file_llama32_3b.png

──────────────────────────────────────────────────────────────────────
Generator                 Self Model        Self BPC  Other Avg      Delta
──────────────────────────────────────────────────────────────────────
Llama 3.2-3B-Instruct     Llama 3.2-3B        0.2172     0.2539    -0.0367

Plots saved to results/self_compression_*.png

######################################################################
# Phase 2: Gemma 3-4B-IT
######################################################################

Starting download of google/gemma-3-4b-it in tmux session 'research'...

Waiting for google/gemma-3-4b-it to finish downloading...
  Still waiting... (0s elapsed)  Still waiting... (30s elapsed)  Still waiting... (60s elapsed)  Still waiting... (90s elapsed)  Still waiting... (120s elapsed)  Still waiting... (150s elapsed)  Still waiting... (180s elapsed)  Still waiting... (210s elapsed)  Still waiting... (240s elapsed)  Still waiting... (270s elapsed)  Still waiting... (300s elapsed)  Still waiting... (330s elapsed)  Still waiting... (360s elapsed)  Still waiting... (390s elapsed)  Still waiting... (420s elapsed)  Still waiting... (450s elapsed)  Still waiting... (480s elapsed)  Still waiting... (510s elapsed)  Still waiting... (540s elapsed)  Still waiting... (570s elapsed)  Still waiting... (600s elapsed)  ✓ google/gemma-3-4b-it is ready at /home/alexn/.cache/huggingface/hub/models--google--gemma-3-4b-it/snapshots/093f9f388b31de276ce2de164bdc2081324b9767

Generating code with gemma3_4b...
Loading model from /home/alexn/.cache/huggingface/hub/models--google--gemma-3-4b-it/snapshots/093f9f388b31de276ce2de164bdc2081324b9767...
`torch_dtype` is deprecated! Use `dtype` instead!
2026-02-16 23:02:25.784995: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
/home/alexn/Documents/code/research/LLMzip_from_scratch/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:6105: UserWarning: expandable_segments not supported on this platform (Triggered internally at /pytorch/c10/hip/HIPAllocatorConfig.h:29.)
  _ = torch.empty(byte_count // factor, dtype=torch.float16, device=device, requires_grad=False)
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.16s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:20<00:00, 11.15s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:20<00:00, 10.40s/it]

[1/5] Generating: Write a Python program that implements a binary search tree ...
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
/home/alexn/Documents/code/research/LLMzip_from_scratch/.venv/lib/python3.12/site-packages/transformers/models/gemma3/modeling_gemma3.py:174: UserWarning: Attempting to use hipBLASLt on an unsupported architecture! Overriding blas backend to hipblas (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:296.)
  freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
  Saved: source_text/generated_gemma3_4b_1.py.txt (1907 chars)

[2/5] Generating: Write a Python program that reads a CSV file and computes st...
  Saved: source_text/generated_gemma3_4b_2.py.txt (1789 chars)

[3/5] Generating: Write a Python program that implements a simple LRU cache us...
  Saved: source_text/generated_gemma3_4b_3.py.txt (1093 chars)

[4/5] Generating: Write a Python program that implements the Sieve of Eratosth...
  Saved: source_text/generated_gemma3_4b_4.py.txt (1155 chars)

[5/5] Generating: Write a Python program that implements a basic HTTP server u...
  Saved: source_text/generated_gemma3_4b_5.py.txt (1779 chars)

Generated 5 files:
  source_text/generated_gemma3_4b_1.py.txt
  source_text/generated_gemma3_4b_2.py.txt
  source_text/generated_gemma3_4b_3.py.txt
  source_text/generated_gemma3_4b_4.py.txt
  source_text/generated_gemma3_4b_5.py.txt
  Generated 5 files

======================================================================
Compressing 5 files from Gemma 3-4B-IT
with 7 base models (window=50)
======================================================================

[1/35] Qwen 2.5-3B | generated_gemma3_4b_1.py.txt
Loading model from /home/alexn/.cache/huggingface/hub/models--Qwen--Qwen2.5-3B/snapshots/3aab1f1954e9cc14eb9509a215f9e5ca08227a9b...
`torch_dtype` is deprecated! Use `dtype` instead!
2026-02-16 23:18:34.273031: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
/home/alexn/Documents/code/research/LLMzip_from_scratch/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:6105: UserWarning: expandable_segments not supported on this platform (Triggered internally at /pytorch/c10/hip/HIPAllocatorConfig.h:29.)
  _ = torch.empty(byte_count // factor, dtype=torch.float16, device=device, requires_grad=False)
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.78s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.53s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.71s/it]
Compressing /home/alexn/Documents/code/research/LLMzip_from_scratch/source_text/generated_gemma3_4b_1.py.txt to /home/alexn/Documents/code/research/LLMzip_from_scratch/encoded_documents/selfcomp_Qwen_253B_generated_gemma3_4b_1.py_w50.llmzip...
Total tokens to encode: 424
/home/alexn/Documents/code/research/LLMzip_from_scratch/.venv/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:301: UserWarning: Attempting to use hipBLASLt on an unsupported architecture! Overriding blas backend to hipblas (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:296.)
  freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
Encoded 10/424 tokens (cache: 10)Encoded 20/424 tokens (cache: 20)Encoded 30/424 tokens (cache: 30)Encoded 40/424 tokens (cache: 40)Encoded 50/424 tokens (cache: 50)Encoded 60/424 tokens (cache: 60)Encoded 70/424 tokens (cache: 70)Encoded 80/424 tokens (cache: 80)Encoded 90/424 tokens (cache: 90)Encoded 100/424 tokens (cache: 100)Encoded 110/424 tokens (cache: 110)Encoded 120/424 tokens (cache: 120)Encoded 130/424 tokens (cache: 130)Encoded 140/424 tokens (cache: 140)Encoded 150/424 tokens (cache: 150)Encoded 160/424 tokens (cache: 160)Encoded 170/424 tokens (cache: 170)Encoded 180/424 tokens (cache: 180)Encoded 190/424 tokens (cache: 190)Encoded 200/424 tokens (cache: 200)Encoded 210/424 tokens (cache: 210)Encoded 220/424 tokens (cache: 220)Encoded 230/424 tokens (cache: 230)Encoded 240/424 tokens (cache: 240)Encoded 250/424 tokens (cache: 250)Encoded 260/424 tokens (cache: 260)Encoded 270/424 tokens (cache: 270)Encoded 280/424 tokens (cache: 280)Encoded 290/424 tokens (cache: 290)Encoded 300/424 tokens (cache: 300)Encoded 310/424 tokens (cache: 310)Encoded 320/424 tokens (cache: 320)Encoded 330/424 tokens (cache: 330)Encoded 340/424 tokens (cache: 340)Encoded 350/424 tokens (cache: 350)Encoded 360/424 tokens (cache: 360)Encoded 370/424 tokens (cache: 370)Encoded 380/424 tokens (cache: 380)Encoded 390/424 tokens (cache: 390)Encoded 400/424 tokens (cache: 400)Encoded 410/424 tokens (cache: 410)Encoded 420/424 tokens (cache: 420)
Encoding complete.
Original size: 1907 chars
Compressed size: 26 bytes
Bits per character: 0.1091
Compression finished in 33.49 seconds.
Results saved to benchmarks.db
Results saved to model_results.json
  BPC: 0.1091 | Ratio: 73.3462

[2/35] Qwen 2.5-3B | generated_gemma3_4b_2.py.txt
Loading model from /home/alexn/.cache/huggingface/hub/models--Qwen--Qwen2.5-3B/snapshots/3aab1f1954e9cc14eb9509a215f9e5ca08227a9b...
`torch_dtype` is deprecated! Use `dtype` instead!
2026-02-16 23:19:22.399234: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
/home/alexn/Documents/code/research/LLMzip_from_scratch/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:6105: UserWarning: expandable_segments not supported on this platform (Triggered internally at /pytorch/c10/hip/HIPAllocatorConfig.h:29.)
  _ = torch.empty(byte_count // factor, dtype=torch.float16, device=device, requires_grad=False)
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.73s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.84s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.97s/it]
Compressing /home/alexn/Documents/code/research/LLMzip_from_scratch/source_text/generated_gemma3_4b_2.py.txt to /home/alexn/Documents/code/research/LLMzip_from_scratch/encoded_documents/selfcomp_Qwen_253B_generated_gemma3_4b_2.py_w50.llmzip...
Total tokens to encode: 409
/home/alexn/Documents/code/research/LLMzip_from_scratch/.venv/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:301: UserWarning: Attempting to use hipBLASLt on an unsupported architecture! Overriding blas backend to hipblas (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:296.)
  freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
Encoded 10/409 tokens (cache: 10)Encoded 20/409 tokens (cache: 20)Encoded 30/409 tokens (cache: 30)Encoded 40/409 tokens (cache: 40)Encoded 50/409 tokens (cache: 50)Encoded 60/409 tokens (cache: 60)Encoded 70/409 tokens (cache: 70)Encoded 80/409 tokens (cache: 80)Encoded 90/409 tokens (cache: 90)Encoded 100/409 tokens (cache: 100)Encoded 110/409 tokens (cache: 110)Encoded 120/409 tokens (cache: 120)Encoded 130/409 tokens (cache: 130)Encoded 140/409 tokens (cache: 140)Encoded 150/409 tokens (cache: 150)Encoded 160/409 tokens (cache: 160)Encoded 170/409 tokens (cache: 170)Encoded 180/409 tokens (cache: 180)Encoded 190/409 tokens (cache: 190)Encoded 200/409 tokens (cache: 200)Encoded 210/409 tokens (cache: 210)Encoded 220/409 tokens (cache: 220)Encoded 230/409 tokens (cache: 230)Encoded 240/409 tokens (cache: 240)Encoded 250/409 tokens (cache: 250)Encoded 260/409 tokens (cache: 260)Encoded 270/409 tokens (cache: 270)Encoded 280/409 tokens (cache: 280)Encoded 290/409 tokens (cache: 290)Encoded 300/409 tokens (cache: 300)Encoded 310/409 tokens (cache: 310)Encoded 320/409 tokens (cache: 320)Encoded 330/409 tokens (cache: 330)Encoded 340/409 tokens (cache: 340)Encoded 350/409 tokens (cache: 350)Encoded 360/409 tokens (cache: 360)Encoded 370/409 tokens (cache: 370)Encoded 380/409 tokens (cache: 380)Encoded 390/409 tokens (cache: 390)Encoded 400/409 tokens (cache: 400)
Encoding complete.
Original size: 1789 chars
Compressed size: 51 bytes
Bits per character: 0.2281
Compression finished in 32.31 seconds.
Results saved to benchmarks.db
Results saved to model_results.json
  BPC: 0.2281 | Ratio: 35.0784

[3/35] Qwen 2.5-3B | generated_gemma3_4b_3.py.txt
Loading model from /home/alexn/.cache/huggingface/hub/models--Qwen--Qwen2.5-3B/snapshots/3aab1f1954e9cc14eb9509a215f9e5ca08227a9b...
`torch_dtype` is deprecated! Use `dtype` instead!
2026-02-16 23:20:05.559365: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
/home/alexn/Documents/code/research/LLMzip_from_scratch/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:6105: UserWarning: expandable_segments not supported on this platform (Triggered internally at /pytorch/c10/hip/HIPAllocatorConfig.h:29.)
  _ = torch.empty(byte_count // factor, dtype=torch.float16, device=device, requires_grad=False)
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.96s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.08s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.22s/it]
Compressing /home/alexn/Documents/code/research/LLMzip_from_scratch/source_text/generated_gemma3_4b_3.py.txt to /home/alexn/Documents/code/research/LLMzip_from_scratch/encoded_documents/selfcomp_Qwen_253B_generated_gemma3_4b_3.py_w50.llmzip...
Total tokens to encode: 333
/home/alexn/Documents/code/research/LLMzip_from_scratch/.venv/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:301: UserWarning: Attempting to use hipBLASLt on an unsupported architecture! Overriding blas backend to hipblas (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:296.)
  freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
Encoded 10/333 tokens (cache: 10)Encoded 20/333 tokens (cache: 20)Encoded 30/333 tokens (cache: 30)Encoded 40/333 tokens (cache: 40)Encoded 50/333 tokens (cache: 50)Encoded 60/333 tokens (cache: 60)Encoded 70/333 tokens (cache: 70)Encoded 80/333 tokens (cache: 80)Encoded 90/333 tokens (cache: 90)Encoded 100/333 tokens (cache: 100)Encoded 110/333 tokens (cache: 110)Encoded 120/333 tokens (cache: 120)Encoded 130/333 tokens (cache: 130)Encoded 140/333 tokens (cache: 140)Encoded 150/333 tokens (cache: 150)Encoded 160/333 tokens (cache: 160)Encoded 170/333 tokens (cache: 170)Encoded 180/333 tokens (cache: 180)Encoded 190/333 tokens (cache: 190)Encoded 200/333 tokens (cache: 200)Encoded 210/333 tokens (cache: 210)Encoded 220/333 tokens (cache: 220)Encoded 230/333 tokens (cache: 230)Encoded 240/333 tokens (cache: 240)Encoded 250/333 tokens (cache: 250)Encoded 260/333 tokens (cache: 260)Encoded 270/333 tokens (cache: 270)Encoded 280/333 tokens (cache: 280)Encoded 290/333 tokens (cache: 290)Encoded 300/333 tokens (cache: 300)Encoded 310/333 tokens (cache: 310)Encoded 320/333 tokens (cache: 320)Encoded 330/333 tokens (cache: 330)
Encoding complete.
Original size: 1093 chars
Compressed size: 26 bytes
Bits per character: 0.1903
Compression finished in 26.38 seconds.
Results saved to benchmarks.db
Results saved to model_results.json
  BPC: 0.1903 | Ratio: 42.0385

[4/35] Qwen 2.5-3B | generated_gemma3_4b_4.py.txt
Loading model from /home/alexn/.cache/huggingface/hub/models--Qwen--Qwen2.5-3B/snapshots/3aab1f1954e9cc14eb9509a215f9e5ca08227a9b...
`torch_dtype` is deprecated! Use `dtype` instead!
2026-02-16 23:20:43.283954: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
/home/alexn/Documents/code/research/LLMzip_from_scratch/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:6105: UserWarning: expandable_segments not supported on this platform (Triggered internally at /pytorch/c10/hip/HIPAllocatorConfig.h:29.)
  _ = torch.empty(byte_count // factor, dtype=torch.float16, device=device, requires_grad=False)
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.08s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.78s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.97s/it]
Compressing /home/alexn/Documents/code/research/LLMzip_from_scratch/source_text/generated_gemma3_4b_4.py.txt to /home/alexn/Documents/code/research/LLMzip_from_scratch/encoded_documents/selfcomp_Qwen_253B_generated_gemma3_4b_4.py_w50.llmzip...
Total tokens to encode: 318
/home/alexn/Documents/code/research/LLMzip_from_scratch/.venv/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:301: UserWarning: Attempting to use hipBLASLt on an unsupported architecture! Overriding blas backend to hipblas (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:296.)
  freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
Encoded 10/318 tokens (cache: 10)Encoded 20/318 tokens (cache: 20)Encoded 30/318 tokens (cache: 30)Encoded 40/318 tokens (cache: 40)Encoded 50/318 tokens (cache: 50)Encoded 60/318 tokens (cache: 60)Encoded 70/318 tokens (cache: 70)Encoded 80/318 tokens (cache: 80)Encoded 90/318 tokens (cache: 90)Encoded 100/318 tokens (cache: 100)Encoded 110/318 tokens (cache: 110)Encoded 120/318 tokens (cache: 120)Encoded 130/318 tokens (cache: 130)Encoded 140/318 tokens (cache: 140)Encoded 150/318 tokens (cache: 150)Encoded 160/318 tokens (cache: 160)Encoded 170/318 tokens (cache: 170)Encoded 180/318 tokens (cache: 180)Encoded 190/318 tokens (cache: 190)Encoded 200/318 tokens (cache: 200)Encoded 210/318 tokens (cache: 210)Encoded 220/318 tokens (cache: 220)Encoded 230/318 tokens (cache: 230)Encoded 240/318 tokens (cache: 240)Encoded 250/318 tokens (cache: 250)Encoded 260/318 tokens (cache: 260)Encoded 270/318 tokens (cache: 270)Encoded 280/318 tokens (cache: 280)Encoded 290/318 tokens (cache: 290)Encoded 300/318 tokens (cache: 300)Encoded 310/318 tokens (cache: 310)
Encoding complete.
Original size: 1155 chars
Compressed size: 22 bytes
Bits per character: 0.1524
Compression finished in 25.17 seconds.
Results saved to benchmarks.db
Results saved to model_results.json
  BPC: 0.1524 | Ratio: 52.5000

[5/35] Qwen 2.5-3B | generated_gemma3_4b_5.py.txt
Loading model from /home/alexn/.cache/huggingface/hub/models--Qwen--Qwen2.5-3B/snapshots/3aab1f1954e9cc14eb9509a215f9e5ca08227a9b...
`torch_dtype` is deprecated! Use `dtype` instead!
2026-02-16 23:21:21.216544: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
/home/alexn/Documents/code/research/LLMzip_from_scratch/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:6105: UserWarning: expandable_segments not supported on this platform (Triggered internally at /pytorch/c10/hip/HIPAllocatorConfig.h:29.)
  _ = torch.empty(byte_count // factor, dtype=torch.float16, device=device, requires_grad=False)
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  4.00s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  2.85s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.02s/it]
Compressing /home/alexn/Documents/code/research/LLMzip_from_scratch/source_text/generated_gemma3_4b_5.py.txt to /home/alexn/Documents/code/research/LLMzip_from_scratch/encoded_documents/selfcomp_Qwen_253B_generated_gemma3_4b_5.py_w50.llmzip...
Total tokens to encode: 401
/home/alexn/Documents/code/research/LLMzip_from_scratch/.venv/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:301: UserWarning: Attempting to use hipBLASLt on an unsupported architecture! Overriding blas backend to hipblas (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:296.)
  freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
Encoded 10/401 tokens (cache: 10)Encoded 20/401 tokens (cache: 20)Encoded 30/401 tokens (cache: 30)Encoded 40/401 tokens (cache: 40)Encoded 50/401 tokens (cache: 50)Encoded 60/401 tokens (cache: 60)Encoded 70/401 tokens (cache: 70)Encoded 80/401 tokens (cache: 80)Encoded 90/401 tokens (cache: 90)Encoded 100/401 tokens (cache: 100)Encoded 110/401 tokens (cache: 110)Encoded 120/401 tokens (cache: 120)Encoded 130/401 tokens (cache: 130)Encoded 140/401 tokens (cache: 140)Encoded 150/401 tokens (cache: 150)Encoded 160/401 tokens (cache: 160)Encoded 170/401 tokens (cache: 170)Encoded 180/401 tokens (cache: 180)Encoded 190/401 tokens (cache: 190)Encoded 200/401 tokens (cache: 200)Encoded 210/401 tokens (cache: 210)Encoded 220/401 tokens (cache: 220)Encoded 230/401 tokens (cache: 230)Encoded 240/401 tokens (cache: 240)Encoded 250/401 tokens (cache: 250)Encoded 260/401 tokens (cache: 260)Encoded 270/401 tokens (cache: 270)Encoded 280/401 tokens (cache: 280)Encoded 290/401 tokens (cache: 290)Encoded 300/401 tokens (cache: 300)Encoded 310/401 tokens (cache: 310)Encoded 320/401 tokens (cache: 320)Encoded 330/401 tokens (cache: 330)Encoded 340/401 tokens (cache: 340)Encoded 350/401 tokens (cache: 350)Encoded 360/401 tokens (cache: 360)Encoded 370/401 tokens (cache: 370)Encoded 380/401 tokens (cache: 380)Encoded 390/401 tokens (cache: 390)Encoded 400/401 tokens (cache: 400)
Encoding complete.
Original size: 1779 chars
Compressed size: 34 bytes
Bits per character: 0.1529
Compression finished in 31.74 seconds.
Results saved to benchmarks.db
Results saved to model_results.json
  BPC: 0.1529 | Ratio: 52.3235

[6/35] LiquidAI 1.2B | generated_gemma3_4b_1.py.txt
Loading model from /home/alexn/.cache/huggingface/hub/models--LiquidAI--LFM2.5-1.2B-Base/snapshots/1e601c5c9d33bcc8da794c253243d6b258a4d38b...
`torch_dtype` is deprecated! Use `dtype` instead!
2026-02-16 23:22:05.684541: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
/home/alexn/Documents/code/research/LLMzip_from_scratch/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:6105: UserWarning: expandable_segments not supported on this platform (Triggered internally at /pytorch/c10/hip/HIPAllocatorConfig.h:29.)
  _ = torch.empty(byte_count // factor, dtype=torch.float16, device=device, requires_grad=False)
Compressing /home/alexn/Documents/code/research/LLMzip_from_scratch/source_text/generated_gemma3_4b_1.py.txt to /home/alexn/Documents/code/research/LLMzip_from_scratch/encoded_documents/selfcomp_LiquidAI_12B_generated_gemma3_4b_1.py_w50.llmzip...
Total tokens to encode: 458
/home/alexn/Documents/code/research/LLMzip_from_scratch/.venv/lib/python3.12/site-packages/transformers/models/lfm2/modeling_lfm2.py:97: UserWarning: Attempting to use hipBLASLt on an unsupported architecture! Overriding blas backend to hipblas (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:296.)
  freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
Encoded 10/458 tokens (cache: 10)Encoded 20/458 tokens (cache: 20)Encoded 30/458 tokens (cache: 30)Encoded 40/458 tokens (cache: 40)Encoded 50/458 tokens (cache: 50)Encoded 60/458 tokens (cache: 60)Encoded 70/458 tokens (cache: 70)Encoded 80/458 tokens (cache: 80)Encoded 90/458 tokens (cache: 90)Encoded 100/458 tokens (cache: 100)Encoded 110/458 tokens (cache: 110)Encoded 120/458 tokens (cache: 120)Encoded 130/458 tokens (cache: 130)Encoded 140/458 tokens (cache: 140)Encoded 150/458 tokens (cache: 150)Encoded 160/458 tokens (cache: 160)Encoded 170/458 tokens (cache: 170)Encoded 180/458 tokens (cache: 180)Encoded 190/458 tokens (cache: 190)Encoded 200/458 tokens (cache: 200)Encoded 210/458 tokens (cache: 210)Encoded 220/458 tokens (cache: 220)Encoded 230/458 tokens (cache: 230)Encoded 240/458 tokens (cache: 240)Encoded 250/458 tokens (cache: 250)Encoded 260/458 tokens (cache: 260)Encoded 270/458 tokens (cache: 270)Encoded 280/458 tokens (cache: 280)Encoded 290/458 tokens (cache: 290)Encoded 300/458 tokens (cache: 300)Encoded 310/458 tokens (cache: 310)Encoded 320/458 tokens (cache: 320)Encoded 330/458 tokens (cache: 330)Encoded 340/458 tokens (cache: 340)Encoded 350/458 tokens (cache: 350)Encoded 360/458 tokens (cache: 360)Encoded 370/458 tokens (cache: 370)Encoded 380/458 tokens (cache: 380)Encoded 390/458 tokens (cache: 390)Encoded 400/458 tokens (cache: 400)Encoded 410/458 tokens (cache: 410)Encoded 420/458 tokens (cache: 420)Encoded 430/458 tokens (cache: 430)Encoded 440/458 tokens (cache: 440)Encoded 450/458 tokens (cache: 450)
Encoding complete.
Original size: 1907 chars
Compressed size: 38 bytes
Bits per character: 0.1594
Compression finished in 18.46 seconds.
Results saved to benchmarks.db
Results saved to model_results.json
  BPC: 0.1594 | Ratio: 50.1842

[7/35] LiquidAI 1.2B | generated_gemma3_4b_2.py.txt
Loading model from /home/alexn/.cache/huggingface/hub/models--LiquidAI--LFM2.5-1.2B-Base/snapshots/1e601c5c9d33bcc8da794c253243d6b258a4d38b...
`torch_dtype` is deprecated! Use `dtype` instead!
2026-02-16 23:22:33.579244: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
/home/alexn/Documents/code/research/LLMzip_from_scratch/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:6105: UserWarning: expandable_segments not supported on this platform (Triggered internally at /pytorch/c10/hip/HIPAllocatorConfig.h:29.)
  _ = torch.empty(byte_count // factor, dtype=torch.float16, device=device, requires_grad=False)
Compressing /home/alexn/Documents/code/research/LLMzip_from_scratch/source_text/generated_gemma3_4b_2.py.txt to /home/alexn/Documents/code/research/LLMzip_from_scratch/encoded_documents/selfcomp_LiquidAI_12B_generated_gemma3_4b_2.py_w50.llmzip...
Total tokens to encode: 450
/home/alexn/Documents/code/research/LLMzip_from_scratch/.venv/lib/python3.12/site-packages/transformers/models/lfm2/modeling_lfm2.py:97: UserWarning: Attempting to use hipBLASLt on an unsupported architecture! Overriding blas backend to hipblas (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:296.)
  freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
Encoded 10/450 tokens (cache: 10)Encoded 20/450 tokens (cache: 20)Encoded 30/450 tokens (cache: 30)Encoded 40/450 tokens (cache: 40)Encoded 50/450 tokens (cache: 50)Encoded 60/450 tokens (cache: 60)Encoded 70/450 tokens (cache: 70)Encoded 80/450 tokens (cache: 80)Encoded 90/450 tokens (cache: 90)Encoded 100/450 tokens (cache: 100)Encoded 110/450 tokens (cache: 110)Encoded 120/450 tokens (cache: 120)Encoded 130/450 tokens (cache: 130)Encoded 140/450 tokens (cache: 140)Encoded 150/450 tokens (cache: 150)Encoded 160/450 tokens (cache: 160)Encoded 170/450 tokens (cache: 170)Encoded 180/450 tokens (cache: 180)Encoded 190/450 tokens (cache: 190)Encoded 200/450 tokens (cache: 200)Encoded 210/450 tokens (cache: 210)Encoded 220/450 tokens (cache: 220)Encoded 230/450 tokens (cache: 230)Encoded 240/450 tokens (cache: 240)Encoded 250/450 tokens (cache: 250)Encoded 260/450 tokens (cache: 260)Encoded 270/450 tokens (cache: 270)Encoded 280/450 tokens (cache: 280)Encoded 290/450 tokens (cache: 290)Encoded 300/450 tokens (cache: 300)Encoded 310/450 tokens (cache: 310)Encoded 320/450 tokens (cache: 320)Encoded 330/450 tokens (cache: 330)Encoded 340/450 tokens (cache: 340)Encoded 350/450 tokens (cache: 350)Encoded 360/450 tokens (cache: 360)Encoded 370/450 tokens (cache: 370)Encoded 380/450 tokens (cache: 380)Encoded 390/450 tokens (cache: 390)Encoded 400/450 tokens (cache: 400)Encoded 410/450 tokens (cache: 410)Encoded 420/450 tokens (cache: 420)Encoded 430/450 tokens (cache: 430)Encoded 440/450 tokens (cache: 440)
Encoding complete.
Original size: 1789 chars
Compressed size: 52 bytes
Bits per character: 0.2325
Compression finished in 18.05 seconds.
Results saved to benchmarks.db
Results saved to model_results.json
  BPC: 0.2325 | Ratio: 34.4038

[8/35] LiquidAI 1.2B | generated_gemma3_4b_3.py.txt
Loading model from /home/alexn/.cache/huggingface/hub/models--LiquidAI--LFM2.5-1.2B-Base/snapshots/1e601c5c9d33bcc8da794c253243d6b258a4d38b...
`torch_dtype` is deprecated! Use `dtype` instead!
2026-02-16 23:23:00.181871: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
/home/alexn/Documents/code/research/LLMzip_from_scratch/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:6105: UserWarning: expandable_segments not supported on this platform (Triggered internally at /pytorch/c10/hip/HIPAllocatorConfig.h:29.)
  _ = torch.empty(byte_count // factor, dtype=torch.float16, device=device, requires_grad=False)
Compressing /home/alexn/Documents/code/research/LLMzip_from_scratch/source_text/generated_gemma3_4b_3.py.txt to /home/alexn/Documents/code/research/LLMzip_from_scratch/encoded_documents/selfcomp_LiquidAI_12B_generated_gemma3_4b_3.py_w50.llmzip...
Total tokens to encode: 368
/home/alexn/Documents/code/research/LLMzip_from_scratch/.venv/lib/python3.12/site-packages/transformers/models/lfm2/modeling_lfm2.py:97: UserWarning: Attempting to use hipBLASLt on an unsupported architecture! Overriding blas backend to hipblas (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:296.)
  freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
Encoded 10/368 tokens (cache: 10)Encoded 20/368 tokens (cache: 20)Encoded 30/368 tokens (cache: 30)Encoded 40/368 tokens (cache: 40)Encoded 50/368 tokens (cache: 50)Encoded 60/368 tokens (cache: 60)Encoded 70/368 tokens (cache: 70)Encoded 80/368 tokens (cache: 80)Encoded 90/368 tokens (cache: 90)Encoded 100/368 tokens (cache: 100)Encoded 110/368 tokens (cache: 110)Encoded 120/368 tokens (cache: 120)Encoded 130/368 tokens (cache: 130)Encoded 140/368 tokens (cache: 140)Encoded 150/368 tokens (cache: 150)Encoded 160/368 tokens (cache: 160)Encoded 170/368 tokens (cache: 170)Encoded 180/368 tokens (cache: 180)Encoded 190/368 tokens (cache: 190)Encoded 200/368 tokens (cache: 200)Encoded 210/368 tokens (cache: 210)Encoded 220/368 tokens (cache: 220)Encoded 230/368 tokens (cache: 230)Encoded 240/368 tokens (cache: 240)Encoded 250/368 tokens (cache: 250)Encoded 260/368 tokens (cache: 260)Encoded 270/368 tokens (cache: 270)Encoded 280/368 tokens (cache: 280)Encoded 290/368 tokens (cache: 290)Encoded 300/368 tokens (cache: 300)Encoded 310/368 tokens (cache: 310)Encoded 320/368 tokens (cache: 320)Encoded 330/368 tokens (cache: 330)Encoded 340/368 tokens (cache: 340)Encoded 350/368 tokens (cache: 350)Encoded 360/368 tokens (cache: 360)
Encoding complete.
Original size: 1093 chars
Compressed size: 39 bytes
Bits per character: 0.2855
Compression finished in 14.80 seconds.
Results saved to benchmarks.db
Results saved to model_results.json
  BPC: 0.2855 | Ratio: 28.0256

[9/35] LiquidAI 1.2B | generated_gemma3_4b_4.py.txt
Loading model from /home/alexn/.cache/huggingface/hub/models--LiquidAI--LFM2.5-1.2B-Base/snapshots/1e601c5c9d33bcc8da794c253243d6b258a4d38b...
`torch_dtype` is deprecated! Use `dtype` instead!
2026-02-16 23:23:23.585191: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
/home/alexn/Documents/code/research/LLMzip_from_scratch/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:6105: UserWarning: expandable_segments not supported on this platform (Triggered internally at /pytorch/c10/hip/HIPAllocatorConfig.h:29.)
  _ = torch.empty(byte_count // factor, dtype=torch.float16, device=device, requires_grad=False)
Compressing /home/alexn/Documents/code/research/LLMzip_from_scratch/source_text/generated_gemma3_4b_4.py.txt to /home/alexn/Documents/code/research/LLMzip_from_scratch/encoded_documents/selfcomp_LiquidAI_12B_generated_gemma3_4b_4.py_w50.llmzip...
Total tokens to encode: 352
/home/alexn/Documents/code/research/LLMzip_from_scratch/.venv/lib/python3.12/site-packages/transformers/models/lfm2/modeling_lfm2.py:97: UserWarning: Attempting to use hipBLASLt on an unsupported architecture! Overriding blas backend to hipblas (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:296.)
  freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
Encoded 10/352 tokens (cache: 10)Encoded 20/352 tokens (cache: 20)Encoded 30/352 tokens (cache: 30)Encoded 40/352 tokens (cache: 40)Encoded 50/352 tokens (cache: 50)Encoded 60/352 tokens (cache: 60)Encoded 70/352 tokens (cache: 70)Encoded 80/352 tokens (cache: 80)Encoded 90/352 tokens (cache: 90)Encoded 100/352 tokens (cache: 100)Encoded 110/352 tokens (cache: 110)Encoded 120/352 tokens (cache: 120)Encoded 130/352 tokens (cache: 130)Encoded 140/352 tokens (cache: 140)Encoded 150/352 tokens (cache: 150)Encoded 160/352 tokens (cache: 160)Encoded 170/352 tokens (cache: 170)Encoded 180/352 tokens (cache: 180)Encoded 190/352 tokens (cache: 190)Encoded 200/352 tokens (cache: 200)Encoded 210/352 tokens (cache: 210)Encoded 220/352 tokens (cache: 220)Encoded 230/352 tokens (cache: 230)Encoded 240/352 tokens (cache: 240)Encoded 250/352 tokens (cache: 250)Encoded 260/352 tokens (cache: 260)Encoded 270/352 tokens (cache: 270)Encoded 280/352 tokens (cache: 280)Encoded 290/352 tokens (cache: 290)Encoded 300/352 tokens (cache: 300)Encoded 310/352 tokens (cache: 310)Encoded 320/352 tokens (cache: 320)Encoded 330/352 tokens (cache: 330)Encoded 340/352 tokens (cache: 340)Encoded 350/352 tokens (cache: 350)
Encoding complete.
Original size: 1155 chars
Compressed size: 33 bytes
Bits per character: 0.2286
Compression finished in 14.25 seconds.
Results saved to benchmarks.db
Results saved to model_results.json
  BPC: 0.2286 | Ratio: 35.0000

[10/35] LiquidAI 1.2B | generated_gemma3_4b_5.py.txt
Loading model from /home/alexn/.cache/huggingface/hub/models--LiquidAI--LFM2.5-1.2B-Base/snapshots/1e601c5c9d33bcc8da794c253243d6b258a4d38b...
`torch_dtype` is deprecated! Use `dtype` instead!
2026-02-16 23:23:46.401773: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
/home/alexn/Documents/code/research/LLMzip_from_scratch/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:6105: UserWarning: expandable_segments not supported on this platform (Triggered internally at /pytorch/c10/hip/HIPAllocatorConfig.h:29.)
  _ = torch.empty(byte_count // factor, dtype=torch.float16, device=device, requires_grad=False)
Compressing /home/alexn/Documents/code/research/LLMzip_from_scratch/source_text/generated_gemma3_4b_5.py.txt to /home/alexn/Documents/code/research/LLMzip_from_scratch/encoded_documents/selfcomp_LiquidAI_12B_generated_gemma3_4b_5.py_w50.llmzip...
Total tokens to encode: 440
/home/alexn/Documents/code/research/LLMzip_from_scratch/.venv/lib/python3.12/site-packages/transformers/models/lfm2/modeling_lfm2.py:97: UserWarning: Attempting to use hipBLASLt on an unsupported architecture! Overriding blas backend to hipblas (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:296.)
  freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
Encoded 10/440 tokens (cache: 10)Encoded 20/440 tokens (cache: 20)Encoded 30/440 tokens (cache: 30)Encoded 40/440 tokens (cache: 40)Encoded 50/440 tokens (cache: 50)Encoded 60/440 tokens (cache: 60)Encoded 70/440 tokens (cache: 70)Encoded 80/440 tokens (cache: 80)Encoded 90/440 tokens (cache: 90)Encoded 100/440 tokens (cache: 100)Encoded 110/440 tokens (cache: 110)Encoded 120/440 tokens (cache: 120)Encoded 130/440 tokens (cache: 130)Encoded 140/440 tokens (cache: 140)Encoded 150/440 tokens (cache: 150)Encoded 160/440 tokens (cache: 160)Encoded 170/440 tokens (cache: 170)Encoded 180/440 tokens (cache: 180)Encoded 190/440 tokens (cache: 190)Encoded 200/440 tokens (cache: 200)Encoded 210/440 tokens (cache: 210)Encoded 220/440 tokens (cache: 220)Encoded 230/440 tokens (cache: 230)Encoded 240/440 tokens (cache: 240)Encoded 250/440 tokens (cache: 250)Encoded 260/440 tokens (cache: 260)Encoded 270/440 tokens (cache: 270)Encoded 280/440 tokens (cache: 280)Encoded 290/440 tokens (cache: 290)Encoded 300/440 tokens (cache: 300)Encoded 310/440 tokens (cache: 310)Encoded 320/440 tokens (cache: 320)Encoded 330/440 tokens (cache: 330)Encoded 340/440 tokens (cache: 340)Encoded 350/440 tokens (cache: 350)Encoded 360/440 tokens (cache: 360)Encoded 370/440 tokens (cache: 370)Encoded 380/440 tokens (cache: 380)Encoded 390/440 tokens (cache: 390)Encoded 400/440 tokens (cache: 400)Encoded 410/440 tokens (cache: 410)Encoded 420/440 tokens (cache: 420)Encoded 430/440 tokens (cache: 430)
Encoding complete.
Original size: 1779 chars
Compressed size: 61 bytes
Bits per character: 0.2743
Compression finished in 17.76 seconds.
Results saved to benchmarks.db
Results saved to model_results.json
  BPC: 0.2743 | Ratio: 29.1639

[11/35] Gemma 3-1B | generated_gemma3_4b_1.py.txt
Loading model from /home/alexn/.cache/huggingface/hub/models--google--gemma-3-1b-pt/snapshots/fcf18a2a879aab110ca39f8bffbccd5d49d8eb29...
`torch_dtype` is deprecated! Use `dtype` instead!
2026-02-16 23:24:13.735348: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
/home/alexn/Documents/code/research/LLMzip_from_scratch/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:6105: UserWarning: expandable_segments not supported on this platform (Triggered internally at /pytorch/c10/hip/HIPAllocatorConfig.h:29.)
  _ = torch.empty(byte_count // factor, dtype=torch.float16, device=device, requires_grad=False)
Compressing /home/alexn/Documents/code/research/LLMzip_from_scratch/source_text/generated_gemma3_4b_1.py.txt to /home/alexn/Documents/code/research/LLMzip_from_scratch/encoded_documents/selfcomp_Gemma_31B_generated_gemma3_4b_1.py_w50.llmzip...
Total tokens to encode: 521
/home/alexn/Documents/code/research/LLMzip_from_scratch/.venv/lib/python3.12/site-packages/transformers/models/gemma3/modeling_gemma3.py:174: UserWarning: Attempting to use hipBLASLt on an unsupported architecture! Overriding blas backend to hipblas (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:296.)
  freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
Encoded 10/521 tokens (cache: 10)Encoded 20/521 tokens (cache: 20)Encoded 30/521 tokens (cache: 30)Encoded 40/521 tokens (cache: 40)Encoded 50/521 tokens (cache: 50)Encoded 60/521 tokens (cache: 60)Encoded 70/521 tokens (cache: 70)Encoded 80/521 tokens (cache: 80)Encoded 90/521 tokens (cache: 90)Encoded 100/521 tokens (cache: 100)Encoded 110/521 tokens (cache: 110)Encoded 120/521 tokens (cache: 120)Encoded 130/521 tokens (cache: 130)Encoded 140/521 tokens (cache: 140)Encoded 150/521 tokens (cache: 150)Encoded 160/521 tokens (cache: 160)Encoded 170/521 tokens (cache: 170)Encoded 180/521 tokens (cache: 180)Encoded 190/521 tokens (cache: 190)Encoded 200/521 tokens (cache: 200)Encoded 210/521 tokens (cache: 210)Encoded 220/521 tokens (cache: 220)Encoded 230/521 tokens (cache: 230)Encoded 240/521 tokens (cache: 240)Encoded 250/521 tokens (cache: 250)Encoded 260/521 tokens (cache: 260)Encoded 270/521 tokens (cache: 270)Encoded 280/521 tokens (cache: 280)Encoded 290/521 tokens (cache: 290)Encoded 300/521 tokens (cache: 300)Encoded 310/521 tokens (cache: 310)Encoded 320/521 tokens (cache: 320)Encoded 330/521 tokens (cache: 330)Encoded 340/521 tokens (cache: 340)Encoded 350/521 tokens (cache: 350)Encoded 360/521 tokens (cache: 360)Encoded 370/521 tokens (cache: 370)Encoded 380/521 tokens (cache: 380)Encoded 390/521 tokens (cache: 390)Encoded 400/521 tokens (cache: 400)Encoded 410/521 tokens (cache: 410)Encoded 420/521 tokens (cache: 420)Encoded 430/521 tokens (cache: 430)Encoded 440/521 tokens (cache: 440)Encoded 450/521 tokens (cache: 450)Encoded 460/521 tokens (cache: 460)Encoded 470/521 tokens (cache: 470)Encoded 480/521 tokens (cache: 480)Encoded 490/521 tokens (cache: 490)Encoded 500/521 tokens (cache: 500)Encoded 510/521 tokens (cache: 510)Encoded 520/521 tokens (cache: 520)
Encoding complete.
Original size: 1907 chars
Compressed size: 56 bytes
Bits per character: 0.2349
Compression finished in 68.75 seconds.
Results saved to benchmarks.db
Results saved to model_results.json
  BPC: 0.2349 | Ratio: 34.0536

[12/35] Gemma 3-1B | generated_gemma3_4b_2.py.txt
Loading model from /home/alexn/.cache/huggingface/hub/models--google--gemma-3-1b-pt/snapshots/fcf18a2a879aab110ca39f8bffbccd5d49d8eb29...
`torch_dtype` is deprecated! Use `dtype` instead!
2026-02-16 23:25:42.574438: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
/home/alexn/Documents/code/research/LLMzip_from_scratch/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:6105: UserWarning: expandable_segments not supported on this platform (Triggered internally at /pytorch/c10/hip/HIPAllocatorConfig.h:29.)
  _ = torch.empty(byte_count // factor, dtype=torch.float16, device=device, requires_grad=False)
Compressing /home/alexn/Documents/code/research/LLMzip_from_scratch/source_text/generated_gemma3_4b_2.py.txt to /home/alexn/Documents/code/research/LLMzip_from_scratch/encoded_documents/selfcomp_Gemma_31B_generated_gemma3_4b_2.py_w50.llmzip...
Total tokens to encode: 473
/home/alexn/Documents/code/research/LLMzip_from_scratch/.venv/lib/python3.12/site-packages/transformers/models/gemma3/modeling_gemma3.py:174: UserWarning: Attempting to use hipBLASLt on an unsupported architecture! Overriding blas backend to hipblas (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:296.)
  freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
Encoded 10/473 tokens (cache: 10)Encoded 20/473 tokens (cache: 20)Encoded 30/473 tokens (cache: 30)Encoded 40/473 tokens (cache: 40)Encoded 50/473 tokens (cache: 50)Encoded 60/473 tokens (cache: 60)Encoded 70/473 tokens (cache: 70)Encoded 80/473 tokens (cache: 80)Encoded 90/473 tokens (cache: 90)Encoded 100/473 tokens (cache: 100)Encoded 110/473 tokens (cache: 110)Encoded 120/473 tokens (cache: 120)Encoded 130/473 tokens (cache: 130)Encoded 140/473 tokens (cache: 140)Encoded 150/473 tokens (cache: 150)Encoded 160/473 tokens (cache: 160)Encoded 170/473 tokens (cache: 170)Encoded 180/473 tokens (cache: 180)Encoded 190/473 tokens (cache: 190)Encoded 200/473 tokens (cache: 200)Encoded 210/473 tokens (cache: 210)Encoded 220/473 tokens (cache: 220)Encoded 230/473 tokens (cache: 230)Encoded 240/473 tokens (cache: 240)Encoded 250/473 tokens (cache: 250)Encoded 260/473 tokens (cache: 260)Encoded 270/473 tokens (cache: 270)Encoded 280/473 tokens (cache: 280)Encoded 290/473 tokens (cache: 290)Encoded 300/473 tokens (cache: 300)Encoded 310/473 tokens (cache: 310)Encoded 320/473 tokens (cache: 320)Encoded 330/473 tokens (cache: 330)Encoded 340/473 tokens (cache: 340)Encoded 350/473 tokens (cache: 350)Encoded 360/473 tokens (cache: 360)Encoded 370/473 tokens (cache: 370)Encoded 380/473 tokens (cache: 380)Encoded 390/473 tokens (cache: 390)Encoded 400/473 tokens (cache: 400)Encoded 410/473 tokens (cache: 410)Encoded 420/473 tokens (cache: 420)Encoded 430/473 tokens (cache: 430)Encoded 440/473 tokens (cache: 440)Encoded 450/473 tokens (cache: 450)Encoded 460/473 tokens (cache: 460)Encoded 470/473 tokens (cache: 470)
Encoding complete.
Original size: 1789 chars
Compressed size: 85 bytes
Bits per character: 0.3801
Compression finished in 62.28 seconds.
Results saved to benchmarks.db
Results saved to model_results.json
  BPC: 0.3801 | Ratio: 21.0471

[13/35] Gemma 3-1B | generated_gemma3_4b_3.py.txt
Loading model from /home/alexn/.cache/huggingface/hub/models--google--gemma-3-1b-pt/snapshots/fcf18a2a879aab110ca39f8bffbccd5d49d8eb29...
`torch_dtype` is deprecated! Use `dtype` instead!
2026-02-16 23:26:54.851198: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
/home/alexn/Documents/code/research/LLMzip_from_scratch/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:6105: UserWarning: expandable_segments not supported on this platform (Triggered internally at /pytorch/c10/hip/HIPAllocatorConfig.h:29.)
  _ = torch.empty(byte_count // factor, dtype=torch.float16, device=device, requires_grad=False)
Compressing /home/alexn/Documents/code/research/LLMzip_from_scratch/source_text/generated_gemma3_4b_3.py.txt to /home/alexn/Documents/code/research/LLMzip_from_scratch/encoded_documents/selfcomp_Gemma_31B_generated_gemma3_4b_3.py_w50.llmzip...
Total tokens to encode: 408
/home/alexn/Documents/code/research/LLMzip_from_scratch/.venv/lib/python3.12/site-packages/transformers/models/gemma3/modeling_gemma3.py:174: UserWarning: Attempting to use hipBLASLt on an unsupported architecture! Overriding blas backend to hipblas (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:296.)
  freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
Encoded 10/408 tokens (cache: 10)Encoded 20/408 tokens (cache: 20)Encoded 30/408 tokens (cache: 30)Encoded 40/408 tokens (cache: 40)Encoded 50/408 tokens (cache: 50)Encoded 60/408 tokens (cache: 60)Encoded 70/408 tokens (cache: 70)Encoded 80/408 tokens (cache: 80)Encoded 90/408 tokens (cache: 90)Encoded 100/408 tokens (cache: 100)Encoded 110/408 tokens (cache: 110)Encoded 120/408 tokens (cache: 120)Encoded 130/408 tokens (cache: 130)Encoded 140/408 tokens (cache: 140)Encoded 150/408 tokens (cache: 150)Encoded 160/408 tokens (cache: 160)Encoded 170/408 tokens (cache: 170)Encoded 180/408 tokens (cache: 180)Encoded 190/408 tokens (cache: 190)Encoded 200/408 tokens (cache: 200)Encoded 210/408 tokens (cache: 210)Encoded 220/408 tokens (cache: 220)Encoded 230/408 tokens (cache: 230)Encoded 240/408 tokens (cache: 240)Encoded 250/408 tokens (cache: 250)Encoded 260/408 tokens (cache: 260)Encoded 270/408 tokens (cache: 270)Encoded 280/408 tokens (cache: 280)Encoded 290/408 tokens (cache: 290)Encoded 300/408 tokens (cache: 300)Encoded 310/408 tokens (cache: 310)Encoded 320/408 tokens (cache: 320)Encoded 330/408 tokens (cache: 330)Encoded 340/408 tokens (cache: 340)Encoded 350/408 tokens (cache: 350)Encoded 360/408 tokens (cache: 360)Encoded 370/408 tokens (cache: 370)Encoded 380/408 tokens (cache: 380)Encoded 390/408 tokens (cache: 390)Encoded 400/408 tokens (cache: 400)
Encoding complete.
Original size: 1093 chars
Compressed size: 51 bytes
Bits per character: 0.3733
Compression finished in 53.73 seconds.
Results saved to benchmarks.db
Results saved to model_results.json
  BPC: 0.3733 | Ratio: 21.4314

[14/35] Gemma 3-1B | generated_gemma3_4b_4.py.txt
Loading model from /home/alexn/.cache/huggingface/hub/models--google--gemma-3-1b-pt/snapshots/fcf18a2a879aab110ca39f8bffbccd5d49d8eb29...
`torch_dtype` is deprecated! Use `dtype` instead!
2026-02-16 23:27:58.272240: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
/home/alexn/Documents/code/research/LLMzip_from_scratch/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:6105: UserWarning: expandable_segments not supported on this platform (Triggered internally at /pytorch/c10/hip/HIPAllocatorConfig.h:29.)
  _ = torch.empty(byte_count // factor, dtype=torch.float16, device=device, requires_grad=False)
Compressing /home/alexn/Documents/code/research/LLMzip_from_scratch/source_text/generated_gemma3_4b_4.py.txt to /home/alexn/Documents/code/research/LLMzip_from_scratch/encoded_documents/selfcomp_Gemma_31B_generated_gemma3_4b_4.py_w50.llmzip...
Total tokens to encode: 377
/home/alexn/Documents/code/research/LLMzip_from_scratch/.venv/lib/python3.12/site-packages/transformers/models/gemma3/modeling_gemma3.py:174: UserWarning: Attempting to use hipBLASLt on an unsupported architecture! Overriding blas backend to hipblas (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:296.)
  freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
Encoded 10/377 tokens (cache: 10)Encoded 20/377 tokens (cache: 20)Encoded 30/377 tokens (cache: 30)Encoded 40/377 tokens (cache: 40)Encoded 50/377 tokens (cache: 50)Encoded 60/377 tokens (cache: 60)Encoded 70/377 tokens (cache: 70)Encoded 80/377 tokens (cache: 80)Encoded 90/377 tokens (cache: 90)Encoded 100/377 tokens (cache: 100)Encoded 110/377 tokens (cache: 110)Encoded 120/377 tokens (cache: 120)Encoded 130/377 tokens (cache: 130)Encoded 140/377 tokens (cache: 140)Encoded 150/377 tokens (cache: 150)Encoded 160/377 tokens (cache: 160)Encoded 170/377 tokens (cache: 170)Encoded 180/377 tokens (cache: 180)Encoded 190/377 tokens (cache: 190)Encoded 200/377 tokens (cache: 200)Encoded 210/377 tokens (cache: 210)Encoded 220/377 tokens (cache: 220)Encoded 230/377 tokens (cache: 230)Encoded 240/377 tokens (cache: 240)Encoded 250/377 tokens (cache: 250)Encoded 260/377 tokens (cache: 260)Encoded 270/377 tokens (cache: 270)Encoded 280/377 tokens (cache: 280)Encoded 290/377 tokens (cache: 290)Encoded 300/377 tokens (cache: 300)Encoded 310/377 tokens (cache: 310)Encoded 320/377 tokens (cache: 320)Encoded 330/377 tokens (cache: 330)Encoded 340/377 tokens (cache: 340)Encoded 350/377 tokens (cache: 350)Encoded 360/377 tokens (cache: 360)Encoded 370/377 tokens (cache: 370)
Encoding complete.
Original size: 1155 chars
Compressed size: 48 bytes
Bits per character: 0.3325
Compression finished in 49.71 seconds.
Results saved to benchmarks.db
Results saved to model_results.json
  BPC: 0.3325 | Ratio: 24.0625

[15/35] Gemma 3-1B | generated_gemma3_4b_5.py.txt
Loading model from /home/alexn/.cache/huggingface/hub/models--google--gemma-3-1b-pt/snapshots/fcf18a2a879aab110ca39f8bffbccd5d49d8eb29...
`torch_dtype` is deprecated! Use `dtype` instead!
2026-02-16 23:28:58.067065: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
/home/alexn/Documents/code/research/LLMzip_from_scratch/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:6105: UserWarning: expandable_segments not supported on this platform (Triggered internally at /pytorch/c10/hip/HIPAllocatorConfig.h:29.)
  _ = torch.empty(byte_count // factor, dtype=torch.float16, device=device, requires_grad=False)
Compressing /home/alexn/Documents/code/research/LLMzip_from_scratch/source_text/generated_gemma3_4b_5.py.txt to /home/alexn/Documents/code/research/LLMzip_from_scratch/encoded_documents/selfcomp_Gemma_31B_generated_gemma3_4b_5.py_w50.llmzip...
Total tokens to encode: 526
/home/alexn/Documents/code/research/LLMzip_from_scratch/.venv/lib/python3.12/site-packages/transformers/models/gemma3/modeling_gemma3.py:174: UserWarning: Attempting to use hipBLASLt on an unsupported architecture! Overriding blas backend to hipblas (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:296.)
  freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
Encoded 10/526 tokens (cache: 10)Encoded 20/526 tokens (cache: 20)Encoded 30/526 tokens (cache: 30)Encoded 40/526 tokens (cache: 40)Encoded 50/526 tokens (cache: 50)Encoded 60/526 tokens (cache: 60)Encoded 70/526 tokens (cache: 70)Encoded 80/526 tokens (cache: 80)Encoded 90/526 tokens (cache: 90)Encoded 100/526 tokens (cache: 100)Encoded 110/526 tokens (cache: 110)Encoded 120/526 tokens (cache: 120)Encoded 130/526 tokens (cache: 130)Encoded 140/526 tokens (cache: 140)Encoded 150/526 tokens (cache: 150)Encoded 160/526 tokens (cache: 160)Encoded 170/526 tokens (cache: 170)Encoded 180/526 tokens (cache: 180)Encoded 190/526 tokens (cache: 190)Encoded 200/526 tokens (cache: 200)Encoded 210/526 tokens (cache: 210)Encoded 220/526 tokens (cache: 220)Encoded 230/526 tokens (cache: 230)Encoded 240/526 tokens (cache: 240)Encoded 250/526 tokens (cache: 250)Encoded 260/526 tokens (cache: 260)Encoded 270/526 tokens (cache: 270)Encoded 280/526 tokens (cache: 280)Encoded 290/526 tokens (cache: 290)Encoded 300/526 tokens (cache: 300)Encoded 310/526 tokens (cache: 310)Encoded 320/526 tokens (cache: 320)Encoded 330/526 tokens (cache: 330)Encoded 340/526 tokens (cache: 340)Encoded 350/526 tokens (cache: 350)Encoded 360/526 tokens (cache: 360)Encoded 370/526 tokens (cache: 370)Encoded 380/526 tokens (cache: 380)Encoded 390/526 tokens (cache: 390)Encoded 400/526 tokens (cache: 400)Encoded 410/526 tokens (cache: 410)Encoded 420/526 tokens (cache: 420)Encoded 430/526 tokens (cache: 430)Encoded 440/526 tokens (cache: 440)Encoded 450/526 tokens (cache: 450)Encoded 460/526 tokens (cache: 460)Encoded 470/526 tokens (cache: 470)Encoded 480/526 tokens (cache: 480)Encoded 490/526 tokens (cache: 490)Encoded 500/526 tokens (cache: 500)Encoded 510/526 tokens (cache: 510)Encoded 520/526 tokens (cache: 520)
Encoding complete.
Original size: 1779 chars
Compressed size: 71 bytes
Bits per character: 0.3193
Compression finished in 69.36 seconds.
Results saved to benchmarks.db
Results saved to model_results.json
  BPC: 0.3193 | Ratio: 25.0563

[16/35] Gemma 3-4B | generated_gemma3_4b_1.py.txt
Loading model from /home/alexn/.cache/huggingface/hub/models--google--gemma-3-4b-pt/snapshots/cc012e0a6d0787b4adcc0fa2c4da74402494554d...
`torch_dtype` is deprecated! Use `dtype` instead!
2026-02-16 23:30:17.308960: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
/home/alexn/Documents/code/research/LLMzip_from_scratch/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:6105: UserWarning: expandable_segments not supported on this platform (Triggered internally at /pytorch/c10/hip/HIPAllocatorConfig.h:29.)
  _ = torch.empty(byte_count // factor, dtype=torch.float16, device=device, requires_grad=False)
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:28<00:28, 28.86s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:43<00:00, 20.44s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:43<00:00, 21.70s/it]
Compressing /home/alexn/Documents/code/research/LLMzip_from_scratch/source_text/generated_gemma3_4b_1.py.txt to /home/alexn/Documents/code/research/LLMzip_from_scratch/encoded_documents/selfcomp_Gemma_34B_generated_gemma3_4b_1.py_w50.llmzip...
Total tokens to encode: 521
/home/alexn/Documents/code/research/LLMzip_from_scratch/.venv/lib/python3.12/site-packages/transformers/models/gemma3/modeling_gemma3.py:174: UserWarning: Attempting to use hipBLASLt on an unsupported architecture! Overriding blas backend to hipblas (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:296.)
  freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
Encoded 10/521 tokens (cache: 10)Encoded 20/521 tokens (cache: 20)Encoded 30/521 tokens (cache: 30)Encoded 40/521 tokens (cache: 40)Encoded 50/521 tokens (cache: 50)Encoded 60/521 tokens (cache: 60)Encoded 70/521 tokens (cache: 70)Encoded 80/521 tokens (cache: 80)Encoded 90/521 tokens (cache: 90)Encoded 100/521 tokens (cache: 100)Encoded 110/521 tokens (cache: 110)Encoded 120/521 tokens (cache: 120)Encoded 130/521 tokens (cache: 130)Encoded 140/521 tokens (cache: 140)Encoded 150/521 tokens (cache: 150)Encoded 160/521 tokens (cache: 160)Encoded 170/521 tokens (cache: 170)Encoded 180/521 tokens (cache: 180)Encoded 190/521 tokens (cache: 190)Encoded 200/521 tokens (cache: 200)Encoded 210/521 tokens (cache: 210)Encoded 220/521 tokens (cache: 220)Encoded 230/521 tokens (cache: 230)Encoded 240/521 tokens (cache: 240)Encoded 250/521 tokens (cache: 250)Encoded 260/521 tokens (cache: 260)Encoded 270/521 tokens (cache: 270)Encoded 280/521 tokens (cache: 280)Encoded 290/521 tokens (cache: 290)Encoded 300/521 tokens (cache: 300)Encoded 310/521 tokens (cache: 310)Encoded 320/521 tokens (cache: 320)Encoded 330/521 tokens (cache: 330)Encoded 340/521 tokens (cache: 340)Encoded 350/521 tokens (cache: 350)Encoded 360/521 tokens (cache: 360)Encoded 370/521 tokens (cache: 370)Encoded 380/521 tokens (cache: 380)Encoded 390/521 tokens (cache: 390)Encoded 400/521 tokens (cache: 400)Encoded 410/521 tokens (cache: 410)Encoded 420/521 tokens (cache: 420)Encoded 430/521 tokens (cache: 430)Encoded 440/521 tokens (cache: 440)Encoded 450/521 tokens (cache: 450)Encoded 460/521 tokens (cache: 460)Encoded 470/521 tokens (cache: 470)Encoded 480/521 tokens (cache: 480)Encoded 490/521 tokens (cache: 490)Encoded 500/521 tokens (cache: 500)Encoded 510/521 tokens (cache: 510)Encoded 520/521 tokens (cache: 520)
Encoding complete.
Original size: 1907 chars
Compressed size: 40 bytes
Bits per character: 0.1678
Compression finished in 211.56 seconds.
Results saved to benchmarks.db
Results saved to model_results.json
  BPC: 0.1678 | Ratio: 47.6750

[17/35] Gemma 3-4B | generated_gemma3_4b_2.py.txt
Loading model from /home/alexn/.cache/huggingface/hub/models--google--gemma-3-4b-pt/snapshots/cc012e0a6d0787b4adcc0fa2c4da74402494554d...
`torch_dtype` is deprecated! Use `dtype` instead!
2026-02-16 23:34:40.206475: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
/home/alexn/Documents/code/research/LLMzip_from_scratch/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:6105: UserWarning: expandable_segments not supported on this platform (Triggered internally at /pytorch/c10/hip/HIPAllocatorConfig.h:29.)
  _ = torch.empty(byte_count // factor, dtype=torch.float16, device=device, requires_grad=False)
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.82s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  5.19s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  5.43s/it]
Compressing /home/alexn/Documents/code/research/LLMzip_from_scratch/source_text/generated_gemma3_4b_2.py.txt to /home/alexn/Documents/code/research/LLMzip_from_scratch/encoded_documents/selfcomp_Gemma_34B_generated_gemma3_4b_2.py_w50.llmzip...
Total tokens to encode: 473
/home/alexn/Documents/code/research/LLMzip_from_scratch/.venv/lib/python3.12/site-packages/transformers/models/gemma3/modeling_gemma3.py:174: UserWarning: Attempting to use hipBLASLt on an unsupported architecture! Overriding blas backend to hipblas (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:296.)
  freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
Encoded 10/473 tokens (cache: 10)Encoded 20/473 tokens (cache: 20)Encoded 30/473 tokens (cache: 30)Encoded 40/473 tokens (cache: 40)Encoded 50/473 tokens (cache: 50)Encoded 60/473 tokens (cache: 60)Encoded 70/473 tokens (cache: 70)Encoded 80/473 tokens (cache: 80)Encoded 90/473 tokens (cache: 90)Encoded 100/473 tokens (cache: 100)Encoded 110/473 tokens (cache: 110)Encoded 120/473 tokens (cache: 120)Encoded 130/473 tokens (cache: 130)Encoded 140/473 tokens (cache: 140)Encoded 150/473 tokens (cache: 150)Encoded 160/473 tokens (cache: 160)Encoded 170/473 tokens (cache: 170)Encoded 180/473 tokens (cache: 180)Encoded 190/473 tokens (cache: 190)Encoded 200/473 tokens (cache: 200)Encoded 210/473 tokens (cache: 210)Encoded 220/473 tokens (cache: 220)Encoded 230/473 tokens (cache: 230)Encoded 240/473 tokens (cache: 240)Encoded 250/473 tokens (cache: 250)Encoded 260/473 tokens (cache: 260)Encoded 270/473 tokens (cache: 270)Encoded 280/473 tokens (cache: 280)Encoded 290/473 tokens (cache: 290)Encoded 300/473 tokens (cache: 300)Encoded 310/473 tokens (cache: 310)Encoded 320/473 tokens (cache: 320)Encoded 330/473 tokens (cache: 330)Encoded 340/473 tokens (cache: 340)Encoded 350/473 tokens (cache: 350)Encoded 360/473 tokens (cache: 360)Encoded 370/473 tokens (cache: 370)Encoded 380/473 tokens (cache: 380)Encoded 390/473 tokens (cache: 390)Encoded 400/473 tokens (cache: 400)Encoded 410/473 tokens (cache: 410)Encoded 420/473 tokens (cache: 420)Encoded 430/473 tokens (cache: 430)Encoded 440/473 tokens (cache: 440)Encoded 450/473 tokens (cache: 450)Encoded 460/473 tokens (cache: 460)Encoded 470/473 tokens (cache: 470)
Encoding complete.
Original size: 1789 chars
Compressed size: 62 bytes
Bits per character: 0.2772
Compression finished in 191.91 seconds.
Results saved to benchmarks.db
Results saved to model_results.json
  BPC: 0.2772 | Ratio: 28.8548

[18/35] Gemma 3-4B | generated_gemma3_4b_3.py.txt
Loading model from /home/alexn/.cache/huggingface/hub/models--google--gemma-3-4b-pt/snapshots/cc012e0a6d0787b4adcc0fa2c4da74402494554d...
`torch_dtype` is deprecated! Use `dtype` instead!
2026-02-16 23:38:10.828446: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
/home/alexn/Documents/code/research/LLMzip_from_scratch/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:6105: UserWarning: expandable_segments not supported on this platform (Triggered internally at /pytorch/c10/hip/HIPAllocatorConfig.h:29.)
  _ = torch.empty(byte_count // factor, dtype=torch.float16, device=device, requires_grad=False)
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.50s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.71s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.83s/it]
Compressing /home/alexn/Documents/code/research/LLMzip_from_scratch/source_text/generated_gemma3_4b_3.py.txt to /home/alexn/Documents/code/research/LLMzip_from_scratch/encoded_documents/selfcomp_Gemma_34B_generated_gemma3_4b_3.py_w50.llmzip...
Total tokens to encode: 408
/home/alexn/Documents/code/research/LLMzip_from_scratch/.venv/lib/python3.12/site-packages/transformers/models/gemma3/modeling_gemma3.py:174: UserWarning: Attempting to use hipBLASLt on an unsupported architecture! Overriding blas backend to hipblas (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:296.)
  freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
Encoded 10/408 tokens (cache: 10)Encoded 20/408 tokens (cache: 20)Encoded 30/408 tokens (cache: 30)Encoded 40/408 tokens (cache: 40)Encoded 50/408 tokens (cache: 50)Encoded 60/408 tokens (cache: 60)Encoded 70/408 tokens (cache: 70)Encoded 80/408 tokens (cache: 80)Encoded 90/408 tokens (cache: 90)Encoded 100/408 tokens (cache: 100)Encoded 110/408 tokens (cache: 110)Encoded 120/408 tokens (cache: 120)Encoded 130/408 tokens (cache: 130)Encoded 140/408 tokens (cache: 140)Encoded 150/408 tokens (cache: 150)Encoded 160/408 tokens (cache: 160)Encoded 170/408 tokens (cache: 170)Encoded 180/408 tokens (cache: 180)Encoded 190/408 tokens (cache: 190)Encoded 200/408 tokens (cache: 200)Encoded 210/408 tokens (cache: 210)Encoded 220/408 tokens (cache: 220)Encoded 230/408 tokens (cache: 230)Encoded 240/408 tokens (cache: 240)Encoded 250/408 tokens (cache: 250)Encoded 260/408 tokens (cache: 260)Encoded 270/408 tokens (cache: 270)Encoded 280/408 tokens (cache: 280)Encoded 290/408 tokens (cache: 290)Encoded 300/408 tokens (cache: 300)Encoded 310/408 tokens (cache: 310)Encoded 320/408 tokens (cache: 320)Encoded 330/408 tokens (cache: 330)Encoded 340/408 tokens (cache: 340)Encoded 350/408 tokens (cache: 350)Encoded 360/408 tokens (cache: 360)Encoded 370/408 tokens (cache: 370)Encoded 380/408 tokens (cache: 380)Encoded 390/408 tokens (cache: 390)Encoded 400/408 tokens (cache: 400)
Encoding complete.
Original size: 1093 chars
Compressed size: 36 bytes
Bits per character: 0.2635
Compression finished in 165.47 seconds.
Results saved to benchmarks.db
Results saved to model_results.json
  BPC: 0.2635 | Ratio: 30.3611

[19/35] Gemma 3-4B | generated_gemma3_4b_4.py.txt
Loading model from /home/alexn/.cache/huggingface/hub/models--google--gemma-3-4b-pt/snapshots/cc012e0a6d0787b4adcc0fa2c4da74402494554d...
`torch_dtype` is deprecated! Use `dtype` instead!
2026-02-16 23:41:13.958742: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
/home/alexn/Documents/code/research/LLMzip_from_scratch/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:6105: UserWarning: expandable_segments not supported on this platform (Triggered internally at /pytorch/c10/hip/HIPAllocatorConfig.h:29.)
  _ = torch.empty(byte_count // factor, dtype=torch.float16, device=device, requires_grad=False)
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.60s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.88s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.99s/it]
Compressing /home/alexn/Documents/code/research/LLMzip_from_scratch/source_text/generated_gemma3_4b_4.py.txt to /home/alexn/Documents/code/research/LLMzip_from_scratch/encoded_documents/selfcomp_Gemma_34B_generated_gemma3_4b_4.py_w50.llmzip...
Total tokens to encode: 377
/home/alexn/Documents/code/research/LLMzip_from_scratch/.venv/lib/python3.12/site-packages/transformers/models/gemma3/modeling_gemma3.py:174: UserWarning: Attempting to use hipBLASLt on an unsupported architecture! Overriding blas backend to hipblas (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:296.)
  freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
Encoded 10/377 tokens (cache: 10)Encoded 20/377 tokens (cache: 20)Encoded 30/377 tokens (cache: 30)Encoded 40/377 tokens (cache: 40)Encoded 50/377 tokens (cache: 50)Encoded 60/377 tokens (cache: 60)Encoded 70/377 tokens (cache: 70)Encoded 80/377 tokens (cache: 80)Encoded 90/377 tokens (cache: 90)Encoded 100/377 tokens (cache: 100)Encoded 110/377 tokens (cache: 110)Encoded 120/377 tokens (cache: 120)Encoded 130/377 tokens (cache: 130)Encoded 140/377 tokens (cache: 140)Encoded 150/377 tokens (cache: 150)Encoded 160/377 tokens (cache: 160)Encoded 170/377 tokens (cache: 170)Encoded 180/377 tokens (cache: 180)Encoded 190/377 tokens (cache: 190)Encoded 200/377 tokens (cache: 200)Encoded 210/377 tokens (cache: 210)Encoded 220/377 tokens (cache: 220)Encoded 230/377 tokens (cache: 230)Encoded 240/377 tokens (cache: 240)Encoded 250/377 tokens (cache: 250)Encoded 260/377 tokens (cache: 260)Encoded 270/377 tokens (cache: 270)Encoded 280/377 tokens (cache: 280)Encoded 290/377 tokens (cache: 290)Encoded 300/377 tokens (cache: 300)Encoded 310/377 tokens (cache: 310)Encoded 320/377 tokens (cache: 320)Encoded 330/377 tokens (cache: 330)Encoded 340/377 tokens (cache: 340)Encoded 350/377 tokens (cache: 350)Encoded 360/377 tokens (cache: 360)Encoded 370/377 tokens (cache: 370)
Encoding complete.
Original size: 1155 chars
Compressed size: 33 bytes
Bits per character: 0.2286
Compression finished in 152.88 seconds.
Results saved to benchmarks.db
Results saved to model_results.json
  BPC: 0.2286 | Ratio: 35.0000

[20/35] Gemma 3-4B | generated_gemma3_4b_5.py.txt
Loading model from /home/alexn/.cache/huggingface/hub/models--google--gemma-3-4b-pt/snapshots/cc012e0a6d0787b4adcc0fa2c4da74402494554d...
`torch_dtype` is deprecated! Use `dtype` instead!
2026-02-16 23:44:04.678372: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
/home/alexn/Documents/code/research/LLMzip_from_scratch/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:6105: UserWarning: expandable_segments not supported on this platform (Triggered internally at /pytorch/c10/hip/HIPAllocatorConfig.h:29.)
  _ = torch.empty(byte_count // factor, dtype=torch.float16, device=device, requires_grad=False)
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.62s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.85s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.97s/it]
Compressing /home/alexn/Documents/code/research/LLMzip_from_scratch/source_text/generated_gemma3_4b_5.py.txt to /home/alexn/Documents/code/research/LLMzip_from_scratch/encoded_documents/selfcomp_Gemma_34B_generated_gemma3_4b_5.py_w50.llmzip...
Total tokens to encode: 526
/home/alexn/Documents/code/research/LLMzip_from_scratch/.venv/lib/python3.12/site-packages/transformers/models/gemma3/modeling_gemma3.py:174: UserWarning: Attempting to use hipBLASLt on an unsupported architecture! Overriding blas backend to hipblas (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:296.)
  freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
Encoded 10/526 tokens (cache: 10)Encoded 20/526 tokens (cache: 20)Encoded 30/526 tokens (cache: 30)Encoded 40/526 tokens (cache: 40)Encoded 50/526 tokens (cache: 50)Encoded 60/526 tokens (cache: 60)Encoded 70/526 tokens (cache: 70)Encoded 80/526 tokens (cache: 80)Encoded 90/526 tokens (cache: 90)Encoded 100/526 tokens (cache: 100)Encoded 110/526 tokens (cache: 110)Encoded 120/526 tokens (cache: 120)Encoded 130/526 tokens (cache: 130)Encoded 140/526 tokens (cache: 140)Encoded 150/526 tokens (cache: 150)Encoded 160/526 tokens (cache: 160)Encoded 170/526 tokens (cache: 170)Encoded 180/526 tokens (cache: 180)Encoded 190/526 tokens (cache: 190)Encoded 200/526 tokens (cache: 200)Encoded 210/526 tokens (cache: 210)Encoded 220/526 tokens (cache: 220)Encoded 230/526 tokens (cache: 230)Encoded 240/526 tokens (cache: 240)Encoded 250/526 tokens (cache: 250)Encoded 260/526 tokens (cache: 260)Encoded 270/526 tokens (cache: 270)Encoded 280/526 tokens (cache: 280)Encoded 290/526 tokens (cache: 290)Encoded 300/526 tokens (cache: 300)Encoded 310/526 tokens (cache: 310)Encoded 320/526 tokens (cache: 320)Encoded 330/526 tokens (cache: 330)Encoded 340/526 tokens (cache: 340)Encoded 350/526 tokens (cache: 350)Encoded 360/526 tokens (cache: 360)Encoded 370/526 tokens (cache: 370)Encoded 380/526 tokens (cache: 380)Encoded 390/526 tokens (cache: 390)Encoded 400/526 tokens (cache: 400)Encoded 410/526 tokens (cache: 410)Encoded 420/526 tokens (cache: 420)Encoded 430/526 tokens (cache: 430)Encoded 440/526 tokens (cache: 440)Encoded 450/526 tokens (cache: 450)Encoded 460/526 tokens (cache: 460)Encoded 470/526 tokens (cache: 470)Encoded 480/526 tokens (cache: 480)Encoded 490/526 tokens (cache: 490)Encoded 500/526 tokens (cache: 500)Encoded 510/526 tokens (cache: 510)Encoded 520/526 tokens (cache: 520)
Encoding complete.
Original size: 1779 chars
Compressed size: 50 bytes
Bits per character: 0.2248
Compression finished in 213.62 seconds.
Results saved to benchmarks.db
Results saved to model_results.json
  BPC: 0.2248 | Ratio: 35.5800

[21/35] Qwen 3-1.7B | generated_gemma3_4b_1.py.txt
Loading model from /home/alexn/.cache/huggingface/hub/models--Qwen--Qwen3-1.7B/snapshots/70d244cc86ccca08cf5af4e1e306ecf908b1ad5e...
`torch_dtype` is deprecated! Use `dtype` instead!
2026-02-16 23:47:55.336450: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
/home/alexn/Documents/code/research/LLMzip_from_scratch/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:6105: UserWarning: expandable_segments not supported on this platform (Triggered internally at /pytorch/c10/hip/HIPAllocatorConfig.h:29.)
  _ = torch.empty(byte_count // factor, dtype=torch.float16, device=device, requires_grad=False)
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.61s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.08s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.16s/it]
Compressing /home/alexn/Documents/code/research/LLMzip_from_scratch/source_text/generated_gemma3_4b_1.py.txt to /home/alexn/Documents/code/research/LLMzip_from_scratch/encoded_documents/selfcomp_Qwen_317B_generated_gemma3_4b_1.py_w50.llmzip...
Total tokens to encode: 424
/home/alexn/Documents/code/research/LLMzip_from_scratch/.venv/lib/python3.12/site-packages/transformers/models/qwen3/modeling_qwen3.py:327: UserWarning: Attempting to use hipBLASLt on an unsupported architecture! Overriding blas backend to hipblas (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:296.)
  freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
Encoded 10/424 tokens (cache: 10)Encoded 20/424 tokens (cache: 20)Encoded 30/424 tokens (cache: 30)Encoded 40/424 tokens (cache: 40)Encoded 50/424 tokens (cache: 50)Encoded 60/424 tokens (cache: 60)Encoded 70/424 tokens (cache: 70)Encoded 80/424 tokens (cache: 80)Encoded 90/424 tokens (cache: 90)Encoded 100/424 tokens (cache: 100)Encoded 110/424 tokens (cache: 110)Encoded 120/424 tokens (cache: 120)Encoded 130/424 tokens (cache: 130)Encoded 140/424 tokens (cache: 140)Encoded 150/424 tokens (cache: 150)Encoded 160/424 tokens (cache: 160)Encoded 170/424 tokens (cache: 170)Encoded 180/424 tokens (cache: 180)Encoded 190/424 tokens (cache: 190)Encoded 200/424 tokens (cache: 200)Encoded 210/424 tokens (cache: 210)Encoded 220/424 tokens (cache: 220)Encoded 230/424 tokens (cache: 230)Encoded 240/424 tokens (cache: 240)Encoded 250/424 tokens (cache: 250)Encoded 260/424 tokens (cache: 260)Encoded 270/424 tokens (cache: 270)Encoded 280/424 tokens (cache: 280)Encoded 290/424 tokens (cache: 290)Encoded 300/424 tokens (cache: 300)Encoded 310/424 tokens (cache: 310)Encoded 320/424 tokens (cache: 320)Encoded 330/424 tokens (cache: 330)Encoded 340/424 tokens (cache: 340)Encoded 350/424 tokens (cache: 350)Encoded 360/424 tokens (cache: 360)Encoded 370/424 tokens (cache: 370)Encoded 380/424 tokens (cache: 380)Encoded 390/424 tokens (cache: 390)Encoded 400/424 tokens (cache: 400)Encoded 410/424 tokens (cache: 410)Encoded 420/424 tokens (cache: 420)
Encoding complete.
Original size: 1907 chars
Compressed size: 35 bytes
Bits per character: 0.1468
Compression finished in 21.32 seconds.
Results saved to benchmarks.db
Results saved to model_results.json
  BPC: 0.1468 | Ratio: 54.4857

[22/35] Qwen 3-1.7B | generated_gemma3_4b_2.py.txt
Loading model from /home/alexn/.cache/huggingface/hub/models--Qwen--Qwen3-1.7B/snapshots/70d244cc86ccca08cf5af4e1e306ecf908b1ad5e...
`torch_dtype` is deprecated! Use `dtype` instead!
2026-02-16 23:48:25.930347: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
/home/alexn/Documents/code/research/LLMzip_from_scratch/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:6105: UserWarning: expandable_segments not supported on this platform (Triggered internally at /pytorch/c10/hip/HIPAllocatorConfig.h:29.)
  _ = torch.empty(byte_count // factor, dtype=torch.float16, device=device, requires_grad=False)
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.50s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.08it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.01s/it]
Compressing /home/alexn/Documents/code/research/LLMzip_from_scratch/source_text/generated_gemma3_4b_2.py.txt to /home/alexn/Documents/code/research/LLMzip_from_scratch/encoded_documents/selfcomp_Qwen_317B_generated_gemma3_4b_2.py_w50.llmzip...
Total tokens to encode: 409
/home/alexn/Documents/code/research/LLMzip_from_scratch/.venv/lib/python3.12/site-packages/transformers/models/qwen3/modeling_qwen3.py:327: UserWarning: Attempting to use hipBLASLt on an unsupported architecture! Overriding blas backend to hipblas (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:296.)
  freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
Encoded 10/409 tokens (cache: 10)Encoded 20/409 tokens (cache: 20)Encoded 30/409 tokens (cache: 30)Encoded 40/409 tokens (cache: 40)Encoded 50/409 tokens (cache: 50)Encoded 60/409 tokens (cache: 60)Encoded 70/409 tokens (cache: 70)Encoded 80/409 tokens (cache: 80)Encoded 90/409 tokens (cache: 90)Encoded 100/409 tokens (cache: 100)Encoded 110/409 tokens (cache: 110)Encoded 120/409 tokens (cache: 120)Encoded 130/409 tokens (cache: 130)Encoded 140/409 tokens (cache: 140)Encoded 150/409 tokens (cache: 150)Encoded 160/409 tokens (cache: 160)Encoded 170/409 tokens (cache: 170)Encoded 180/409 tokens (cache: 180)Encoded 190/409 tokens (cache: 190)Encoded 200/409 tokens (cache: 200)Encoded 210/409 tokens (cache: 210)Encoded 220/409 tokens (cache: 220)Encoded 230/409 tokens (cache: 230)Encoded 240/409 tokens (cache: 240)Encoded 250/409 tokens (cache: 250)Encoded 260/409 tokens (cache: 260)Encoded 270/409 tokens (cache: 270)Encoded 280/409 tokens (cache: 280)Encoded 290/409 tokens (cache: 290)Encoded 300/409 tokens (cache: 300)Encoded 310/409 tokens (cache: 310)Encoded 320/409 tokens (cache: 320)Encoded 330/409 tokens (cache: 330)Encoded 340/409 tokens (cache: 340)Encoded 350/409 tokens (cache: 350)Encoded 360/409 tokens (cache: 360)Encoded 370/409 tokens (cache: 370)Encoded 380/409 tokens (cache: 380)Encoded 390/409 tokens (cache: 390)Encoded 400/409 tokens (cache: 400)
Encoding complete.
Original size: 1789 chars
Compressed size: 58 bytes
Bits per character: 0.2594
Compression finished in 20.64 seconds.
Results saved to benchmarks.db
Results saved to model_results.json
  BPC: 0.2594 | Ratio: 30.8448

[23/35] Qwen 3-1.7B | generated_gemma3_4b_3.py.txt
Loading model from /home/alexn/.cache/huggingface/hub/models--Qwen--Qwen3-1.7B/snapshots/70d244cc86ccca08cf5af4e1e306ecf908b1ad5e...
`torch_dtype` is deprecated! Use `dtype` instead!
2026-02-16 23:48:55.557669: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
/home/alexn/Documents/code/research/LLMzip_from_scratch/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:6105: UserWarning: expandable_segments not supported on this platform (Triggered internally at /pytorch/c10/hip/HIPAllocatorConfig.h:29.)
  _ = torch.empty(byte_count // factor, dtype=torch.float16, device=device, requires_grad=False)
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.48s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.03it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.05s/it]
Compressing /home/alexn/Documents/code/research/LLMzip_from_scratch/source_text/generated_gemma3_4b_3.py.txt to /home/alexn/Documents/code/research/LLMzip_from_scratch/encoded_documents/selfcomp_Qwen_317B_generated_gemma3_4b_3.py_w50.llmzip...
Total tokens to encode: 333
/home/alexn/Documents/code/research/LLMzip_from_scratch/.venv/lib/python3.12/site-packages/transformers/models/qwen3/modeling_qwen3.py:327: UserWarning: Attempting to use hipBLASLt on an unsupported architecture! Overriding blas backend to hipblas (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:296.)
  freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
Encoded 10/333 tokens (cache: 10)Encoded 20/333 tokens (cache: 20)Encoded 30/333 tokens (cache: 30)Encoded 40/333 tokens (cache: 40)Encoded 50/333 tokens (cache: 50)Encoded 60/333 tokens (cache: 60)Encoded 70/333 tokens (cache: 70)Encoded 80/333 tokens (cache: 80)Encoded 90/333 tokens (cache: 90)Encoded 100/333 tokens (cache: 100)Encoded 110/333 tokens (cache: 110)Encoded 120/333 tokens (cache: 120)Encoded 130/333 tokens (cache: 130)Encoded 140/333 tokens (cache: 140)Encoded 150/333 tokens (cache: 150)Encoded 160/333 tokens (cache: 160)Encoded 170/333 tokens (cache: 170)Encoded 180/333 tokens (cache: 180)Encoded 190/333 tokens (cache: 190)Encoded 200/333 tokens (cache: 200)Encoded 210/333 tokens (cache: 210)Encoded 220/333 tokens (cache: 220)Encoded 230/333 tokens (cache: 230)Encoded 240/333 tokens (cache: 240)Encoded 250/333 tokens (cache: 250)Encoded 260/333 tokens (cache: 260)Encoded 270/333 tokens (cache: 270)Encoded 280/333 tokens (cache: 280)Encoded 290/333 tokens (cache: 290)Encoded 300/333 tokens (cache: 300)Encoded 310/333 tokens (cache: 310)Encoded 320/333 tokens (cache: 320)Encoded 330/333 tokens (cache: 330)
Encoding complete.
Original size: 1093 chars
Compressed size: 33 bytes
Bits per character: 0.2415
Compression finished in 16.85 seconds.
Results saved to benchmarks.db
Results saved to model_results.json
  BPC: 0.2415 | Ratio: 33.1212

[24/35] Qwen 3-1.7B | generated_gemma3_4b_4.py.txt
Loading model from /home/alexn/.cache/huggingface/hub/models--Qwen--Qwen3-1.7B/snapshots/70d244cc86ccca08cf5af4e1e306ecf908b1ad5e...
`torch_dtype` is deprecated! Use `dtype` instead!
2026-02-16 23:49:21.459795: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
/home/alexn/Documents/code/research/LLMzip_from_scratch/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:6105: UserWarning: expandable_segments not supported on this platform (Triggered internally at /pytorch/c10/hip/HIPAllocatorConfig.h:29.)
  _ = torch.empty(byte_count // factor, dtype=torch.float16, device=device, requires_grad=False)
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.45s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.07it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.01s/it]
Compressing /home/alexn/Documents/code/research/LLMzip_from_scratch/source_text/generated_gemma3_4b_4.py.txt to /home/alexn/Documents/code/research/LLMzip_from_scratch/encoded_documents/selfcomp_Qwen_317B_generated_gemma3_4b_4.py_w50.llmzip...
Total tokens to encode: 318
/home/alexn/Documents/code/research/LLMzip_from_scratch/.venv/lib/python3.12/site-packages/transformers/models/qwen3/modeling_qwen3.py:327: UserWarning: Attempting to use hipBLASLt on an unsupported architecture! Overriding blas backend to hipblas (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:296.)
  freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
Encoded 10/318 tokens (cache: 10)Encoded 20/318 tokens (cache: 20)Encoded 30/318 tokens (cache: 30)Encoded 40/318 tokens (cache: 40)Encoded 50/318 tokens (cache: 50)Encoded 60/318 tokens (cache: 60)Encoded 70/318 tokens (cache: 70)Encoded 80/318 tokens (cache: 80)Encoded 90/318 tokens (cache: 90)Encoded 100/318 tokens (cache: 100)Encoded 110/318 tokens (cache: 110)Encoded 120/318 tokens (cache: 120)Encoded 130/318 tokens (cache: 130)Encoded 140/318 tokens (cache: 140)Encoded 150/318 tokens (cache: 150)Encoded 160/318 tokens (cache: 160)Encoded 170/318 tokens (cache: 170)Encoded 180/318 tokens (cache: 180)Encoded 190/318 tokens (cache: 190)Encoded 200/318 tokens (cache: 200)Encoded 210/318 tokens (cache: 210)Encoded 220/318 tokens (cache: 220)Encoded 230/318 tokens (cache: 230)Encoded 240/318 tokens (cache: 240)Encoded 250/318 tokens (cache: 250)Encoded 260/318 tokens (cache: 260)Encoded 270/318 tokens (cache: 270)Encoded 280/318 tokens (cache: 280)Encoded 290/318 tokens (cache: 290)Encoded 300/318 tokens (cache: 300)Encoded 310/318 tokens (cache: 310)
Encoding complete.
Original size: 1155 chars
Compressed size: 30 bytes
Bits per character: 0.2078
Compression finished in 16.04 seconds.
Results saved to benchmarks.db
Results saved to model_results.json
  BPC: 0.2078 | Ratio: 38.5000

[25/35] Qwen 3-1.7B | generated_gemma3_4b_5.py.txt
Loading model from /home/alexn/.cache/huggingface/hub/models--Qwen--Qwen3-1.7B/snapshots/70d244cc86ccca08cf5af4e1e306ecf908b1ad5e...
`torch_dtype` is deprecated! Use `dtype` instead!
2026-02-16 23:49:46.443369: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
/home/alexn/Documents/code/research/LLMzip_from_scratch/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:6105: UserWarning: expandable_segments not supported on this platform (Triggered internally at /pytorch/c10/hip/HIPAllocatorConfig.h:29.)
  _ = torch.empty(byte_count // factor, dtype=torch.float16, device=device, requires_grad=False)
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.46s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.05it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.03s/it]
Compressing /home/alexn/Documents/code/research/LLMzip_from_scratch/source_text/generated_gemma3_4b_5.py.txt to /home/alexn/Documents/code/research/LLMzip_from_scratch/encoded_documents/selfcomp_Qwen_317B_generated_gemma3_4b_5.py_w50.llmzip...
Total tokens to encode: 401
/home/alexn/Documents/code/research/LLMzip_from_scratch/.venv/lib/python3.12/site-packages/transformers/models/qwen3/modeling_qwen3.py:327: UserWarning: Attempting to use hipBLASLt on an unsupported architecture! Overriding blas backend to hipblas (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:296.)
  freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
Encoded 10/401 tokens (cache: 10)Encoded 20/401 tokens (cache: 20)Encoded 30/401 tokens (cache: 30)Encoded 40/401 tokens (cache: 40)Encoded 50/401 tokens (cache: 50)Encoded 60/401 tokens (cache: 60)Encoded 70/401 tokens (cache: 70)Encoded 80/401 tokens (cache: 80)Encoded 90/401 tokens (cache: 90)Encoded 100/401 tokens (cache: 100)Encoded 110/401 tokens (cache: 110)Encoded 120/401 tokens (cache: 120)Encoded 130/401 tokens (cache: 130)Encoded 140/401 tokens (cache: 140)Encoded 150/401 tokens (cache: 150)Encoded 160/401 tokens (cache: 160)Encoded 170/401 tokens (cache: 170)Encoded 180/401 tokens (cache: 180)Encoded 190/401 tokens (cache: 190)Encoded 200/401 tokens (cache: 200)Encoded 210/401 tokens (cache: 210)Encoded 220/401 tokens (cache: 220)Encoded 230/401 tokens (cache: 230)Encoded 240/401 tokens (cache: 240)Encoded 250/401 tokens (cache: 250)Encoded 260/401 tokens (cache: 260)Encoded 270/401 tokens (cache: 270)Encoded 280/401 tokens (cache: 280)Encoded 290/401 tokens (cache: 290)Encoded 300/401 tokens (cache: 300)Encoded 310/401 tokens (cache: 310)Encoded 320/401 tokens (cache: 320)Encoded 330/401 tokens (cache: 330)Encoded 340/401 tokens (cache: 340)Encoded 350/401 tokens (cache: 350)Encoded 360/401 tokens (cache: 360)Encoded 370/401 tokens (cache: 370)Encoded 380/401 tokens (cache: 380)Encoded 390/401 tokens (cache: 390)Encoded 400/401 tokens (cache: 400)
Encoding complete.
Original size: 1779 chars
Compressed size: 48 bytes
Bits per character: 0.2159
Compression finished in 20.18 seconds.
Results saved to benchmarks.db
Results saved to model_results.json
  BPC: 0.2159 | Ratio: 37.0625

[26/35] Llama 3.2-1B | generated_gemma3_4b_1.py.txt
Loading model from /home/alexn/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08...
`torch_dtype` is deprecated! Use `dtype` instead!
2026-02-16 23:50:15.569764: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
/home/alexn/Documents/code/research/LLMzip_from_scratch/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:6105: UserWarning: expandable_segments not supported on this platform (Triggered internally at /pytorch/c10/hip/HIPAllocatorConfig.h:29.)
  _ = torch.empty(byte_count // factor, dtype=torch.float16, device=device, requires_grad=False)
Compressing /home/alexn/Documents/code/research/LLMzip_from_scratch/source_text/generated_gemma3_4b_1.py.txt to /home/alexn/Documents/code/research/LLMzip_from_scratch/encoded_documents/selfcomp_Llama_321B_generated_gemma3_4b_1.py_w50.llmzip...
Total tokens to encode: 407
/home/alexn/Documents/code/research/LLMzip_from_scratch/.venv/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:101: UserWarning: Attempting to use hipBLASLt on an unsupported architecture! Overriding blas backend to hipblas (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:296.)
  freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
Encoded 10/407 tokens (cache: 10)Encoded 20/407 tokens (cache: 20)Encoded 30/407 tokens (cache: 30)Encoded 40/407 tokens (cache: 40)Encoded 50/407 tokens (cache: 50)Encoded 60/407 tokens (cache: 60)Encoded 70/407 tokens (cache: 70)Encoded 80/407 tokens (cache: 80)Encoded 90/407 tokens (cache: 90)Encoded 100/407 tokens (cache: 100)Encoded 110/407 tokens (cache: 110)Encoded 120/407 tokens (cache: 120)Encoded 130/407 tokens (cache: 130)Encoded 140/407 tokens (cache: 140)Encoded 150/407 tokens (cache: 150)Encoded 160/407 tokens (cache: 160)Encoded 170/407 tokens (cache: 170)Encoded 180/407 tokens (cache: 180)Encoded 190/407 tokens (cache: 190)Encoded 200/407 tokens (cache: 200)Encoded 210/407 tokens (cache: 210)Encoded 220/407 tokens (cache: 220)Encoded 230/407 tokens (cache: 230)Encoded 240/407 tokens (cache: 240)Encoded 250/407 tokens (cache: 250)Encoded 260/407 tokens (cache: 260)Encoded 270/407 tokens (cache: 270)Encoded 280/407 tokens (cache: 280)Encoded 290/407 tokens (cache: 290)Encoded 300/407 tokens (cache: 300)Encoded 310/407 tokens (cache: 310)Encoded 320/407 tokens (cache: 320)Encoded 330/407 tokens (cache: 330)Encoded 340/407 tokens (cache: 340)Encoded 350/407 tokens (cache: 350)Encoded 360/407 tokens (cache: 360)Encoded 370/407 tokens (cache: 370)Encoded 380/407 tokens (cache: 380)Encoded 390/407 tokens (cache: 390)Encoded 400/407 tokens (cache: 400)
Encoding complete.
Original size: 1907 chars
Compressed size: 34 bytes
Bits per character: 0.1426
Compression finished in 17.87 seconds.
Results saved to benchmarks.db
Results saved to model_results.json
  BPC: 0.1426 | Ratio: 56.0882

[27/35] Llama 3.2-1B | generated_gemma3_4b_2.py.txt
Loading model from /home/alexn/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08...
`torch_dtype` is deprecated! Use `dtype` instead!
2026-02-16 23:50:42.949398: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
/home/alexn/Documents/code/research/LLMzip_from_scratch/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:6105: UserWarning: expandable_segments not supported on this platform (Triggered internally at /pytorch/c10/hip/HIPAllocatorConfig.h:29.)
  _ = torch.empty(byte_count // factor, dtype=torch.float16, device=device, requires_grad=False)
Compressing /home/alexn/Documents/code/research/LLMzip_from_scratch/source_text/generated_gemma3_4b_2.py.txt to /home/alexn/Documents/code/research/LLMzip_from_scratch/encoded_documents/selfcomp_Llama_321B_generated_gemma3_4b_2.py_w50.llmzip...
Total tokens to encode: 394
/home/alexn/Documents/code/research/LLMzip_from_scratch/.venv/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:101: UserWarning: Attempting to use hipBLASLt on an unsupported architecture! Overriding blas backend to hipblas (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:296.)
  freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
Encoded 10/394 tokens (cache: 10)Encoded 20/394 tokens (cache: 20)Encoded 30/394 tokens (cache: 30)Encoded 40/394 tokens (cache: 40)Encoded 50/394 tokens (cache: 50)Encoded 60/394 tokens (cache: 60)Encoded 70/394 tokens (cache: 70)Encoded 80/394 tokens (cache: 80)Encoded 90/394 tokens (cache: 90)Encoded 100/394 tokens (cache: 100)Encoded 110/394 tokens (cache: 110)Encoded 120/394 tokens (cache: 120)Encoded 130/394 tokens (cache: 130)Encoded 140/394 tokens (cache: 140)Encoded 150/394 tokens (cache: 150)Encoded 160/394 tokens (cache: 160)Encoded 170/394 tokens (cache: 170)Encoded 180/394 tokens (cache: 180)Encoded 190/394 tokens (cache: 190)Encoded 200/394 tokens (cache: 200)Encoded 210/394 tokens (cache: 210)Encoded 220/394 tokens (cache: 220)Encoded 230/394 tokens (cache: 230)Encoded 240/394 tokens (cache: 240)Encoded 250/394 tokens (cache: 250)Encoded 260/394 tokens (cache: 260)Encoded 270/394 tokens (cache: 270)Encoded 280/394 tokens (cache: 280)Encoded 290/394 tokens (cache: 290)Encoded 300/394 tokens (cache: 300)Encoded 310/394 tokens (cache: 310)Encoded 320/394 tokens (cache: 320)Encoded 330/394 tokens (cache: 330)Encoded 340/394 tokens (cache: 340)Encoded 350/394 tokens (cache: 350)Encoded 360/394 tokens (cache: 360)Encoded 370/394 tokens (cache: 370)Encoded 380/394 tokens (cache: 380)Encoded 390/394 tokens (cache: 390)
Encoding complete.
Original size: 1789 chars
Compressed size: 61 bytes
Bits per character: 0.2728
Compression finished in 17.27 seconds.
Results saved to benchmarks.db
Results saved to model_results.json
  BPC: 0.2728 | Ratio: 29.3279

[28/35] Llama 3.2-1B | generated_gemma3_4b_3.py.txt
Loading model from /home/alexn/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08...
`torch_dtype` is deprecated! Use `dtype` instead!
2026-02-16 23:51:08.950434: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
/home/alexn/Documents/code/research/LLMzip_from_scratch/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:6105: UserWarning: expandable_segments not supported on this platform (Triggered internally at /pytorch/c10/hip/HIPAllocatorConfig.h:29.)
  _ = torch.empty(byte_count // factor, dtype=torch.float16, device=device, requires_grad=False)
Compressing /home/alexn/Documents/code/research/LLMzip_from_scratch/source_text/generated_gemma3_4b_3.py.txt to /home/alexn/Documents/code/research/LLMzip_from_scratch/encoded_documents/selfcomp_Llama_321B_generated_gemma3_4b_3.py_w50.llmzip...
Total tokens to encode: 334
/home/alexn/Documents/code/research/LLMzip_from_scratch/.venv/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:101: UserWarning: Attempting to use hipBLASLt on an unsupported architecture! Overriding blas backend to hipblas (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:296.)
  freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
Encoded 10/334 tokens (cache: 10)Encoded 20/334 tokens (cache: 20)Encoded 30/334 tokens (cache: 30)Encoded 40/334 tokens (cache: 40)Encoded 50/334 tokens (cache: 50)Encoded 60/334 tokens (cache: 60)Encoded 70/334 tokens (cache: 70)Encoded 80/334 tokens (cache: 80)Encoded 90/334 tokens (cache: 90)Encoded 100/334 tokens (cache: 100)Encoded 110/334 tokens (cache: 110)Encoded 120/334 tokens (cache: 120)Encoded 130/334 tokens (cache: 130)Encoded 140/334 tokens (cache: 140)Encoded 150/334 tokens (cache: 150)Encoded 160/334 tokens (cache: 160)Encoded 170/334 tokens (cache: 170)Encoded 180/334 tokens (cache: 180)Encoded 190/334 tokens (cache: 190)Encoded 200/334 tokens (cache: 200)Encoded 210/334 tokens (cache: 210)Encoded 220/334 tokens (cache: 220)Encoded 230/334 tokens (cache: 230)Encoded 240/334 tokens (cache: 240)Encoded 250/334 tokens (cache: 250)Encoded 260/334 tokens (cache: 260)Encoded 270/334 tokens (cache: 270)Encoded 280/334 tokens (cache: 280)Encoded 290/334 tokens (cache: 290)Encoded 300/334 tokens (cache: 300)Encoded 310/334 tokens (cache: 310)Encoded 320/334 tokens (cache: 320)Encoded 330/334 tokens (cache: 330)
Encoding complete.
Original size: 1093 chars
Compressed size: 33 bytes
Bits per character: 0.2415
Compression finished in 14.72 seconds.
Results saved to benchmarks.db
Results saved to model_results.json
  BPC: 0.2415 | Ratio: 33.1212

[29/35] Llama 3.2-1B | generated_gemma3_4b_4.py.txt
Loading model from /home/alexn/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08...
`torch_dtype` is deprecated! Use `dtype` instead!
2026-02-16 23:51:32.222017: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
/home/alexn/Documents/code/research/LLMzip_from_scratch/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:6105: UserWarning: expandable_segments not supported on this platform (Triggered internally at /pytorch/c10/hip/HIPAllocatorConfig.h:29.)
  _ = torch.empty(byte_count // factor, dtype=torch.float16, device=device, requires_grad=False)
Compressing /home/alexn/Documents/code/research/LLMzip_from_scratch/source_text/generated_gemma3_4b_4.py.txt to /home/alexn/Documents/code/research/LLMzip_from_scratch/encoded_documents/selfcomp_Llama_321B_generated_gemma3_4b_4.py_w50.llmzip...
Total tokens to encode: 319
/home/alexn/Documents/code/research/LLMzip_from_scratch/.venv/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:101: UserWarning: Attempting to use hipBLASLt on an unsupported architecture! Overriding blas backend to hipblas (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:296.)
  freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
Encoded 10/319 tokens (cache: 10)Encoded 20/319 tokens (cache: 20)Encoded 30/319 tokens (cache: 30)Encoded 40/319 tokens (cache: 40)Encoded 50/319 tokens (cache: 50)Encoded 60/319 tokens (cache: 60)Encoded 70/319 tokens (cache: 70)Encoded 80/319 tokens (cache: 80)Encoded 90/319 tokens (cache: 90)Encoded 100/319 tokens (cache: 100)Encoded 110/319 tokens (cache: 110)Encoded 120/319 tokens (cache: 120)Encoded 130/319 tokens (cache: 130)Encoded 140/319 tokens (cache: 140)Encoded 150/319 tokens (cache: 150)Encoded 160/319 tokens (cache: 160)Encoded 170/319 tokens (cache: 170)Encoded 180/319 tokens (cache: 180)Encoded 190/319 tokens (cache: 190)Encoded 200/319 tokens (cache: 200)Encoded 210/319 tokens (cache: 210)Encoded 220/319 tokens (cache: 220)Encoded 230/319 tokens (cache: 230)Encoded 240/319 tokens (cache: 240)Encoded 250/319 tokens (cache: 250)Encoded 260/319 tokens (cache: 260)Encoded 270/319 tokens (cache: 270)Encoded 280/319 tokens (cache: 280)Encoded 290/319 tokens (cache: 290)Encoded 300/319 tokens (cache: 300)Encoded 310/319 tokens (cache: 310)
Encoding complete.
Original size: 1155 chars
Compressed size: 28 bytes
Bits per character: 0.1939
Compression finished in 14.02 seconds.
Results saved to benchmarks.db
Results saved to model_results.json
  BPC: 0.1939 | Ratio: 41.2500

[30/35] Llama 3.2-1B | generated_gemma3_4b_5.py.txt
Loading model from /home/alexn/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08...
`torch_dtype` is deprecated! Use `dtype` instead!
2026-02-16 23:51:54.649064: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
/home/alexn/Documents/code/research/LLMzip_from_scratch/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:6105: UserWarning: expandable_segments not supported on this platform (Triggered internally at /pytorch/c10/hip/HIPAllocatorConfig.h:29.)
  _ = torch.empty(byte_count // factor, dtype=torch.float16, device=device, requires_grad=False)
Compressing /home/alexn/Documents/code/research/LLMzip_from_scratch/source_text/generated_gemma3_4b_5.py.txt to /home/alexn/Documents/code/research/LLMzip_from_scratch/encoded_documents/selfcomp_Llama_321B_generated_gemma3_4b_5.py_w50.llmzip...
Total tokens to encode: 392
/home/alexn/Documents/code/research/LLMzip_from_scratch/.venv/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:101: UserWarning: Attempting to use hipBLASLt on an unsupported architecture! Overriding blas backend to hipblas (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:296.)
  freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
Encoded 10/392 tokens (cache: 10)Encoded 20/392 tokens (cache: 20)Encoded 30/392 tokens (cache: 30)Encoded 40/392 tokens (cache: 40)Encoded 50/392 tokens (cache: 50)Encoded 60/392 tokens (cache: 60)Encoded 70/392 tokens (cache: 70)Encoded 80/392 tokens (cache: 80)Encoded 90/392 tokens (cache: 90)Encoded 100/392 tokens (cache: 100)Encoded 110/392 tokens (cache: 110)Encoded 120/392 tokens (cache: 120)Encoded 130/392 tokens (cache: 130)Encoded 140/392 tokens (cache: 140)Encoded 150/392 tokens (cache: 150)Encoded 160/392 tokens (cache: 160)Encoded 170/392 tokens (cache: 170)Encoded 180/392 tokens (cache: 180)Encoded 190/392 tokens (cache: 190)Encoded 200/392 tokens (cache: 200)Encoded 210/392 tokens (cache: 210)Encoded 220/392 tokens (cache: 220)Encoded 230/392 tokens (cache: 230)Encoded 240/392 tokens (cache: 240)Encoded 250/392 tokens (cache: 250)Encoded 260/392 tokens (cache: 260)Encoded 270/392 tokens (cache: 270)Encoded 280/392 tokens (cache: 280)Encoded 290/392 tokens (cache: 290)Encoded 300/392 tokens (cache: 300)Encoded 310/392 tokens (cache: 310)Encoded 320/392 tokens (cache: 320)Encoded 330/392 tokens (cache: 330)Encoded 340/392 tokens (cache: 340)Encoded 350/392 tokens (cache: 350)Encoded 360/392 tokens (cache: 360)Encoded 370/392 tokens (cache: 370)Encoded 380/392 tokens (cache: 380)Encoded 390/392 tokens (cache: 390)
Encoding complete.
Original size: 1779 chars
Compressed size: 47 bytes
Bits per character: 0.2114
Compression finished in 17.21 seconds.
Results saved to benchmarks.db
Results saved to model_results.json
  BPC: 0.2114 | Ratio: 37.8511

[31/35] Llama 3.2-3B | generated_gemma3_4b_1.py.txt
Loading model from /home/alexn/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B/snapshots/13afe5124825b4f3751f836b40dafda64c1ed062...
`torch_dtype` is deprecated! Use `dtype` instead!
2026-02-16 23:52:20.319169: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
/home/alexn/Documents/code/research/LLMzip_from_scratch/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:6105: UserWarning: expandable_segments not supported on this platform (Triggered internally at /pytorch/c10/hip/HIPAllocatorConfig.h:29.)
  _ = torch.empty(byte_count // factor, dtype=torch.float16, device=device, requires_grad=False)
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.43s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.01s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.37s/it]
Compressing /home/alexn/Documents/code/research/LLMzip_from_scratch/source_text/generated_gemma3_4b_1.py.txt to /home/alexn/Documents/code/research/LLMzip_from_scratch/encoded_documents/selfcomp_Llama_323B_generated_gemma3_4b_1.py_w50.llmzip...
Total tokens to encode: 407
/home/alexn/Documents/code/research/LLMzip_from_scratch/.venv/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:101: UserWarning: Attempting to use hipBLASLt on an unsupported architecture! Overriding blas backend to hipblas (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:296.)
  freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
Encoded 10/407 tokens (cache: 10)Encoded 20/407 tokens (cache: 20)Encoded 30/407 tokens (cache: 30)Encoded 40/407 tokens (cache: 40)Encoded 50/407 tokens (cache: 50)Encoded 60/407 tokens (cache: 60)Encoded 70/407 tokens (cache: 70)Encoded 80/407 tokens (cache: 80)Encoded 90/407 tokens (cache: 90)Encoded 100/407 tokens (cache: 100)Encoded 110/407 tokens (cache: 110)Encoded 120/407 tokens (cache: 120)Encoded 130/407 tokens (cache: 130)Encoded 140/407 tokens (cache: 140)Encoded 150/407 tokens (cache: 150)Encoded 160/407 tokens (cache: 160)Encoded 170/407 tokens (cache: 170)Encoded 180/407 tokens (cache: 180)Encoded 190/407 tokens (cache: 190)Encoded 200/407 tokens (cache: 200)Encoded 210/407 tokens (cache: 210)Encoded 220/407 tokens (cache: 220)Encoded 230/407 tokens (cache: 230)Encoded 240/407 tokens (cache: 240)Encoded 250/407 tokens (cache: 250)Encoded 260/407 tokens (cache: 260)Encoded 270/407 tokens (cache: 270)Encoded 280/407 tokens (cache: 280)Encoded 290/407 tokens (cache: 290)Encoded 300/407 tokens (cache: 300)Encoded 310/407 tokens (cache: 310)Encoded 320/407 tokens (cache: 320)Encoded 330/407 tokens (cache: 330)Encoded 340/407 tokens (cache: 340)Encoded 350/407 tokens (cache: 350)Encoded 360/407 tokens (cache: 360)Encoded 370/407 tokens (cache: 370)Encoded 380/407 tokens (cache: 380)Encoded 390/407 tokens (cache: 390)Encoded 400/407 tokens (cache: 400)
Encoding complete.
Original size: 1907 chars
Compressed size: 30 bytes
Bits per character: 0.1259
Compression finished in 34.94 seconds.
Results saved to benchmarks.db
Results saved to model_results.json
  BPC: 0.1259 | Ratio: 63.5667

[32/35] Llama 3.2-3B | generated_gemma3_4b_2.py.txt
Loading model from /home/alexn/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B/snapshots/13afe5124825b4f3751f836b40dafda64c1ed062...
`torch_dtype` is deprecated! Use `dtype` instead!
2026-02-16 23:53:09.000451: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
/home/alexn/Documents/code/research/LLMzip_from_scratch/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:6105: UserWarning: expandable_segments not supported on this platform (Triggered internally at /pytorch/c10/hip/HIPAllocatorConfig.h:29.)
  _ = torch.empty(byte_count // factor, dtype=torch.float16, device=device, requires_grad=False)
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.08s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.29s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.56s/it]
Compressing /home/alexn/Documents/code/research/LLMzip_from_scratch/source_text/generated_gemma3_4b_2.py.txt to /home/alexn/Documents/code/research/LLMzip_from_scratch/encoded_documents/selfcomp_Llama_323B_generated_gemma3_4b_2.py_w50.llmzip...
Total tokens to encode: 394
/home/alexn/Documents/code/research/LLMzip_from_scratch/.venv/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:101: UserWarning: Attempting to use hipBLASLt on an unsupported architecture! Overriding blas backend to hipblas (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:296.)
  freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
Encoded 10/394 tokens (cache: 10)Encoded 20/394 tokens (cache: 20)Encoded 30/394 tokens (cache: 30)Encoded 40/394 tokens (cache: 40)Encoded 50/394 tokens (cache: 50)Encoded 60/394 tokens (cache: 60)Encoded 70/394 tokens (cache: 70)Encoded 80/394 tokens (cache: 80)Encoded 90/394 tokens (cache: 90)Encoded 100/394 tokens (cache: 100)Encoded 110/394 tokens (cache: 110)Encoded 120/394 tokens (cache: 120)Encoded 130/394 tokens (cache: 130)Encoded 140/394 tokens (cache: 140)Encoded 150/394 tokens (cache: 150)Encoded 160/394 tokens (cache: 160)Encoded 170/394 tokens (cache: 170)Encoded 180/394 tokens (cache: 180)Encoded 190/394 tokens (cache: 190)Encoded 200/394 tokens (cache: 200)Encoded 210/394 tokens (cache: 210)Encoded 220/394 tokens (cache: 220)Encoded 230/394 tokens (cache: 230)Encoded 240/394 tokens (cache: 240)Encoded 250/394 tokens (cache: 250)Encoded 260/394 tokens (cache: 260)Encoded 270/394 tokens (cache: 270)Encoded 280/394 tokens (cache: 280)Encoded 290/394 tokens (cache: 290)Encoded 300/394 tokens (cache: 300)Encoded 310/394 tokens (cache: 310)Encoded 320/394 tokens (cache: 320)Encoded 330/394 tokens (cache: 330)Encoded 340/394 tokens (cache: 340)Encoded 350/394 tokens (cache: 350)Encoded 360/394 tokens (cache: 360)Encoded 370/394 tokens (cache: 370)Encoded 380/394 tokens (cache: 380)Encoded 390/394 tokens (cache: 390)
Encoding complete.
Original size: 1789 chars
Compressed size: 58 bytes
Bits per character: 0.2594
Compression finished in 33.79 seconds.
Results saved to benchmarks.db
Results saved to model_results.json
  BPC: 0.2594 | Ratio: 30.8448

[33/35] Llama 3.2-3B | generated_gemma3_4b_3.py.txt
Loading model from /home/alexn/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B/snapshots/13afe5124825b4f3751f836b40dafda64c1ed062...
`torch_dtype` is deprecated! Use `dtype` instead!
2026-02-16 23:53:54.917802: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
/home/alexn/Documents/code/research/LLMzip_from_scratch/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:6105: UserWarning: expandable_segments not supported on this platform (Triggered internally at /pytorch/c10/hip/HIPAllocatorConfig.h:29.)
  _ = torch.empty(byte_count // factor, dtype=torch.float16, device=device, requires_grad=False)
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.16s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.79s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.99s/it]
Compressing /home/alexn/Documents/code/research/LLMzip_from_scratch/source_text/generated_gemma3_4b_3.py.txt to /home/alexn/Documents/code/research/LLMzip_from_scratch/encoded_documents/selfcomp_Llama_323B_generated_gemma3_4b_3.py_w50.llmzip...
Total tokens to encode: 334
/home/alexn/Documents/code/research/LLMzip_from_scratch/.venv/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:101: UserWarning: Attempting to use hipBLASLt on an unsupported architecture! Overriding blas backend to hipblas (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:296.)
  freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
Encoded 10/334 tokens (cache: 10)Encoded 20/334 tokens (cache: 20)Encoded 30/334 tokens (cache: 30)Encoded 40/334 tokens (cache: 40)Encoded 50/334 tokens (cache: 50)Encoded 60/334 tokens (cache: 60)Encoded 70/334 tokens (cache: 70)Encoded 80/334 tokens (cache: 80)Encoded 90/334 tokens (cache: 90)Encoded 100/334 tokens (cache: 100)Encoded 110/334 tokens (cache: 110)Encoded 120/334 tokens (cache: 120)Encoded 130/334 tokens (cache: 130)Encoded 140/334 tokens (cache: 140)Encoded 150/334 tokens (cache: 150)Encoded 160/334 tokens (cache: 160)Encoded 170/334 tokens (cache: 170)Encoded 180/334 tokens (cache: 180)Encoded 190/334 tokens (cache: 190)Encoded 200/334 tokens (cache: 200)Encoded 210/334 tokens (cache: 210)Encoded 220/334 tokens (cache: 220)Encoded 230/334 tokens (cache: 230)Encoded 240/334 tokens (cache: 240)Encoded 250/334 tokens (cache: 250)Encoded 260/334 tokens (cache: 260)Encoded 270/334 tokens (cache: 270)Encoded 280/334 tokens (cache: 280)Encoded 290/334 tokens (cache: 290)Encoded 300/334 tokens (cache: 300)Encoded 310/334 tokens (cache: 310)Encoded 320/334 tokens (cache: 320)Encoded 330/334 tokens (cache: 330)
Encoding complete.
Original size: 1093 chars
Compressed size: 27 bytes
Bits per character: 0.1976
Compression finished in 28.65 seconds.
Results saved to benchmarks.db
Results saved to model_results.json
  BPC: 0.1976 | Ratio: 40.4815

[34/35] Llama 3.2-3B | generated_gemma3_4b_4.py.txt
Loading model from /home/alexn/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B/snapshots/13afe5124825b4f3751f836b40dafda64c1ed062...
`torch_dtype` is deprecated! Use `dtype` instead!
2026-02-16 23:54:34.576201: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
/home/alexn/Documents/code/research/LLMzip_from_scratch/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:6105: UserWarning: expandable_segments not supported on this platform (Triggered internally at /pytorch/c10/hip/HIPAllocatorConfig.h:29.)
  _ = torch.empty(byte_count // factor, dtype=torch.float16, device=device, requires_grad=False)
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.10s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  1.92s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.10s/it]
Compressing /home/alexn/Documents/code/research/LLMzip_from_scratch/source_text/generated_gemma3_4b_4.py.txt to /home/alexn/Documents/code/research/LLMzip_from_scratch/encoded_documents/selfcomp_Llama_323B_generated_gemma3_4b_4.py_w50.llmzip...
Total tokens to encode: 319
/home/alexn/Documents/code/research/LLMzip_from_scratch/.venv/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:101: UserWarning: Attempting to use hipBLASLt on an unsupported architecture! Overriding blas backend to hipblas (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:296.)
  freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
Encoded 10/319 tokens (cache: 10)Encoded 20/319 tokens (cache: 20)Encoded 30/319 tokens (cache: 30)Encoded 40/319 tokens (cache: 40)Encoded 50/319 tokens (cache: 50)Encoded 60/319 tokens (cache: 60)Encoded 70/319 tokens (cache: 70)Encoded 80/319 tokens (cache: 80)Encoded 90/319 tokens (cache: 90)Encoded 100/319 tokens (cache: 100)Encoded 110/319 tokens (cache: 110)Encoded 120/319 tokens (cache: 120)Encoded 130/319 tokens (cache: 130)Encoded 140/319 tokens (cache: 140)Encoded 150/319 tokens (cache: 150)Encoded 160/319 tokens (cache: 160)Encoded 170/319 tokens (cache: 170)Encoded 180/319 tokens (cache: 180)Encoded 190/319 tokens (cache: 190)Encoded 200/319 tokens (cache: 200)Encoded 210/319 tokens (cache: 210)Encoded 220/319 tokens (cache: 220)Encoded 230/319 tokens (cache: 230)Encoded 240/319 tokens (cache: 240)Encoded 250/319 tokens (cache: 250)Encoded 260/319 tokens (cache: 260)Encoded 270/319 tokens (cache: 270)Encoded 280/319 tokens (cache: 280)Encoded 290/319 tokens (cache: 290)Encoded 300/319 tokens (cache: 300)Encoded 310/319 tokens (cache: 310)
Encoding complete.
Original size: 1155 chars
Compressed size: 27 bytes
Bits per character: 0.1870
Compression finished in 27.35 seconds.
Results saved to benchmarks.db
Results saved to model_results.json
  BPC: 0.1870 | Ratio: 42.7778

[35/35] Llama 3.2-3B | generated_gemma3_4b_5.py.txt
Loading model from /home/alexn/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B/snapshots/13afe5124825b4f3751f836b40dafda64c1ed062...
`torch_dtype` is deprecated! Use `dtype` instead!
2026-02-16 23:55:13.014594: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
/home/alexn/Documents/code/research/LLMzip_from_scratch/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:6105: UserWarning: expandable_segments not supported on this platform (Triggered internally at /pytorch/c10/hip/HIPAllocatorConfig.h:29.)
  _ = torch.empty(byte_count // factor, dtype=torch.float16, device=device, requires_grad=False)
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.19s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  1.94s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.13s/it]
Compressing /home/alexn/Documents/code/research/LLMzip_from_scratch/source_text/generated_gemma3_4b_5.py.txt to /home/alexn/Documents/code/research/LLMzip_from_scratch/encoded_documents/selfcomp_Llama_323B_generated_gemma3_4b_5.py_w50.llmzip...
Total tokens to encode: 392
/home/alexn/Documents/code/research/LLMzip_from_scratch/.venv/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:101: UserWarning: Attempting to use hipBLASLt on an unsupported architecture! Overriding blas backend to hipblas (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:296.)
  freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
Encoded 10/392 tokens (cache: 10)Encoded 20/392 tokens (cache: 20)Encoded 30/392 tokens (cache: 30)Encoded 40/392 tokens (cache: 40)Encoded 50/392 tokens (cache: 50)Encoded 60/392 tokens (cache: 60)Encoded 70/392 tokens (cache: 70)Encoded 80/392 tokens (cache: 80)Encoded 90/392 tokens (cache: 90)Encoded 100/392 tokens (cache: 100)Encoded 110/392 tokens (cache: 110)Encoded 120/392 tokens (cache: 120)Encoded 130/392 tokens (cache: 130)Encoded 140/392 tokens (cache: 140)Encoded 150/392 tokens (cache: 150)Encoded 160/392 tokens (cache: 160)Encoded 170/392 tokens (cache: 170)Encoded 180/392 tokens (cache: 180)Encoded 190/392 tokens (cache: 190)Encoded 200/392 tokens (cache: 200)Encoded 210/392 tokens (cache: 210)Encoded 220/392 tokens (cache: 220)Encoded 230/392 tokens (cache: 230)Encoded 240/392 tokens (cache: 240)Encoded 250/392 tokens (cache: 250)Encoded 260/392 tokens (cache: 260)Encoded 270/392 tokens (cache: 270)Encoded 280/392 tokens (cache: 280)Encoded 290/392 tokens (cache: 290)Encoded 300/392 tokens (cache: 300)Encoded 310/392 tokens (cache: 310)Encoded 320/392 tokens (cache: 320)Encoded 330/392 tokens (cache: 330)Encoded 340/392 tokens (cache: 340)Encoded 350/392 tokens (cache: 350)Encoded 360/392 tokens (cache: 360)Encoded 370/392 tokens (cache: 370)Encoded 380/392 tokens (cache: 380)Encoded 390/392 tokens (cache: 390)
Encoding complete.
Original size: 1779 chars
Compressed size: 42 bytes
Bits per character: 0.1889
Compression finished in 33.63 seconds.
Results saved to benchmarks.db
Results saved to model_results.json
  BPC: 0.1889 | Ratio: 42.3571

Saved 35 results to /home/alexn/Documents/code/research/LLMzip_from_scratch/self_compression_results.json (total: 70)

Generating plots after Gemma 3-4B-IT phase...
Loaded 70 results

Generators: ['llama32_3b', 'gemma3_4b']
Compressors: ['Gemma 3-1B', 'Gemma 3-4B', 'LiquidAI 1.2B', 'Llama 3.2-1B', 'Llama 3.2-3B', 'Qwen 2.5-3B', 'Qwen 3-1.7B']

Saved: results/self_compression_bpc_by_compressor.png
Saved: results/self_compression_self_vs_other.png
Saved: results/self_compression_heatmap.png
Saved: results/self_compression_model_summary.png
Saved: results/self_compression_affinity.png
Saved: results/self_compression_per_file_gemma3_4b.png
Saved: results/self_compression_per_file_llama32_3b.png

──────────────────────────────────────────────────────────────────────
Generator                 Self Model        Self BPC  Other Avg      Delta
──────────────────────────────────────────────────────────────────────
Gemma 3-4B-IT             Gemma 3-4B          0.2324     0.2249    +0.0075
Llama 3.2-3B-Instruct     Llama 3.2-3B        0.2172     0.2539    -0.0367

Plots saved to results/self_compression_*.png

======================================================================
EXPERIMENT COMPLETE
Total results: 70
Results file: /home/alexn/Documents/code/research/LLMzip_from_scratch/self_compression_results.json
======================================================================
